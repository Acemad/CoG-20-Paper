
@incollection{abbadi_resource_2014,
  title = {Resource {{Entity Action}}: {{A Generalized Design Pattern}} for {{RTS Games}}},
  shorttitle = {Resource {{Entity Action}}},
  booktitle = {Computers and {{Games}}},
  author = {Abbadi, Mohamed and Di Giacomo, Francesco and Orsini, Renzo and Plaat, Aske and Spronck, Pieter and Maggiore, Giuseppe},
  year = {2014},
  volume = {8427},
  pages = {244--256},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-09165-5_21},
  abstract = {In many Real-Time Strategy (RTS) games, players develop an army in real time, then attempt to take out one or more opponents. Despite the existence of basic similarities among the many different RTS games, engines of these games are often built ad hoc, and code re-use among different titles is minimal. We created a design pattern called ``Resource Entity Action'' (REA) abstracting the basic interactions that entities have with each other in most RTS games. The paper discusses the REA pattern and its language abstraction. We also discuss the implementation in the Casanova game programming language. Our analysis shows that the pattern forms a solid basis for a playable RTS game, and that it achieves considerable gains in terms of lines of code and runtime efficiency. We conclude that the REA pattern is a suitable approach to the implementation of many RTS games.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2013\\25 Resource Entity Action A Generalized Design Pattern for RTS Games.pdf},
  isbn = {978-3-319-09164-8 978-3-319-09165-5},
  language = {en}
}

@book{adams_fundamentals_2010,
  title = {Fundamentals of Game Design 2nd},
  author = {Adams, Ernest and Rollings, Andrew},
  year = {2010},
  edition = {2nd ed},
  publisher = {{New Riders}},
  address = {{Berkeley, CA}},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\8LHYDQWC\\Adams and Rollings - 2010 - Fundamentals of game design.pdf},
  isbn = {978-0-321-64337-7},
  keywords = {Computer games,Design,Programming,Video games},
  language = {en},
  lccn = {QA76.76.C672 A322 2010},
  series = {Voices That Matter}
}

@book{adams_fundamentals_2014,
  title = {Fundamentals of Game Design 3rd},
  author = {Adams, Ernest},
  year = {2014},
  edition = {3rd},
  publisher = {{New Riders}},
  address = {{Berkeley, CA}},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\TWSJBVHC\\Adams - 2014 - Fundamentals of game design.epub},
  isbn = {978-0-321-92967-9},
  language = {English}
}

@book{adams_fundamentals_2014-1,
  title = {Fundamentals of Strategy Game Design},
  author = {Adams, Ernest W},
  year = {2014},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\3IKRG87A\\Adams - 2014 - Fundamentals of strategy game design.pdf},
  isbn = {978-0-13-381267-1 978-0-13-381201-5},
  language = {English}
}

@article{adhikari_co-evolving_2018,
  title = {Co-Evolving {{Real}}-{{Time Strategy Game Micro}}},
  author = {Adhikari, Navin K. and Louis, Sushil J. and Liu, Siming and Spurgeon, Walker},
  year = {2018},
  month = mar,
  abstract = {We investigate competitive co-evolution of unit micromanagement in real-time strategy games. Although good longterm macro-strategy and good short-term unit micromanagement both impact real-time strategy games performance, this paper focuses on generating quality micro. Better micro, for example, can help players win skirmishes and battles even when outnumbered. Prior work has shown that we can evolve micro to beat a given opponent. We remove the need for a good opponent to evolve against by using competitive co-evolution to evolve high-quality micro for both sides from scratch. We first co-evolve micro to control a group of ranged units versus a group of melee units. We then move to co-evolve micro for a group of ranged and melee units versus a group of ranged and melee units. Results show that competitive co-evolution produces good quality micro and when combined with the well-known techniques of fitness sharing, shared sampling, and a hall of fame takes less time to produce better quality micro than simple co-evolution. We believe these results indicate the viability of co-evolutionary approaches for generating good unit micro-management.},
  archivePrefix = {arXiv},
  eprint = {1803.10314},
  eprinttype = {arxiv},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\MQGVTEZB\\Adhikari et al. - 2018 - Co-evolving Real-Time Strategy Game Micro.pdf},
  journal = {arXiv:1803.10314 [cs]},
  keywords = {Computer Science - Neural and Evolutionary Computing},
  language = {en},
  primaryClass = {cs}
}

@article{adil_state---art_2017,
  title = {State-of-the-{{Art}} and {{Open Challenges}} in {{RTS Game}}-{{AI}} and {{Starcraft}}},
  author = {Adil, Khan and Jiang, Feng and Liu, Shaohui and Jifara, Worku and Tian, Zhihong and Fu, Yunsheng},
  year = {2017},
  volume = {8},
  issn = {21565570, 2158107X},
  doi = {10.14569/IJACSA.2017.081203},
  abstract = {This paper presents a review of artificial intelligence for different approaches used in real-time strategy games. Real-time strategy (RTS) based games are quick combat games in which the objective is to dominate and destroy the opposing enemy such as Rome-total war, Starcraft, the age of empires, and command \& conquer, etc. In such games, each player needs to utilize resources efficiently, which includes managing different types of soldiers, units, equipment's, economic status, positions and the uncertainty during the combat in real time. Now the best human players face difficulty in defeating the best RTS games due to the recent success and advancement of deep mind technologies. In this paper, we explain state-of-the-art and challenges in artificial intelligence (AI) for RTS games and Starcraft, describing problems and issues carried out by RTS based games with some solutions that are addressed to them. Finally, we conclude by emphasizing on game `CIG \& AIIDE' competitions along with open research problems and questions in the context of RTS Game-AI, where some of the problems and challenges are mostly considered improved and solved but yet some are open for further research.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\3RV4P3CY\\Adil et al. - 2017 - State-of-the-Art and Open Challenges in RTS Game-A.pdf},
  journal = {International Journal of Advanced Computer Science and Applications},
  language = {en},
  number = {12}
}

@inproceedings{alhejali_using_2013,
  title = {Using Genetic Programming to Evolve Heuristics for a {{Monte Carlo Tree Search Ms Pac}}-{{Man}} Agent},
  booktitle = {2013 {{IEEE Conference}} on {{Computational Inteligence}} in {{Games}} ({{CIG}})},
  author = {Alhejali, Atif M. and Lucas, Simon M.},
  year = {2013},
  month = aug,
  pages = {1--8},
  publisher = {{IEEE}},
  address = {{Niagara Falls, ON, Canada}},
  doi = {10.1109/CIG.2013.6633639},
  abstract = {Ms Pac-Man is one of the most challenging test beds in game artificial intelligence (AI). Genetic programming and Monte Carlo Tree Search (MCTS) have already been successful applied to several games including Pac-Man. In this paper, we use Monte Carlo Tree Search to create a Ms Pac-Man playing agent before using genetic programming to enhance its performance by evolving a new default policy to replace the random agent used in the simulations. The new agent with the evolved default policy was able to achieve an 18\% increase on its average score over the agent with random default policy.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Alhejali and Lucas - 2013 - Using genetic programming to evolve heuristics for.pdf},
  isbn = {978-1-4673-5311-3 978-1-4673-5308-3},
  language = {en}
}

@incollection{aloupis_lumines_2007,
  title = {{{LUMINES Strategies}}},
  booktitle = {Computers and {{Games}}},
  author = {Aloupis, Greg and Cardinal, Jean and Collette, S{\'e}bastien and Langerman, Stefan},
  year = {2007},
  volume = {4630},
  pages = {190--199},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-75538-8_17},
  abstract = {We analyze a new popular video-game called Lumines, which was developed by Sony for the PSP platform. It involves a sequence of bichromatic 2 \texttimes{} 2 blocks that fall in a grid and must be shifted or rotated by the player before they land. Patterns of monochromatic 2 \texttimes{} 2 blocks in the terrain are regularly deleted. The primary goal is to contain the terrain within a fixed height and, if possible, clear the grid.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2006\\22 $LUMINES$ Strategies.pdf},
  isbn = {978-3-540-75537-1 978-3-540-75538-8},
  language = {en}
}

@inproceedings{althofer_anomalies_2014,
  title = {Anomalies of {{Pure Monte}}-{{Carlo Search}} in {{Monte}}-{{Carlo Perfect Games}}},
  booktitle = {Computers and {{Games}}},
  author = {Alth{\"o}fer, Ingo and Turner, Wesley Michael},
  editor = {{van den Herik}, H. Jaap and Iida, Hiroyuki and Plaat, Aske},
  year = {2014},
  pages = {84--99},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-09165-5_8},
  abstract = {A game is called ``Monte-Carlo perfect'' when in each position pure Monte-Carlo search converges to perfect play as the number of simulations tends toward infinity. We exhibit three families of Monte-Carlo perfect single-player and two-player games where this convergence is not monotonic. We for example give a class of MC-perfect games in which MC(1) performs arbitrarily well against MC(1,000).},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2013\\12 Anomalies of Pure Monte-Carlo Search in Monte-Carlo Perfect Games.pdf},
  isbn = {978-3-319-09165-5},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@incollection{althofer_computer-aided_2016,
  title = {Computer-{{Aided Go}}: {{Chess}} as a {{Role Model}}},
  shorttitle = {Computer-{{Aided Go}}},
  booktitle = {Computers and {{Games}}},
  author = {Alth{\"o}fer, Ingo},
  year = {2016},
  volume = {10068},
  pages = {149--155},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-50935-8_14},
  abstract = {Recently computers have gained strength in the Asian board game Go. The Chess community experienced some 15 to 30 years ago that teams with humans and computers may be much stronger than each of their components. This paper claims that time is ripe for computer-aided Go on a large scale, although neither most users nor the Go programmers have realized it. A central part of the paper describes successful pioneers in Go play with computer help. Progress in computer-aided Go may also lead to progress in human Go and in computer Go itself.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2016\\18 Computer-Aided Go Chess as a Role Model.pdf},
  isbn = {978-3-319-50934-1 978-3-319-50935-8},
  language = {en}
}

@article{amado_q-table_2018,
  title = {Q-{{Table}} Compression for Reinforcement Learning},
  author = {Amado, Leonardo and Meneguzzi, Felipe},
  year = {2018},
  volume = {33},
  issn = {0269-8889, 1469-8005},
  doi = {10.1017/S0269888918000280},
  abstract = {Reinforcement learning (RL) algorithms are often used to compute agents capable of acting in environments without prior knowledge of the environment dynamics. However, these algorithms struggle to converge in environments with large branching factors and their large resulting state-spaces. In this work, we develop an approach to compress the number of entries in a Q-value table using a deep autoencoder. We develop a set of techniques to mitigate the large branching factor problem. We present the application of such techniques in the scenario of a real-time strategy (RTS) game, where both state space and branching factor are a problem. We empirically evaluate an implementation of the technique to control agents in an RTS game scenario where classical RL fails and provide a number of possible avenues of further work on this problem.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\U7AXU4NE\\Amado and Meneguzzi - 2018 - Q-Table compression for reinforcement learning.pdf},
  journal = {The Knowledge Engineering Review},
  language = {en}
}

@article{amado_reinforcement_2017,
  title = {Reinforcement {{Learning Applied}} to {{RTS}} Games},
  author = {Amado, Leonardo Rosa},
  year = {2017},
  pages = {28},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\EF238P5W\\Amado - 2017 - Reinforcement Learning Applied to RTS games.pdf},
  language = {en}
}

@inproceedings{andersen_deep_2018,
  title = {Deep {{RTS}}: {{A Game Environment}} for {{Deep Reinforcement Learning}} in {{Real}}-{{Time Strategy Games}}},
  shorttitle = {Deep {{RTS}}},
  booktitle = {2018 {{IEEE Conference}} on {{Computational Intelligence}} and {{Games}} ({{CIG}})},
  author = {Andersen, Per-Arne and Goodwin, Morten and Granmo, Ole-Christoffer},
  year = {2018},
  month = aug,
  pages = {149--156},
  issn = {2325-4289},
  doi = {10.1109/CIG.2018.8490409},
  abstract = {Reinforcement learning (RL) is an area of research that has blossomed tremendously in recent years and has shown remarkable potential for artificial intelligence based opponents in computer games. This success is primarily due to the vast capabilities of convolutional neural networks, that can extract useful features from noisy and complex data. Games are excellent tools to test and push the boundaries of novel RL algorithms because they give valuable insight into how well an algorithm can perform in isolated environments without the real-life consequences. Real-time strategy games (RTS) is a genre that has tremendous complexity and challenges the player in short and long-term planning. There is much research that focuses on applied RL in RTS games, and novel advances are therefore anticipated in the not too distant future. However, there are to date few environments for testing RTS AIs. Environments in the literature are often either overly simplistic, such as microRTS, or complex and without the possibility for accelerated learning on consumer hardware like StarCraft II. This paper introduces the Deep RTS game environment for testing cutting-edge artificial intelligence algorithms for RTS games. Deep RTS is a high-performance RTS game made specifically for artificial intelligence research. It supports accelerated learning, meaning that it can learn at a magnitude of 50 000 times faster compared to existing RTS games. Deep RTS has a flexible configuration, enabling research in several different RTS scenarios, including partially observable state-spaces and map complexity. We show that Deep RTS lives up to our promises by comparing its performance with microRTS, ELF, and StarCraft II on high-end consumer hardware. Using Deep RTS, we show that a Deep Q-Network agent beats random-play agents over 70\% of the time. Deep RTS is publicly available at https://github.com/cair/DeepRTS.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\4PJQHCTW\\Andersen et al. - 2018 - Deep RTS A Game Environment for Deep Reinforcemen.pdf;C\:\\Users\\aesou\\Zotero\\storage\\BWK3ZTAL\\Andersen et al. - 2018 - Deep RTS A Game Environment for Deep Reinforcemen.pdf;C\:\\Users\\aesou\\Zotero\\storage\\Z29PFZ56\\8490409.html},
  keywords = {artificial intelligence research,computer games,convolution,cutting-edge artificial intelligence algorithms,deep q-learning,Deep Q-Network agent,deep reinforcement learning,Deep RTS game environment,feedforward neural nets,Games,Geophysical measurement techniques,Ground penetrating radar,high-performance RTS game,learning (artificial intelligence),Learning (artificial intelligence),Machine learning,multi-agent systems,Planning,real-time strategy game,real-time strategy games,RTS AIs,StarCraft II}
}

@incollection{andraos_comparative_2007,
  title = {Comparative {{Study}} of {{Approximate Strategies}} for {{Playing Sum Games Based}} on {{Subgame Types}}},
  booktitle = {Computers and {{Games}}},
  author = {Andraos, Cherif R. S. and Zaky, Manal M. and Ghoneim, Salma A.},
  year = {2007},
  volume = {4630},
  pages = {212--219},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-75538-8_19},
  abstract = {Combinatorial games of the form \{\{A|B\}|\{C|D\}\} can be classified as either left excitable, right excitable, or equitable [2]. Several approximate strategies for playing sums of games of this form have been proposed in the literature [2,3,4]. In this work we propose a new approach for evaluating the different strategies based on the types of the subgames participating in a sum game. While previous comparisons [3,4] were only able to rank the strategies according to their average performance in a large number of randomly generated games, our evaluation is able to pinpoint the strengths and weaknesses of each strategy. We show that none of the strategies can be considered the best in an absolute sense. Therefore we recommend the development of type-based approximate strategies with enhanced performance.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2006\\24 Comparative Study of Approximate Strategies for Playing Sum Games Based on Subgame Types.pdf},
  isbn = {978-3-540-75537-1 978-3-540-75538-8},
  language = {en}
}

@book{anthropy_game_2014,
  title = {A Game Design Vocabulary Exploring the Foundational Principles behind Good Game Design},
  author = {Anthropy, Anna and Clark, Naomi},
  year = {2014},
  publisher = {{Addison-Wesley}},
  address = {{Upper Saddle River, N.J}},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\XMTPK4YI\\Anthropy and Clark - 2014 - A game design vocabulary exploring the foundationa.pdf},
  isbn = {978-0-321-88692-7},
  language = {English}
}

@article{antuori_constrained_2019,
  title = {Constrained Optimization under Uncertainty for Decision-Making Problems: {{Application}} to {{Real}}-{{Time Strategy}} Games},
  shorttitle = {Constrained Optimization under Uncertainty for Decision-Making Problems},
  author = {Antuori, Valentin and Richoux, Florian},
  year = {2019},
  month = jan,
  abstract = {Decision-making problems can be modeled as combinatorial optimization problems with Constraint Programming formalisms such as Constrained Optimization Problems. However, few Constraint Programming formalisms can deal with both optimization and uncertainty at the same time, and none of them are convenient to model problems we tackle in this paper. Here, we propose a way to deal with combinatorial optimization problems under uncertainty within the classical Constrained Optimization Problems formalism by injecting the Rank Dependent Utility from decision theory. We also propose a proof of concept of our method to show it is implementable and can solve concrete decision-making problems using a regular constraint solver, and propose a bot that won the partially observable track of the 2018 \textmu{}RTS AI competition. Our result shows it is possible to handle uncertainty with regular Constraint Programming solvers, without having to define a new formalism neither to develop dedicated solvers. This brings new perspective to tackle uncertainty in Constraint Programming.},
  archivePrefix = {arXiv},
  eprint = {1901.00942},
  eprinttype = {arxiv},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Antuori and Richoux - 2019 - Constrained optimization under uncertainty for dec 2.pdf},
  journal = {arXiv:1901.00942 [cs]},
  keywords = {Computer Science - Artificial Intelligence},
  language = {en},
  primaryClass = {cs}
}

@article{arneson_monte_2010,
  title = {Monte {{Carlo Tree Search}} in {{Hex}}},
  author = {Arneson, Broderick and Hayward, Ryan B. and Henderson, Philip},
  year = {2010},
  month = dec,
  volume = {2},
  pages = {251--258},
  issn = {1943-068X, 1943-0698},
  doi = {10.1109/TCIAIG.2010.2067212},
  abstract = {Hex, the classic board game invented by Piet Hein in 1942 and independently by John Nash in 1948, has been a domain of artificial intelligence research since Claude Shannon's seminal work in the 1950s.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Arneson et al. - 2010 - Monte Carlo Tree Search in Hex.pdf},
  journal = {IEEE Transactions on Computational Intelligence and AI in Games},
  language = {en},
  number = {4}
}

@incollection{arneson_solving_2011,
  title = {Solving {{Hex}}: {{Beyond Humans}}},
  shorttitle = {Solving {{Hex}}},
  booktitle = {Computers and {{Games}}},
  author = {Arneson, Broderick and Hayward, Ryan B. and Henderson, Philip},
  year = {2011},
  volume = {6515},
  pages = {1--10},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-17928-0_1},
  abstract = {For the first time, automated Hex solvers have surpassed humans in their ability to solve Hex positions: they can now solve many 9\texttimes{}9 Hex openings. We summarize the methods that attained this milestone, and examine the future of Hex solvers.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2010\\6 Solving Hex Beyond Humans.pdf},
  isbn = {978-3-642-17927-3 978-3-642-17928-0},
  language = {en}
}

@article{arulkumaran_alphastar_2019,
  title = {{{AlphaStar}}: {{An Evolutionary Computation Perspective}}},
  shorttitle = {{{AlphaStar}}},
  author = {Arulkumaran, Kai and Cully, Antoine and Togelius, Julian},
  year = {2019},
  month = feb,
  abstract = {In January 2019, DeepMind revealed AlphaStar to the world\textemdash{}the first artificial intelligence (AI) system to beat a professional player at the game of StarCraft II\textemdash{}representing a milestone in the progress of AI. AlphaStar draws on many areas of AI research, including deep learning, reinforcement learning, game theory, and evolutionary computation (EC). In this paper we analyze AlphaStar primarily through the lens of EC, presenting a new look at the system and relating it to many concepts in the field. We highlight some of its most interesting aspects\textemdash{}the use of Lamarckian evolution, competitive co-evolution, and quality diversity. In doing so, we hope to provide a bridge between the wider EC community and one of the most significant AI systems developed in recent times.},
  archivePrefix = {arXiv},
  eprint = {1902.01724},
  eprinttype = {arxiv},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Arulkumaran et al. - 2019 - AlphaStar An Evolutionary Computation Perspective 2.pdf;C\:\\Users\\aesou\\Zotero\\storage\\4A5KR56T\\Arulkumaran et al. - 2019 - AlphaStar An Evolutionary Computation Perspective.pdf},
  journal = {arXiv:1902.01724 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  language = {en},
  primaryClass = {cs}
}

@article{arulkumaran_brief_2017,
  title = {A {{Brief Survey}} of {{Deep Reinforcement Learning}}},
  author = {Arulkumaran, Kai and Deisenroth, Marc Peter and Brundage, Miles and Bharath, Anil Anthony},
  year = {2017},
  month = nov,
  volume = {34},
  pages = {26--38},
  issn = {1053-5888},
  doi = {10.1109/MSP.2017.2743240},
  abstract = {Deep reinforcement learning is poised to revolutionise the field of AI and represents a step towards building autonomous systems with a higher level understanding of the visual world. Currently, deep learning is enabling reinforcement learning to scale to problems that were previously intractable, such as learning to play video games directly from pixels. Deep reinforcement learning algorithms are also applied to robotics, allowing control policies for robots to be learned directly from camera inputs in the real world. In this survey, we begin with an introduction to the general field of reinforcement learning, then progress to the main streams of value-based and policybased methods. Our survey will cover central algorithms in deep reinforcement learning, including the deep Q-network, trust region policy optimisation, and asynchronous advantage actor-critic. In parallel, we highlight the unique advantages of deep neural networks, focusing on visual understanding via reinforcement learning. To conclude, we describe several current areas of research within the field.},
  archivePrefix = {arXiv},
  eprint = {1708.05866},
  eprinttype = {arxiv},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\LZWNM4FP\\Arulkumaran et al. - 2017 - A Brief Survey of Deep Reinforcement Learning.pdf},
  journal = {IEEE Signal Processing Magazine},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  number = {6}
}

@incollection{bahri_new_2011,
  title = {New {{Solutions}} for {{Synchronized Domineering}}},
  booktitle = {Computers and {{Games}}},
  author = {Bahri, Sahil and Kruskal, Clyde P.},
  year = {2011},
  volume = {6515},
  pages = {211--229},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-17928-0_20},
  abstract = {Cincotti and Iida invented the game of Synchronized Domineering, and analyzed a few special cases. We develop a more general technique of analysis, and obtain results for many more special cases. We obtain complete results for board sizes 3 \texttimes{} n, 5 \texttimes{} n, 7 \texttimes{} n, and 9 \texttimes{} n (for n large enough) and partial results for board sizes 2 \texttimes{} n, 4 \texttimes{} n, and 6 \texttimes{} n.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2010\\25 New Solutions for Synchronized Domineering.pdf},
  isbn = {978-3-642-17927-3 978-3-642-17928-0},
  language = {en}
}

@inproceedings{baier_evolutionary_2018,
  title = {Evolutionary {{MCTS}} with {{Flexible Search Horizon}}},
  booktitle = {The {{Fourteenth Artificial Intelligence}} and {{Interactive Digital Entertainment Conference}} ({{AIIDE}} 2018)},
  author = {Baier, Hendrik and Cowling, Peter I},
  year = {2018},
  pages = {2--8},
  abstract = {In turn-based multi-action adversarial games each player turn consists of several atomic actions, resulting in an extremely high branching factor. Many strategy board, card, and video games fall into this category, which is currently best played by Evolutionary MCTS (EMCTS) \textendash{} searching a tree with nodes representing action sequences as genomes, and edges representing mutations of those genomes. However, regular EMCTS is unable to search beyond the current player's turn, leading to strategic short-sightedness. In this paper, we extend EMCTS to search to any given search depth beyond the current turn, using simple models of its own and the opponent's behavior. Experiments on the game Hero Academy show that this Flexible-Horizon EMCTS (FH-EMCTS) convincingly outperforms several baselines including regular EMCTS, Online Evolutionary Planning (OEP), and vanilla MCTS, at all tested numbers of atomic actions per turn. Additionally, the separate contributions of the behavior models and the flexible search horizon are analyzed.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Baier and Cowling - 2018 - Evolutionary MCTS with Flexible Search Horizon.pdf},
  language = {en}
}

@inproceedings{baier_evolutionary_2018-1,
  title = {Evolutionary {{MCTS}} for {{Multi}}-{{Action Adversarial Games}}},
  booktitle = {2018 {{IEEE Conference}} on {{Computational Intelligence}} and {{Games}} ({{CIG}})},
  author = {Baier, Hendrik and Cowling, Peter I.},
  year = {2018},
  month = aug,
  pages = {1--8},
  publisher = {{IEEE}},
  address = {{Maastricht}},
  doi = {10.1109/CIG.2018.8490403},
  abstract = {Turn-based multi-action adversarial games are games in which each player turn consists of a sequence of atomic actions, resulting in an extremely high branching factor. Many strategy board, card, and video games fall into this category, for which the current state of the art is Online Evolutionary Planning (OEP) \textendash{} an evolutionary algorithm (EA) that treats atomic actions as genes, and complete action sequences as genomes. In this paper, we introduce Evolutionary Monte Carlo Tree Search (EMCTS) to tackle this challenge, combining the tree search of MCTS with the sequence-based optimization of EAs. Experiments on the game Hero Academy show that EMCTS convincingly outperforms several baselines including OEP and an improved variant of OEP introduced in this paper, at different time settings and numbers of atomic actions per turn. EMCTS also scales better than any existing algorithm with the complexity of the problem.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Baier and Cowling - 2018 - Evolutionary MCTS for Multi-Action Adversarial Gam.pdf},
  isbn = {978-1-5386-4359-4},
  language = {en}
}

@article{baldominos_automated_2020,
  title = {On the Automated, Evolutionary Design of Neural Networks: Past, Present, and Future},
  shorttitle = {On the Automated, Evolutionary Design of Neural Networks},
  author = {Baldominos, Alejandro and Saez, Yago and Isasi, Pedro},
  year = {2020},
  month = jan,
  volume = {32},
  pages = {519--545},
  issn = {1433-3058},
  doi = {10.1007/s00521-019-04160-6},
  abstract = {Neuroevolution is the name given to a field of computer science that applies evolutionary computation for evolving some aspects of neural networks. After the AI Winter came to an end, neural networks reemerged to solve a great variety of problems. However, their usage requires designing their topology, a decision with a potentially high impact on performance. Whereas many works have tried to suggest rules-of-thumb for designing topologies, the truth is that there are not analytic procedures for determining the optimal one for a given problem, and trial-and-error is often used instead. Neuroevolution arose almost 3 decades ago, with some works focusing on the evolutionary design of the topology and most works describing techniques for learning connection weights. Since then, evolutionary computation has been proved to be a convenient approach for determining the topology and weights of neural networks, and neuroevolution has been applied to a great variety of fields. However, for more than 2 decades neuroevolution has mainly focused on simple artificial neural networks models, far from today's deep learning standards. This is insufficient for determining good architectures for modern networks extensively used nowadays, which involve multiple hidden layers, recurrent cells, etc. More importantly, deep and convolutional neural networks have become a de facto standard in representation learning for solving many different problems, and neuroevolution has only focused in this kind of networks in very recent years, with many works being presented in 2017 onward. In this paper, we review the field of neuroevolution during the last 3 decades. We will put the focus on very recent works on the evolution of deep and convolutional neural networks, which is a new but growing field of study. To the best of our knowledge, this is the best survey reviewing the literature in this field, and we have described the features of each work as well as their performance on well-known databases when available. This work aims to provide a complete reference of all works related to neuroevolution of convolutional neural networks up to the date. Finally, we will provide some future directions for the advancement of this research area.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Baldominos et al. - 2020 - On the automated, evolutionary design of neural ne.pdf},
  journal = {Neural Computing and Applications},
  language = {en},
  number = {2}
}

@inproceedings{balla_uct_2009,
  title = {{{UCT}} for {{Tactical Assault Planning}} in {{Real}}-{{Time Strategy Games}}},
  booktitle = {International {{Joint Conference}} on {{Artificial Intelligence}}},
  author = {Balla, Radha-Krishna and Fern, Alan},
  year = {2009},
  pages = {40--45},
  abstract = {We consider the problem of tactical assault planning in real-time strategy games where a team of friendly agents must launch an assault on an enemy. This problem offers many challenges including a highly dynamic and uncertain environment, multiple agents, durative actions, numeric attributes, and different optimization objectives. While the dynamics of this problem are quite complex, it is often possible to provide or learn a coarse simulation-based model of a tactical domain, which makes Monte-Carlo planning an attractive approach. In this paper, we investigate the use of UCT, a recent Monte-Carlo planning algorithm for this problem. UCT has recently shown impressive successes in the area of games, particularly Go, but has not yet been considered in the context of multiagent tactical planning. We discuss the challenges of adapting UCT to our domain and an implementation which allows for the optimization of user specified objective functions. We present an evaluation of our approach on a range of tactical assault problems with different objectives in the RTS game Wargus. The results indicate that our planner is able to generate superior plans compared to several baselines and a human player.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Balla and Fern - 2009 - UCT for Tactical Assault Planning in Real-Time Str.pdf},
  language = {en}
}

@inproceedings{barriga_building_2014,
  title = {Building {{Placement Optimization}} in {{Real}}-{{Time Strategy Games}}},
  booktitle = {Artificial {{Intelligence}} in {{Adversarial Real}}-{{Time Games}}: {{Papers}} from the {{AIIDE Workshop}}},
  author = {Barriga, Nicolas A and Stanescu, Marius and Buro, Michael},
  year = {2014},
  pages = {6},
  abstract = {In this paper we propose using a Genetic Algorithm to optimize the placement of buildings in Real-Time Strategy games. Candidate solutions are evaluated by running base assault simulations. We present experimental results in SparCraft \textemdash{} a StarCraft combat simulator \textemdash{} using battle setups extracted from human and bot StarCraft games. We show that our system is able to turn base assaults that are losses for the defenders into wins, as well as reduce the number of surviving attackers. Performance is heavily dependent on the quality of the prediction of the attacker army composition used for training, and its similarity to the army used for evaluation. These results apply to both human and bot games.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\B6LHBUMT\\Barriga et al. - 2014 - Building Placement Optimization in Real-Time Strat.pdf},
  language = {en}
}

@inproceedings{barriga_combining_2017,
  title = {Combining {{Strategic Learning}} with {{Tactical Search}} in {{Real}}-{{Time Strategy Games}}},
  booktitle = {{{AIIDE}}'17},
  author = {Barriga, Nicolas A. and Stanescu, Marius and Buro, Michael},
  year = {2017},
  month = sep,
  pages = {9--15},
  address = {{Snowbird Ski Resort, Utah}},
  abstract = {A commonly used technique for managing AI complexity in real-time strategy (RTS) games is to use action and/or state abstractions. High-level abstractions can often lead to good strategic decision making, but tactical decision quality may suffer due to lost details. A competing method is to sample the search space which often leads to good tactical performance in simple scenarios, but poor high-level planning. We propose to use a deep convolutional neural network (CNN) to select among a limited set of abstract action choices, and to utilize the remaining computation time for game tree search to improve low level tactics. The CNN is trained by supervised learning on game states labelled by Puppet Search, a strategic search algorithm that uses action abstractions. The network is then used to select a script \textemdash{} an abstract action -\textemdash{} to produce low level actions for all units. Subsequently, the game tree search algorithm improves the tactical actions of a subset of units using a limited view of the game state only considering units close to opponent units. Experiments in the microRTS game show that the combined algorithm results in higher win-rates than either of its two independent components and other state-of-the-art microRTS agents. To the best of our knowledge, this is the first successful application of a convolutional network to play a full RTS game on standard game maps, as previous work has focused on sub-problems, such as combat, or on very small maps.},
  copyright = {Authors who publish a paper in this conference agree to the following terms:     Author(s) agree to transfer their copyrights in their article/paper to the Association for the Advancement of Artificial Intelligence (AAAI), in order to deal with future requests for reprints, translations, anthologies, reproductions, excerpts, and other publications. This grant will include, without limitation, the entire copyright in the article/paper in all countries of the world, including all renewals, extensions, and reversions thereof, whether such rights current exist or hereafter come into effect, and also the exclusive right to create electronic versions of the article/paper, to the extent that such right is not subsumed under copyright.   The author(s) warrants that they are the sole author and owner of the copyright in the above article/paper, except for those portions shown to be in quotations; that the article/paper is original throughout; and that the undersigned right to make the grants set forth above is complete and unencumbered   The author(s) agree that if anyone brings any claim or action alleging facts that, if true, constitute a breach of any of the foregoing warranties, the author(s) will hold harmless and indemnify AAAI, their grantees, their licensees, and their distributors against any liability, whether under judgment, decree, or compromise, and any legal fees and expenses arising out of that claim or actions, and the undersigned will cooperate fully in any defense AAAI may make to such claim or action. Moreover, the undersigned agrees to cooperate in any claim or other action seeking to protect or enforce any right the undersigned has granted to AAAI in the article/paper. If any such claim or action fails because of facts that constitute a breach of any of the foregoing warranties, the undersigned agrees to reimburse whomever brings such claim or action for expenses and attorneys' fees incurred therein.   Author(s) retain all proprietary rights other than copyright (such as patent rights).   Author(s) may make personal reuse of all or portions of the above article/paper in other works of their own authorship.   Author(s) may reproduce, or have reproduced, their article/paper for the author's personal use, or for company use provided that AAAI copyright and the source are indicated, and that the copies are not used in a way that implies AAAI endorsement of a product or service of an employer, and that the copies per se are not offered for sale. The foregoing right shall not permit the posting of the article/paper in electronic or digital form on any computer network, except by the author or the author's employer, and then only on the author's or the employer's own web page or ftp site. Such web page or ftp site, in addition to the aforementioned requirements of this Paragraph, must provide an electronic reference or link back to the AAAI electronic server, and shall not post other AAAI copyrighted materials not of the author's or the employer's creation (including tables of contents with links to other papers) without AAAI's written permission.   Author(s) may make limited distribution of all or portions of their article/paper prior to publication.   In the case of work performed under U.S. Government contract, AAAI grants the U.S. Government royalty-free permission to reproduce all or portions of the above article/paper, and to authorize others to do so, for U.S. Government purposes.   In the event the above article/paper is not accepted and published by AAAI, or is withdrawn by the author(s) before acceptance by AAAI, this agreement becomes null and void.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\4K3A9DPM\\Barriga et al. - 2017 - Combining Strategic Learning with Tactical Search .pdf;C\:\\Users\\aesou\\Zotero\\storage\\8F2VJVRD\\Barriga et al. - 2017 - Combining Strategic Learning and Tactical Search i.pdf;C\:\\Users\\aesou\\Zotero\\storage\\KS9E984U\\15814.html},
  language = {en}
}

@article{barriga_game_2017,
  title = {Game {{Tree Search Based}} on {{Nondeterministic Action Scripts}} in {{Real}}-{{Time Strategy Games}}},
  author = {Barriga, Nicolas A. and Stanescu, Marius and Buro, Michael},
  year = {2017},
  month = jun,
  volume = {10},
  pages = {69--77},
  issn = {2475-1502, 2475-1510},
  doi = {10.1109/TCIAIG.2017.2717902},
  abstract = {Significant progress has been made in recent years towards stronger Real-Time Strategy (RTS) game playing agents. Some of the latest approaches have focused on enhancing standard game tree search techniques with a smart sampling of the search space, or on directly reducing this search space. However, experiments have thus far only been performed using small scenarios. We provide experimental results on the performance of these agents on increasingly larger scenarios. Our main contribution is Puppet Search, a new adversarial search framework that reduces the search space by using scripts that can expose choice points to a look-ahead search procedure. Selecting a combination of a script and decisions for its choice points represents an abstract move to be applied next. Such moves can be directly executed in the actual game, or in an abstract representation of the game state which can be used by an adversarial tree search algorithm. We tested Puppet Search in \textmu{}RTS, an abstract RTS game popular within the research community, allowing us to directly compare our algorithm against state-of-the-art agents published in the last few years. We show a similar performance to other scripted and search based agents on smaller scenarios, while outperforming them on larger ones.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\IAZU8EP7\\Barriga et al. - 2018 - Game Tree Search Based on Nondeterministic Action .pdf},
  journal = {IEEE Transactions on Games},
  language = {en},
  number = {1}
}

@article{barriga_improving_2019,
  title = {Improving {{RTS Game AI}} by {{Supervised Policy Learning}}, {{Tactical Search}}, and {{Deep Reinforcement Learning}}},
  author = {Barriga, Nicolas A. and Stanescu, Marius and Besoain, Felipe and Buro, Michael},
  year = {2019},
  month = aug,
  volume = {14},
  pages = {8--18},
  issn = {1556-603X, 1556-6048},
  doi = {10.1109/MCI.2019.2919363},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Barriga et al. - 2019 - Improving RTS Game AI by Supervised Policy Learnin.pdf},
  journal = {IEEE Computational Intelligence Magazine},
  language = {en},
  number = {3}
}

@inproceedings{barriga_puppet_2015,
  title = {Puppet {{Search}}: {{Enhancing Scripted Behavior}} by {{Look}}-{{Ahead Search}} with {{Applications}} to {{Real}}-{{Time Strategy Games}}},
  shorttitle = {Puppet {{Search}}},
  booktitle = {{{AIIDE}}'15},
  author = {Barriga, Nicolas Arturo and Stanescu, Marius and Buro, Michael},
  year = {2015},
  month = sep,
  pages = {9--15},
  address = {{Santa Cruz, California}},
  abstract = {Real-Time Strategy (RTS) games have shown to be very resilient to standard~  adversarial tree search techniques. Recently, a few approaches to tackle their   complexity have emerged that use game state or move abstractions, or both.   Unfortunately, the supporting experiments were either limited to simpler RTS   environments ( u RTS, SparCraft) or lack testing against state-of-the-art   game playing agents.   Here, we propose Puppet Search ,  a new adversarial search framework based   on scripts that can expose choice points to a look-ahead search   procedure. Selecting a combination of a script and decisions for its choice   points represents a move to be applied next. Such moves can be executed in the   actual game, thus letting the script play, or in an abstract representation of   the game state which can be used by an adversarial tree search   algorithm. Puppet Search returns a principal variation of scripts and choices   to be executed by the agent for a given time span.   We implemented the algorithm in a complete StarCraft bot. Experiments show   that it matches or outperforms all of the individual scripts that it   uses when playing against state-of-the-art bots from the 2014 AIIDE StarCraft   competition.},
  copyright = {Authors who publish a paper in this conference agree to the following terms:     Author(s) agree to transfer their copyrights in their article/paper to the Association for the Advancement of Artificial Intelligence (AAAI), in order to deal with future requests for reprints, translations, anthologies, reproductions, excerpts, and other publications. This grant will include, without limitation, the entire copyright in the article/paper in all countries of the world, including all renewals, extensions, and reversions thereof, whether such rights current exist or hereafter come into effect, and also the exclusive right to create electronic versions of the article/paper, to the extent that such right is not subsumed under copyright.   The author(s) warrants that they are the sole author and owner of the copyright in the above article/paper, except for those portions shown to be in quotations; that the article/paper is original throughout; and that the undersigned right to make the grants set forth above is complete and unencumbered   The author(s) agree that if anyone brings any claim or action alleging facts that, if true, constitute a breach of any of the foregoing warranties, the author(s) will hold harmless and indemnify AAAI, their grantees, their licensees, and their distributors against any liability, whether under judgment, decree, or compromise, and any legal fees and expenses arising out of that claim or actions, and the undersigned will cooperate fully in any defense AAAI may make to such claim or action. Moreover, the undersigned agrees to cooperate in any claim or other action seeking to protect or enforce any right the undersigned has granted to AAAI in the article/paper. If any such claim or action fails because of facts that constitute a breach of any of the foregoing warranties, the undersigned agrees to reimburse whomever brings such claim or action for expenses and attorneys' fees incurred therein.   Author(s) retain all proprietary rights other than copyright (such as patent rights).   Author(s) may make personal reuse of all or portions of the above article/paper in other works of their own authorship.   Author(s) may reproduce, or have reproduced, their article/paper for the author's personal use, or for company use provided that AAAI copyright and the source are indicated, and that the copies are not used in a way that implies AAAI endorsement of a product or service of an employer, and that the copies per se are not offered for sale. The foregoing right shall not permit the posting of the article/paper in electronic or digital form on any computer network, except by the author or the author's employer, and then only on the author's or the employer's own web page or ftp site. Such web page or ftp site, in addition to the aforementioned requirements of this Paragraph, must provide an electronic reference or link back to the AAAI electronic server, and shall not post other AAAI copyrighted materials not of the author's or the employer's creation (including tables of contents with links to other papers) without AAAI's written permission.   Author(s) may make limited distribution of all or portions of their article/paper prior to publication.   In the case of work performed under U.S. Government contract, AAAI grants the U.S. Government royalty-free permission to reproduce all or portions of the above article/paper, and to authorize others to do so, for U.S. Government purposes.   In the event the above article/paper is not accepted and published by AAAI, or is withdrawn by the author(s) before acceptance by AAAI, this agreement becomes null and void.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Barriga - 2015 - Puppet Search Enhancing Scripted Behavior by Look.pdf;C\:\\Users\\aesou\\Zotero\\storage\\LUJUEEX7\\Barriga et al. - 2015 - Puppet Search Enhancing Scripted Behavior by Look.pdf;C\:\\Users\\aesou\\Zotero\\storage\\G4K2LMRP\\11528.html},
  language = {en}
}

@phdthesis{barriga_search_2017,
  title = {Search, {{Abstractions}} and {{Learning}} in {{Real}}-{{Time Strategy Games}}},
  author = {Barriga, Nicolas Arturo},
  year = {2017},
  abstract = {Real-time strategy (RTS) games are war simulation video games in which the players perform several simultaneous tasks like gathering and spending resources, building a base, and controlling units in combat against an enemy force. RTS games have recently drawn the interest of the game AI research community, due to its interesting sub-problems and the availability of professional human players. Large state and action space make standard adversarial search techniques impractical. Sampling the action space can lead to strong tactical performance on smaller scenarios, but doesn't scale to the sizes used on commercial RTS games. Using state and/or action abstractions contributes to solid strategic decision making, but tactical performance suffers, due to the necessary simplifications introduced by the abstractions. Combining both techniques is not straightforward, due to the real-time constraints involved. We first present Puppet Search, a search framework that employs scripts as action abstractions. It produces agents with strong strategic awareness, as well as adequate tactical performance. The tactical performance comes from incorporating sub-problem solutions, such as pathfinding and build order search, into the scripts. We then split the available computation time between this strategic search and NaiveMCTS, a strong tactical search algorithm that samples the low-level action space. This second search refines the output of the first one by reassigning actions to units engaged in combat with the opponent's units. Finally, we present a deep convolutional neural network (CNN) that can accurately predict Puppet Search output in a fraction of the time, thus leaving more time available for tactical search. Experiments in StarCraft: Brood War show that Puppet Search outperforms its component scripts, while in microRTS it also surpasses other state-of-the-art agents. Further experimental results show that the combined Puppet Search/NaiveMCTS algorithm achieves higher win-rates than either of its two independent components and other state-of-the-art microRTS agents. Finally, replacing Puppet Search with a CNN shows even higher performance. To the best of our knowledge, this is the first successful application of a convolutional network to play a full RTS game on standard sized game maps, as previous work has focused on sub-problems, such as combat, or on very small maps. We propose further work to focus on partial observability and CNNs for tactical decision-making. Finally, we explore possible utilization in other game genres and potential applications on the game development process itself, such as playtesting and game balancing.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\GULVT633\\Barriga - 2017 - Search, Abstractions and Learning in Real-Time Str.pdf},
  language = {en},
  school = {University of Alberta},
  type = {{{PhD}}}
}

@article{beaubouef_computer_2008,
  title = {{{COMPUTER SCIENCE}}: {{STUDENT MYTHS AND MISCONCEPTIONS}}},
  author = {Beaubouef, T and McDowell, P},
  year = {2008},
  pages = {6},
  abstract = {This paper discusses common myths and misconceptions about the field of computer science. It addresses and attempts to dispel these notions in an effort to provide prospective computer science students and the general public with a more realistic view of the field.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Beaubouef and McDowell - 2008 - COMPUTER SCIENCE STUDENT MYTHS AND MISCONCEPTIONS.pdf},
  language = {en}
}

@article{belzner_monte_2017,
  title = {Monte {{Carlo Action Programming}}},
  author = {Belzner, Lenz},
  year = {2017},
  month = feb,
  abstract = {This paper proposes Monte Carlo Action Programming, a programming language framework for autonomous systems that act in large probabilistic state spaces with high branching factors. It comprises formal syntax and semantics of a nondeterministic action programming language. The language is interpreted stochastically via Monte Carlo Tree Search. Effectiveness of the approach is shown empirically.},
  archivePrefix = {arXiv},
  eprint = {1702.08441},
  eprinttype = {arxiv},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Belzner - 2017 - Monte Carlo Action Programming.pdf},
  journal = {arXiv:1702.08441 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Programming Languages},
  language = {en},
  primaryClass = {cs}
}

@inproceedings{benbassat_evomcts_2013,
  title = {{{EvoMCTS}}: {{Enhancing MCTS}}-Based Players through Genetic Programming},
  shorttitle = {{{EvoMCTS}}},
  booktitle = {2013 {{IEEE Conference}} on {{Computational Inteligence}} in {{Games}} ({{CIG}})},
  author = {Benbassat, Amit and Sipper, Moshe},
  year = {2013},
  month = aug,
  pages = {1--8},
  publisher = {{IEEE}},
  address = {{Niagara Falls, ON, Canada}},
  doi = {10.1109/CIG.2013.6633631},
  abstract = {We present EvoMCTS, a genetic programming method for enhancing level of play in games. Our work focuses on the zero-sum, deterministic, perfect-information board game of Reversi. Expanding on our previous work on evolving board-state evaluation functions for alpha-beta search algorithm variants, we now evolve evaluation functions that augment the MTCS algorithm. We use strongly typed genetic programming, explicitly defined introns, and a selective directional crossover method. Our system regularly evolves players that outperform MCTS players that use the same amount of search. Our results prove scalable and EvoMCTS players whose search is increased offline still outperform MCTS counterparts. To demonstrate the generality of our method we apply EvoMCTS successfully to the game of Dodgem.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Benbassat and Sipper - 2013 - EvoMCTS Enhancing MCTS-based players through gene.pdf},
  isbn = {978-1-4673-5311-3 978-1-4673-5308-3},
  language = {en}
}

@inproceedings{beume_intelligent_2008,
  title = {Intelligent Anti-Grouping in Real-Time Strategy Games},
  booktitle = {2008 {{IEEE Symposium On Computational Intelligence}} and {{Games}}},
  author = {Beume, Nicola and Hein, Tobias and Naujoks, Boris and Piatkowski, Nico and Preuss, Mike and Wessing, Simon},
  year = {2008},
  month = dec,
  pages = {63--70},
  publisher = {{IEEE}},
  address = {{Perth, Australia}},
  doi = {10.1109/CIG.2008.5035622},
  abstract = {Assembling suitable groups of fighting units to combat incoming enemy groups is a tactical necessity in realtime strategy (RTS) games. Furthermore it heavily influences future strategic decisions like unit building. Here, we demonstrate how to efficiently (offline) solve the problem of finding matches for the current enemy group(s) based on self-organizing maps (SOMs), powered by a simple evolutionary algorithm. The concept is implemented and thoroughly experimentally investigated in the RTS game Glest. We show that the offline learning is reliable and can be sped up considerably by employing a very simple substitute objective function instead of game simulations, making it a nearly universal, simple, and transparent technique.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\VYNTDX3S\\Beume et al. - 2008 - Intelligent anti-grouping in real-time strategy ga.pdf},
  isbn = {978-1-4244-2973-8},
  language = {en}
}

@incollection{bi_human-side_2016,
  title = {Human-{{Side Strategies}} in the {{Werewolf Game Against}} the {{Stealth Werewolf Strategy}}},
  booktitle = {Computers and {{Games}}},
  author = {Bi, Xiaoheng and Tanaka, Tetsuro},
  year = {2016},
  volume = {10068},
  pages = {93--102},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-50935-8_9},
  abstract = {The werewolf game contains unique features, such as persuasion and deception, which are not included in games that have been previously studied in AI research. Studying the werewolf game could be one of the next challenging targets for AI research. In this paper, we concentrate on a werewolf-side strategy called the ``stealth werewolf'' strategy. With this strategy, each of the werewolf-side players behaves like a villager, and the player does not pretend to have a special role. Even though the strategy is thought to be suboptimal, so far this has not been proved. In this paper, we limit the human-side strategies such that the seer reveals his/her role on the first day and the bodyguard never reveals his/her role. So, the advantage of the werewolves in determining the player to be eliminated by vote is nullified. We calculated the {$\epsilon$}-Nash equilibrium of strategies for both sides under this limitation. The solution shows that the winning rates of the human-side are more than half when the number of werewolves is assigned as in common play. Since it is thought to be fair and interesting for the winning rate to stay near 50\%, the result suggests that the ``stealth werewolf'' strategy is not a good strategy for werewolf-side players. Furthermore, the result also suggests that there exist unusual actions in the strategies that result in an {$\epsilon$}-Nash equilibrium.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2016\\13 Human-Side Strategies in the Werewolf Game Against the Stealth Werewolf Strategy.pdf},
  isbn = {978-3-319-50934-1 978-3-319-50935-8},
  language = {en}
}

@article{bonet_planning_2001,
  title = {Planning as Heuristic Search},
  author = {Bonet, Blai and Geffner, H{\'e}ctor},
  year = {2001},
  month = jun,
  volume = {129},
  pages = {5--33},
  issn = {00043702},
  doi = {10.1016/S0004-3702(01)00108-4},
  abstract = {In the AIPS98 Planning Contest, the HSP planner showed that heuristic search planners can be competitive with state-of-the-art Graphplan and SAT planners. Heuristic search planners like HSP transform planning problems into problems of heuristic search by automatically extracting heuristics from Strips encodings. They differ from specialized problem solvers such as those developed for the 24-Puzzle and Rubik's Cube in that they use a general declarative language for stating problems and a general mechanism for extracting heuristics from these representations.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\M7THA78Y\\Bonet and Geffner - 2001 - Planning as heuristic search.pdf},
  journal = {Artificial Intelligence},
  language = {en},
  number = {1-2}
}

@incollection{bonnet_havannah_2014,
  title = {Havannah and {{TwixT}} Are {{PSPACE}}-Complete},
  booktitle = {Computers and {{Games}}},
  author = {Bonnet, {\'E}douard and Jamain, Florian and Saffidine, Abdallah},
  year = {2014},
  volume = {8427},
  pages = {175--186},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-09165-5_15},
  abstract = {Numerous popular abstract strategy games ranging from Hex and Havannah via TwixT and Slither to Lines of Action belong to the class of connection games. Still, very few complexity results on such games have been obtained since Hex was proved pspace-complete in the early 1980s.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2013\\19 Havannah and TwixT are PSPACE-complete.pdf},
  isbn = {978-3-319-09164-8 978-3-319-09165-5},
  language = {en}
}

@incollection{bonnet_nash_2016,
  title = {Nash {{Equilibrium}} in {{Mastermind}}},
  booktitle = {Computers and {{Games}}},
  author = {Bonnet, Fran{\c c}ois and Viennot, Simon},
  year = {2016},
  volume = {10068},
  pages = {115--128},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-50935-8_11},
  abstract = {Mastermind is a famous two-player deduction game. A Codemaker chooses a secret code and a Codebreaker tries to guess this secret code in as few guesses as possible, with feedback information after each guess. Many existing works have computed optimal worst-case and average-case strategies of the Codebreaker, assuming that the Codemaker chooses the secret code uniformly at random. However, the Codemaker can freely choose any distribution probability on the secret codes. An optimal strategy in this more general setting is known as a Nash Equilibrium. In this research, we compute such a Nash Equilibrium for all instances of Mastermind up to the most classical instance of 4 pegs and 6 colors, showing that the uniform distribution is not always the best choice for the Codemaker. We also show the direct relation between Nash Equilibrium computations and computations of worst-case and average-case strategies.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2016\\15 Nash Equilibrium in Mastermind.pdf},
  isbn = {978-3-319-50934-1 978-3-319-50935-8},
  language = {en}
}

@book{bourg_ai_2004,
  title = {{{AI}} for Game Developers},
  author = {Bourg, David M. and Seemann, Glenn},
  year = {2004},
  edition = {1st ed},
  publisher = {{O'Reilly}},
  address = {{Sebastopol, CA}},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\3KXXIJK9\\Bourg and Seemann - 2004 - AI for game developers.pdf},
  isbn = {978-0-596-00555-9},
  keywords = {Artificial intelligence,Computer games,Design,Programming,Video games},
  language = {en},
  lccn = {QA76.76.C672 B68 2004}
}

@incollection{bourki_scalability_2011,
  title = {Scalability and {{Parallelization}} of {{Monte}}-{{Carlo Tree Search}}},
  booktitle = {Computers and {{Games}}},
  author = {Bourki, Amine and Chaslot, Guillaume and Coulm, Matthieu and Danjean, Vincent and Doghmen, Hassen and Hoock, Jean-Baptiste and H{\'e}rault, Thomas and Rimmel, Arpad and Teytaud, Fabien and Teytaud, Olivier and Vayssi{\`e}re, Paul and Yu, Ziqin},
  year = {2011},
  volume = {6515},
  pages = {48--58},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-17928-0_5},
  abstract = {Monte-Carlo Tree Search is now a well established algorithm, in games and beyond. We analyze its scalability, and in particular its limitations, and the implications in terms of parallelization, in particular for our program MoGo but also for our Havannah program Shakti. In particular, we get a good efficiency for the parallel versions, both for multicore machines and for message-passing machines, but in spite of promising results in self-play there are situations for which increasing the time per move does not solve anything, and therefore parallelization is not the solution either. Nonetheless, for problems on which the Monte-Carlo part is less biased than in Go, parallelization should be very efficient even without shared memory.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Bourki et al. - 2011 - Scalability and Parallelization of Monte-Carlo Tre.pdf},
  isbn = {978-3-642-17927-3 978-3-642-17928-0},
  language = {en}
}

@incollection{bourki_scalability_2011-1,
  title = {Scalability and {{Parallelization}} of {{Monte}}-{{Carlo Tree Search}}},
  booktitle = {Computers and {{Games}}},
  author = {Bourki, Amine and Chaslot, Guillaume and Coulm, Matthieu and Danjean, Vincent and Doghmen, Hassen and Hoock, Jean-Baptiste and H{\'e}rault, Thomas and Rimmel, Arpad and Teytaud, Fabien and Teytaud, Olivier and Vayssi{\`e}re, Paul and Yu, Ziqin},
  year = {2011},
  volume = {6515},
  pages = {48--58},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-17928-0_5},
  abstract = {Monte-Carlo Tree Search is now a well established algorithm, in games and beyond. We analyze its scalability, and in particular its limitations and the implications in terms of parallelization. We focus on our Go program MoGo and our Havannah program Shakti. We use multicore machines and message-passing machines. For both games and on both type of machines we achieve adequate efficiency for the parallel version. However, in spite of promising results in self-play there are situations for which increasing the time per move does not solve anything. Therefore parallelization is not a solution to all our problems. Nonetheless, for problems where the Monte-Carlo part is less biased than in the game of Go, parallelization should be quite efficient, even without shared memory.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2010\\10 Scalability and Parallelization of Monte-Carlo Tree Search.pdf},
  isbn = {978-3-642-17927-3 978-3-642-17928-0},
  language = {en}
}

@inproceedings{bouzy_move-pruning_2006,
  title = {Move-{{Pruning Techniques}} for {{Monte}}-{{Carlo Go}}},
  booktitle = {Advances in {{Computer Games}}},
  author = {Bouzy, Bruno},
  editor = {{van den Herik}, H. Jaap and Hsu, Shun-Chin and Hsu, Tsan-sheng and Donkers, H. H. L. M. (Jeroen)},
  year = {2006},
  pages = {104--119},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/11922155_8},
  abstract = {Progressive Pruning (PP) is employed in the Monte-Carlo Go-playing program Indigo. For each candidate move, PP launches random games starting with this move. The goal of PP is: (1) to gather statistics on moves, and (2) to prune moves statistically inferior to the best one [7]. This papers yields two new pruning techniques: Miai Pruning (MP) and Set Pruning (SP). In MP the second move of the random games is selected at random among the set of candidate moves. SP consists in gathering statistics about two sets of moves, good and bad, and it prunes the latter when statistically inferior to the former. Both enhancements clearly speed up the process of selecting a move on 9\texttimes{}9 boards, and MP improves slightly the playing level. Scaling up MP to 19\texttimes{}19 boards results in a 30\% speed-up enhancement and in a four-point improvement on average.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Bouzy - 2006 - Move-Pruning Techniques for Monte-Carlo Go.pdf;C\:\\Users\\aesou\\Zotero\\storage\\558LF3I6\\Bouzy - 2006 - Move-Pruning Techniques for Monte-Carlo Go.pdf},
  isbn = {978-3-540-48889-7},
  keywords = {Candidate Move,Computer Game,Good Move,Pruning Technique,Relative Speed},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@misc{brown_can_2020,
  title = {Can Real-Time Strategy Come Back from the Brink of Death?},
  author = {Brown, Fraser},
  year = {2020},
  month = jan,
  abstract = {The prognosis is grim.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\XFX8XGAR\\can-real-time-strategy-come-back-from-the-brink-of-death.html},
  journal = {PC Gamer},
  language = {en-US}
}

@misc{brown_history_2018,
  title = {The History of the Strategy Game},
  author = {Brown, Fraser},
  year = {2018},
  month = dec,
  abstract = {The origin and evolution of one of PC gaming's quintessential genres.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\L3BR3UKR\\the-history-of-the-strategy-game.html},
  journal = {PC Gamer},
  language = {en-US}
}

@incollection{browne_class_2016,
  title = {A {{Class Grammar}} for {{General Games}}},
  booktitle = {Computers and {{Games}}},
  author = {Browne, Cameron},
  year = {2016},
  volume = {10068},
  pages = {167--182},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-50935-8_16},
  abstract = {While there exist a variety of game description languages (GDLs) for modeling various classes of games, these are aimed at game playing rather than the more particular needs of game design. This paper describes a new approach to general game modeling that arose from this need. A class grammar is automatically generated from a given library of source code, from the constructors and associated parameters found along its class hierarchy, to give a context-free grammar that provides access to the underlying code while hiding its implementation details.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2016\\20 A Class Grammar for General Games.pdf},
  isbn = {978-3-319-50934-1 978-3-319-50935-8},
  language = {en}
}

@article{browne_survey_2012,
  title = {A {{Survey}} of {{Monte Carlo Tree Search Methods}}},
  author = {Browne, Cameron B. and Powley, Edward and Whitehouse, Daniel and Lucas, Simon M. and Cowling, Peter I. and Rohlfshagen, Philipp and Tavener, Stephen and Perez, Diego and Samothrakis, Spyridon and Colton, Simon},
  year = {2012},
  month = mar,
  volume = {4},
  pages = {1--49},
  issn = {1943-068X, 1943-0698},
  doi = {10.1109/TCIAIG.2012.2186810},
  abstract = {Monte Carlo Tree Search (MCTS) is a recently proposed search method that combines the precision of tree search with the generality of random sampling. It has received considerable interest due to its spectacular success in the difficult problem of computer Go, but has also proved beneficial in a range of other domains. This paper is a survey of the literature to date, intended to provide a snapshot of the state of the art after the first five years of MCTS research. We outline the core algorithm's derivation, impart some structure on the many variations and enhancements that have been proposed, and summarise the results from the key game and non-game domains to which MCTS methods have been applied. A number of open research questions indicate that the field is ripe for future work.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\F2URAFL2\\Browne et al. - 2012 - A Survey of Monte Carlo Tree Search Methods.pdf},
  journal = {IEEE Transactions on Computational Intelligence and AI in Games},
  language = {en},
  number = {1}
}

@book{buckland_ai_2002,
  title = {{{AI}} Techniques for Game Programming},
  author = {Buckland, Mat},
  year = {2002},
  publisher = {{Premier Press}},
  address = {{Cincinnati, Ohio}},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\29ASCRJK\\Buckland - 2002 - AI techniques for game programming.pdf},
  isbn = {978-1-931841-08-5},
  keywords = {Artificial intelligence,Computer games,Programming},
  language = {en},
  lccn = {QA76.76.C672 B83 2002},
  series = {The {{Premier}} Press Game Development Series}
}

@book{buckland_ai_2002-1,
  title = {{{AI}} Techniques for Game Programming},
  author = {Buckland, Mat},
  year = {2002},
  publisher = {{Premier Press}},
  address = {{Cincinnati, Ohio}},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\I6BSRAIS\\Buckland - 2002 - AI techniques for game programming 2.pdf;C\:\\Users\\aesou\\Zotero\\storage\\QQ8ZCRNL\\Mat Buckland - AI Techniques for Game Programming.pdf},
  isbn = {978-1-931841-08-5},
  language = {English}
}

@book{buckland_programming_2005,
  title = {Programming Game {{AI}} by Example},
  author = {Buckland, Mat},
  year = {2005},
  publisher = {{Wordware Pub}},
  address = {{Plano, Texas}},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\FUY5GMHJ\\Buckland - 2005 - Programming game AI by example.pdf},
  isbn = {978-1-55622-078-4},
  keywords = {Computer games,Computer graphics,Design,Programming},
  language = {en},
  lccn = {QA76.76.C672 B85 2005}
}

@book{buckland_programming_2005-1,
  title = {Programming Game {{AI}} by Example},
  author = {Buckland, Mat},
  year = {2005},
  publisher = {{Wordware Pub}},
  address = {{Plano, Texas}},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\XX3UPGG5\\Buckland - 2005 - Programming game AI by example 2.pdf},
  isbn = {978-1-55622-078-4},
  keywords = {Computer games,Computer graphics,Design,Programming},
  language = {en},
  lccn = {QA76.76.C672 B85 2005}
}

@inproceedings{burch_automatic_2011,
  title = {Automatic {{Move Pruning}} in {{General Single}}-{{Player Games}}},
  booktitle = {Fourth {{Annual Symposium}} on {{Combinatorial Search}}},
  author = {Burch, Neil and Holte, Robert C.},
  year = {2011},
  month = jul,
  abstract = {Move pruning is a low-overhead technique for reducing the size of a depth first search tree. The existing algorithm for automatically discovering move pruning information is restricted to games where all moves can be applied to every state. This paper demonstrates an algorithm which handles a general class of single player games. It gives experimental results for our technique, demonstrating both the applicability to a range of games, and the reduction in search tree size. We also provide some conditions under which move pruning is safe, and when it may interfere with other search reduction techniques.},
  copyright = {Authors who publish a paper in this conference agree to the following terms:     Author(s) agree to transfer their copyrights in their article/paper to the Association for the Advancement of Artificial Intelligence (AAAI), in order to deal with future requests for reprints, translations, anthologies, reproductions, excerpts, and other publications. This grant will include, without limitation, the entire copyright in the article/paper in all countries of the world, including all renewals, extensions, and reversions thereof, whether such rights current exist or hereafter come into effect, and also the exclusive right to create electronic versions of the article/paper, to the extent that such right is not subsumed under copyright.   The author(s) warrants that they are the sole author and owner of the copyright in the above article/paper, except for those portions shown to be in quotations; that the article/paper is original throughout; and that the undersigned right to make the grants set forth above is complete and unencumbered   The author(s) agree that if anyone brings any claim or action alleging facts that, if true, constitute a breach of any of the foregoing warranties, the author(s) will hold harmless and indemnify AAAI, their grantees, their licensees, and their distributors against any liability, whether under judgment, decree, or compromise, and any legal fees and expenses arising out of that claim or actions, and the undersigned will cooperate fully in any defense AAAI may make to such claim or action. Moreover, the undersigned agrees to cooperate in any claim or other action seeking to protect or enforce any right the undersigned has granted to AAAI in the article/paper. If any such claim or action fails because of facts that constitute a breach of any of the foregoing warranties, the undersigned agrees to reimburse whomever brings such claim or action for expenses and attorneys\&rsquo; fees incurred therein.   Author(s) retain all proprietary rights other than copyright (such as patent rights).   Author(s) may make personal reuse of all or portions of the above article/paper in other works of their own authorship.   Author(s) may reproduce, or have reproduced, their article/paper for the author\&rsquo;s personal use, or for company use provided that AAAI copyright and the source are indicated, and that the copies are not used in a way that implies AAAI endorsement of a product or service of an employer, and that the copies per se are not offered for sale. The foregoing right shall not permit the posting of the article/paper in electronic or digital form on any computer network, except by the author or the author\&rsquo;s employer, and then only on the author\&rsquo;s or the employer\&rsquo;s own web page or ftp site. Such web page or ftp site, in addition to the aforementioned requirements of this Paragraph, must provide an electronic reference or link back to the AAAI electronic server, and shall not post other AAAI copyrighted materials not of the author\&rsquo;s or the employer\&rsquo;s creation (including tables of contents with links to other papers) without AAAI\&rsquo;s written permission.   Author(s) may make limited distribution of all or portions of their article/paper prior to publication.   In the case of work performed under U.S. Government contract, AAAI grants the U.S. Government royalty-free permission to reproduce all or portions of the above article/paper, and to authorize others to do so, for U.S. Government purposes.   In the event the above article/paper is not accepted and published by AAAI, or is withdrawn by the author(s) before acceptance by AAAI, this agreement becomes null and void.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\W3EP3HZB\\Burch and Holte - 2011 - Automatic Move Pruning in General Single-Player Ga.pdf;C\:\\Users\\aesou\\Zotero\\storage\\PF9W5U6R\\4012.html},
  language = {en}
}

@inproceedings{burch_automatic_2012,
  title = {Automatic {{Move Pruning Revisited}}},
  booktitle = {Fifth {{Annual Symposium}} on {{Combinatorial Search}}},
  author = {Burch, Neil and Holte, Robert C.},
  year = {2012},
  publisher = {{AAAI Press}},
  address = {{Niagara Falls, Canada}},
  abstract = {In this paper we show that the move pruning method we presented at SoCS last year sometimes prunes all the least-cost paths from one state to another. We present two examples exhibiting this erroneous behaviour\textendash{}a simple, artificial example and a slightly more complex example that arose in last year's experiments. We then formally prove that a simple modification to our move pruning method makes it ``safe'', i.e., it will never prune all the least-cost paths between a pair of states. Finally, we present the results of rerunning last year's experiments with this provably safe version of move pruning and show that last year's main conclusions still hold, namely: (1) in domains where there are short redundant sequences move pruning produces substantial speedups in depth-first search, and (2) in domains where the only short redundant sequences are 2-cycles, move pruning is faster than parent pruning by a factor of two or more.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Burch and Holte - 2012 - Automatic Move Pruning Revisited.pdf;C\:\\Users\\aesou\\Zotero\\storage\\U8C8S8RE\\Burch and Holte - Automatic Move Pruning Revisited.pdf;C\:\\Users\\aesou\\Zotero\\storage\\HJIP7BJD\\summary.html}
}

@inproceedings{buro_call_2004,
  title = {Call for {{AI}} Research in {{RTS}} Games},
  booktitle = {Proceedings of the {{AAAI Workshop}} on {{AI}} in {{Games}}},
  author = {Buro, Michael},
  year = {2004},
  pages = {139--141},
  publisher = {{AAAI Press}},
  abstract = {This position paper discusses AI challenges in the area of real\textendash{}time strategy games and presents a research agenda aimed at improving AI performance in these popular multi\textendash{} player computer games. RTS Games and AI Research Real\textendash{}time strategy (RTS) games such as Blizzard Entertainment's Starcraft (tm) and Warcraft (tm) series form a large and growing part of the multi\textendash{}billion dollar computer games industry. In these games several players fight over resources, which are scattered over a terrain, by first setting up economies, building armies, and ultimately trying to eliminate all enemy units and buildings. The current AI performance in commercial RTS games is poor. The main reasons why the AI performance in RTS games is lagging behind developments},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\GF6R46KM\\Buro - 2004 - Call for AI research in RTS games.pdf;C\:\\Users\\aesou\\Zotero\\storage\\325QBXBL\\summary.html}
}

@inproceedings{buro_orts_2003,
  title = {{{ORTS}}: {{A Hack}}-{{Free RTS Game Environment}}},
  shorttitle = {{{ORTS}}},
  booktitle = {Computers and {{Games}}},
  author = {Buro, Michael},
  editor = {Schaeffer, Jonathan and M{\"u}ller, Martin and Bj{\"o}rnsson, Yngvi},
  year = {2003},
  pages = {280--291},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-40031-8_19},
  abstract = {This paper presents a novel approach to Real-Time-Strategy (RTS) gaming which allows human players as well as machines to compete in a hack-free environment. The main idea is to replace popular but inherently insecure client-side game simulations by a secure server-side game simulation. Only visible parts of the game state are sent to the respective clients. Client-side hacking is therefore impossible and players are free to choose any client software they please. We discuss performance issues arising from server-side simulation and present ORTS \textendash{} an open RTS game toolkit. This software package provides efficient C++ implementations for 2D object motion and collision detection, visibility computation, and incremental server-client data synchronization, as well as connectivity to the Generic Game Server (GGS). It is therefore well suited as a platform for RTS related A.I. research.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Buro - 2003 - ORTS A Hack-Free RTS Game Environment.pdf},
  isbn = {978-3-540-40031-8},
  keywords = {Client Software,Collision Time,Game Simulation,Game State,Information Hiding},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{buro_real-time_2003,
  title = {Real-Time Strategy Gaines: A New {{AI}} Research Challenge},
  shorttitle = {Real-Time Strategy Gaines},
  booktitle = {{{IJCAI}} 2003},
  author = {Buro, Michael},
  year = {2003},
  abstract = {This poster motivates AI research in the area of real-time strategy (RTS) games and describes the current status of a project whose goals are to implement an RTS game programming environment and to build AIs that eventually can outperform human experts in this challenging and popular domain. 1 Real-Time Strategy Games Commercial computer games are a growing part of the entertainment industry and simulations are a critical aspect of modern military training. The two fields have much in common, cross-fertilize, and are driving real-time AI research [Herz and Macedonia, 2002]. With the advent of fast personal computers, simulation-based games have become very popular. Today, these games constitute a multi-billion dollar enterprise. Examples are sports games and real-time strategy games. The common elements of simulation games are severe time constraints and a strong demand of real-time AI which must be capable of solving real-world decision tasks quickly and satisfactorily. Popular simulation games are therefore ideal test applications for real-time AI research. Real-Time-Strategy (RTS) games such as the millionsellers Starcraft by Blizzard Entertainment and Age of Empires by Ensemble Studios can be viewed as simplified military simulations. Several players struggle over resources scattered over a 2D terrain by setting up an economy, building armies, and guiding them into battle in real-time. RTS games offer a large variety of fundamental AI research problems, unlike other game genres studied by the AI community so far: \textbullet{} Resource management. Players start off by gathering local resources to build up defenses and attack forces, to upgrade weaponry, and to climb up the technology tree. Proper resource management is a vital part of any successful strategy. \textbullet{} Decision making under uncertainty. Initially, players are not aware of the enemies' base locations and intentions. They have to gather intelligence by sending out scouts. If no information is available yet, the players must form plausible hypotheses and act accordingly. \textbullet{} Spatial and temporal reasoning. Static and dynamic terrain analysis as well as understanding temporal relations of actions is of utmost importance in RTS games and yet, current game AIs largely ignore these issues and fall victim to simple common-sense reasoning [Forbus et al., 2002]. \textbullet{} Collaboration. In RTS games groups of players can join forces and intelligence. How to coordinate actions effectively by communication among the parties is a challenging research problem. \textbullet{} Opponent modeling, Learning. One of the biggest shortcomings of most (RTS) game AI systems is their inability to learn from experience. Human players only need a couple of games to spot opponents' weaknesses and to exploit them in upcoming games. Current machine learning approaches in this area are inadequate. \textbullet{} Adversarial real-time planning. In fine-grained simulations, agents cannot afford to think in terms of micro actions. Instead, abstractions have to be found which allow a machine to conduct forward searches in a manageable abstract space and to translate found solutions back. Because the environment is also dynamic, hostile, and smart adversarial realtime planning approaches need to be investigated. Playing RTS games is challenging. Even more challenging is the creation of autonomous real-time systems capable of outperforming human experts in this domain. Because search space abstraction, real-time planning, and temporal and spatial reasoning are central to many other problems, the scope of applications seems endless. One example is highperformance combat simulators which are in large demand for training military personnel today and wil l become the core of automated battlefield decision-support systems of tomorrow, [von der Lippe et al., 1999] predicts that 20\% of the US armed forces wil l be robotic by 2015. 2 An RTS Game Programming Environment The lack of AI interfaces even in upcoming RTS game titles makes it hard to conduct real-time AI research in this area and to compare the strength of the resulting Al systems with that of human experts. In order to solve this problem we launched an open source RTS game programming project [Buro, 2002] with the following goals: \textbullet{} Building a hack-free server-client RTS game system. At the core of the system is a simulator to which players connect via UNIX sockets (Fig. 1). The unique system features include: server-side simulation which only sends visible information to clients thereby rendering common maprevealing client hacks useless and an open message protocol that allows Al researchers and players to connect whatever client software they like. \textbullet{} Sparking competition among players and researchers. Popular games in which human players still have the upper hand are ideal test-domains for AI research. Unlike the confined GUIs of commercial RTS games, our open design allows the construction of hybrid AI systems in which the human general is aided by Al modules of growing capabilities. Competitive game playing on an open Internet RTS game server is therefore likely to improve AI performance and ergonomic GUI design. \textbullet{} Applying planning and machine learning techniques to RTS games. Classic game Al methods such as alpha-},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\CW2W9D47\\Buro - 2003 - Real-time strategy gaines a new AI research chall.pdf},
  keywords = {Age of Empires,Artificial intelligence (video games),Automated planning and scheduling,Client (computing),Game programming,Game server,Graphical user interface,Heuristic,High- and low-level,Human factors and ergonomics,Integrated development environment,List of video games in development,Machine learning,Nouvelle AI,Open design,PC game,Personal computer,Real-time computing,Real-time locating system,Real-time transcription,Server (computing),Server-side,Simulation,Software release life cycle,Spatial–temporal reasoning}
}

@article{buro_rts_2003,
  title = {{{RTS Games}} as {{Test}}\textendash{{Bed}} for {{Real}}\textendash{{Time AI Research}}},
  author = {Buro, Michael and Furtak, Timothy},
  year = {2003},
  pages = {20},
  abstract = {This article motivates AI research in the area of real\textendash{}time strategy (RTS) games and describes the road\textendash{}map and the current status of the ORTS project whose goals are to implement an RTS game programming environment and to build AI systems that eventually can outperform human experts in this popular and challenging domain.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\UUJV9KPV\\Buro and Furtak - 2003 - RTS Games as Test–Bed for Real–Time AI Research.pdf},
  language = {en}
}

@inproceedings{buro_rts_2003-1,
  title = {{{RTS Games}} and {{Real}}\textendash{{Time AI Research}}},
  booktitle = {{{IJCAI}}},
  author = {Buro, Michael and Furtak, Timothy M},
  year = {2003},
  pages = {8},
  abstract = {This article1 motivates AI research in the area of real\textendash{}time strategy (RTS) games and describes the current status of the ORTS project whose goals are to implement an RTS game programming environment and to build AI systems that eventually can outperform human experts in this popular and challenging domain.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\S958MJ5P\\Buro and Furtak - 2003 - RTS Games and Real–Time AI Research.pdf},
  language = {en}
}

@article{buro_second_2007,
  title = {The {{Second Annual Real}}-{{Time Strategy Game AI Competition}}},
  author = {Buro, Michael and Lanctot, Marc and Orsten, Sterling},
  year = {2007},
  pages = {5},
  abstract = {Real-time strategy (RTS) games are complex decision domains which require quick reactions as well as strategic planning and adversarial reasoning. In this paper we describe the second RTS game AI tournament, which was held in June 2007, the competition entries that participated, and plans for next year's tournament.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\EJCWEJEZ\\Buro et al. - 2007 - The Second Annual Real-Time Strategy Game AI Compe.pdf},
  language = {en}
}

@inproceedings{cadena_fuzzy_2011,
  title = {Fuzzy {{Case}}-{{Based Reasoning}} for {{Managing Strategic}} and {{Tactical Reasoning}} in {{StarCraft}}},
  booktitle = {Advances in {{Artificial Intelligence}}},
  author = {Cadena, Pedro and Garrido, Leonardo},
  editor = {Batyrshin, Ildar and Sidorov, Grigori},
  year = {2011},
  pages = {113--124},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-25324-9_10},
  abstract = {We present the combination of Fuzzy sets and Case-Based Reasoning (FCBR) to deal with strategic and tactical management in the real-time strategy environment of StarCraft. Case-based reasoning is a problem solving AI approach that uses past experience to deal with actual problems. Fuzzy set theory is used in case representation to provide a characterization of imprecise and uncertain information. The results revealed that our system can successfully reason about strategies and tactics, defeating the built-in AI of StarCraft. The principal conclusion was that FCBR can reason with abstract information and a large space of actions. Moreover, the resulting system shows its potential to incorporate human knowledge and can effectively adapt to varying conditions of the map.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Cadena and Garrido - 2011 - Fuzzy Case-Based Reasoning for Managing Strategic .pdf},
  isbn = {978-3-642-25324-9},
  keywords = {Case-based reasoning,fuzzy sets,real-time strategy,tactical management},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@techreport{cao_enhanced_2019,
  title = {Enhanced {{Forward Pruning}}},
  author = {Cao, Qingyang},
  year = {2019},
  month = aug,
  address = {{Heidelberg University}},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Cao - 2019 - Enhanced Forward Pruning.pdf}
}

@incollection{cardon_markovian_2011,
  title = {A {{Markovian Process Modeling}} for {{Pickomino}}},
  booktitle = {Computers and {{Games}}},
  author = {Cardon, St{\'e}phane and {Chetcuti-Sperandio}, Nathalie and Delorme, Fabien and Lagrue, Sylvain},
  year = {2011},
  volume = {6515},
  pages = {199--210},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-17928-0_19},
  abstract = {This paper deals with a nondeterministic game based on die rolls and on the "stop or continue" principle: Pickomino. During his1 turn, each participant has to make the best decisions first to choose the dice to keep, then to choose between continuing or stopping depending on the previous rolls and on the available resources. Markov Decision Processes (MDPs) offer the formal framework to model this game. The two main problems are first to determine the set of states, then to compute the transition probabilities.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2010\\24 A Markovian Process Modeling for Pickomino.pdf},
  isbn = {978-3-642-17927-3 978-3-642-17928-0},
  language = {en}
}

@article{cazenave_monte_2012,
  title = {Monte {{Carlo Beam Search}}},
  author = {Cazenave, Tristan},
  year = {2012},
  month = mar,
  volume = {4},
  pages = {68--72},
  issn = {1943-068X, 1943-0698},
  doi = {10.1109/TCIAIG.2011.2180723},
  abstract = {Monte-Carlo Tree Search is state of the art for multiple games and for solving puzzles such as Morpion Solitaire. Nested Monte-Carlo Search is a Monte-Carlo Tree Search algorithm that works well for solving puzzles. We propose to enhance Nested Monte-Carlo Search with Beam Search. We test the algorithm on Morpion Solitaire. Thanks to beam search, our program has been able to match the record score of 82 moves. Monte-Carlo Beam Search achieves better scores in less time than Nested Monte-Carlo Search alone.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Cazenave - 2012 - Monte Carlo Beam Search.pdf},
  journal = {IEEE Transactions on Computational Intelligence and AI in Games},
  language = {en},
  number = {1}
}

@inproceedings{cazenave_multi-player_2008,
  title = {Multi-Player {{Go}}},
  booktitle = {Computers and {{Games}}},
  author = {Cazenave, Tristan},
  editor = {{van den Herik}, H. Jaap and Xu, Xinhe and Ma, Zongmin and Winands, Mark H. M.},
  year = {2008},
  pages = {50--59},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-87608-3_5},
  abstract = {Multi-player Go is Go played with more than two colors. Monte-Carlo Tree Search is an adequate algorithm to program the game of Go with two players. We address the application of Monte-Carlo Tree Search to multi-player Go.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2008\\10 Multi-player Go.pdf},
  isbn = {978-3-540-87608-3},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{cazenave_parallel_2008,
  title = {A {{Parallel Monte}}-{{Carlo Tree Search Algorithm}}},
  booktitle = {Computers and {{Games}}},
  author = {Cazenave, Tristan and Jouandeau, Nicolas},
  year = {2008},
  pages = {72--80},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-87608-3_7},
  abstract = {Monte-Carlo Tree Search is a powerful paradigm for the game of Go. In this contribution we present a parallel Master-Slave algorithm for Monte-Carlo Tree Search and test it on a network of computers using various configurations: from 12,500 to 100,000 playouts, from 1 to 64 slaves, and from 1 to 16 computers. On our own architecture we obtain a speedup of 14 for 16 slaves. With a single slave and five seconds per move our algorithm scores 40.5\% against GNU Go, with sixteen slaves and five seconds per move it scores 70.5\%. At the end we give the potential speedups of our algorithm for various playout times.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Cazenave and Jouandeau - 2008 - A Parallel Monte-Carlo Tree Search Algorithm.pdf;C\:\\Users\\aesou\\Zotero\\storage\\4V67CFGP\\Cazenave and Jouandeau - 2008 - A Parallel Monte-Carlo Tree Search Algorithm.pdf},
  isbn = {978-3-540-87608-3},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@incollection{cazenave_parallel_2008-1,
  title = {A {{Parallel Monte}}-{{Carlo Tree Search Algorithm}}},
  booktitle = {Computers and {{Games}}},
  author = {Cazenave, Tristan and Jouandeau, Nicolas},
  year = {2008},
  volume = {5131},
  pages = {72--80},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-87608-3_7},
  abstract = {Monte-Carlo Tree Search is a powerful paradigm for the game of Go. In this contribution we present a parallel Master-Slave algorithm for Monte-Carlo Tree Search and test it on a network of computers using various configurations: from 12,500 to 100,000 playouts, from 1 to 64 slaves, and from 1 to 16 computers. On our own architecture we obtain a speedup of 14 for 16 slaves. With a single slave and five seconds per move our algorithm scores 40.5\% against GNU Go, with sixteen slaves and five seconds per move it scores 70.5\%. At the end we give the potential speedups of our algorithm for various playout times.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2008\\12 A Parallel Monte-Carlo Tree Search Algorithm.pdf},
  isbn = {978-3-540-87607-6 978-3-540-87608-3},
  language = {en}
}

@incollection{cazenave_score_2011,
  title = {Score {{Bounded Monte}}-{{Carlo Tree Search}}},
  booktitle = {Computers and {{Games}}},
  author = {Cazenave, Tristan and Saffidine, Abdallah},
  year = {2011},
  volume = {6515},
  pages = {93--104},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-17928-0_9},
  abstract = {Monte-Carlo Tree Search (MCTS) is a successful algorithm used in many state of the art game engines. We propose to improve a MCTS solver when a game has more than two outcomes. It is for example the case in games that can end in draw positions. In this case it improves significantly a MCTS solver to take into account bounds on the possible scores of a node in order to select the nodes to explore. We apply our algorithm to solving Seki in the game of Go and to Connect Four.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2010\\14 Score Bounded Monte-Carlo Tree Search.pdf},
  isbn = {978-3-642-17927-3 978-3-642-17928-0},
  language = {en}
}

@article{cazenave_sequential_2015,
  title = {Sequential {{Halving Applied}} to {{Trees}}},
  author = {Cazenave, Tristan},
  year = {2015},
  volume = {7},
  pages = {102--105},
  doi = {10.1109/TCIAIG.2014.2317737},
  abstract = {Monte Carlo tree search (MCTS) is state of the art for multiple games and problems. The base algorithm currently used for MCTS is UCT. We propose an alternative MCTS algorithm: sequential halving applied to Trees (SHOT). It has multiple advantages over UCT: it spends less time in the tree, it uses less memory, it is parameter free, at equal time settings it beats UCT for a complex combinatorial game and it can be efficiently parallelized.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\ENZP3UGD\\Cazenave - 2015 - Sequential Halving Applied to Trees.pdf},
  journal = {IEEE Transactions on Computational Intelligence and AI in Games},
  keywords = {Algorithm,Division by two,Monte Carlo method,Monte Carlo tree search,Multi-core processor,Parallel computing}
}

@article{cazenave_sequential_2015-1,
  title = {Sequential {{Halving Applied}} to {{Trees}}},
  author = {Cazenave, Tristan},
  year = {2015},
  month = mar,
  volume = {7},
  pages = {102--105},
  issn = {1943-068X, 1943-0698},
  doi = {10.1109/TCIAIG.2014.2317737},
  abstract = {Monte Carlo Tree Search (MCTS) is state of the art for multiple games and problems. The base algorithm currently used for Monte Carlo Tree Search is UCT. We propose an alternative Monte Carlo Tree Search algorithm: Sequential Halving applied to Trees (SHOT). It has multiple advantages over UCT: it spends less time in the tree, it uses less memory, it is parameter free, at equal time settings it beats UCT for a complex combinatorial game and it can be efficiently parallelized.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\R8WST3BT\\Cazenave - 2015 - Sequential Halving Applied to Trees.pdf},
  journal = {IEEE Transactions on Computational Intelligence and AI in Games},
  language = {en},
  number = {1}
}

@incollection{cazenave_virtual_2007,
  title = {Virtual {{Global Search}}: {{Application}} to 9\texttimes{}9 {{Go}}},
  shorttitle = {Virtual {{Global Search}}},
  booktitle = {Computers and {{Games}}},
  author = {Cazenave, Tristan},
  year = {2007},
  volume = {4630},
  pages = {62--71},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-75538-8_6},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2006\\11 Virtual Global Search Application to 9×9 Go.pdf},
  isbn = {978-3-540-75537-1 978-3-540-75538-8},
  language = {en}
}

@inproceedings{certicky_current_2017,
  title = {The {{Current State}} of {{StarCraft AI Competitions}} and {{Bots}}},
  author = {Certick{\'y}, Michal},
  year = {2017},
  abstract = {Real-Time Strategy (RTS) games have become an increasingly popular test-bed for modern artificial intelligence techniques. With this rise in popularity has come the creation of several annual competitions, in which AI agents (bots) play the full game of StarCraft: Broodwar by Blizzard Entertainment. The three major annual StarCraft AI Competitions are the Student StarCraft AI Tournament (SSCAIT), the Computational Intelligence in Games (CIG) competition, and the Artificial Intelligence and Interactive Digital Entertainment (AIIDE) competition. In this paper we will give an overview of the current state of these competitions, and the bots that compete in them.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\FIR7VXHG\\Certický - 2017 - The Current State of StarCraft AI Competitions and.pdf},
  keywords = {AI winter,Apolipoprotein A-I,Artificial intelligence,Blizzard,Computation,Computational intelligence,Inscriptiones Graecae,Real-time transcription,StarCraft,StarCraft: Brood War,Student Health Services,Testbed}
}

@misc{chalk_deepminds_2019,
  title = {{{DeepMind}}'s {{AlphaStar AI}} Is Now a Full-Blown {{StarCraft}} 2 {{Grandmaster}}},
  author = {Chalk, Andy},
  year = {2019},
  month = oct,
  abstract = {The machine is better than 99.8 percent of players on Battle.net, 'under professionally approved conditions.'},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\R2HBA8F9\\deepminds-alphastar-ai-is-now-a-full-blown-starcraft-2-grandmaster.html},
  journal = {PC Gamer},
  language = {en-US}
}

@inproceedings{chang_quantitative_2014,
  title = {A {{Quantitative Study}} of 2 x 4 {{Chinese Dark Chess}}},
  booktitle = {Computers and {{Games}}},
  author = {Chang, Hung-Jui and Hsu, Tsan-sheng},
  editor = {{van den Herik}, H. Jaap and Iida, Hiroyuki and Plaat, Aske},
  year = {2014},
  pages = {151--162},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-09165-5_13},
  abstract = {In this paper, we study Chinese dark chess (CDC), a popular 2-player imperfect information game that is a variation of Chinese chess played on a 2\texttimes{}42\texttimes{}42\textbackslash{}times 4 game board. The 2\texttimes{}42\texttimes{}42\textbackslash{}times 4 version is solved by computing the exact value of each board position for all possible fair piece combinations. The results of the experiments demonstrate that the initial arrangement of the pieces and the place to reveal the first piece are the most important factors to affect the outcome of a game.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2013\\17 A Quantitative Study of 2 4 Chinese Dark Chess.pdf},
  isbn = {978-3-319-09165-5},
  keywords = {Board Position,Game Board,Indexing Function,Normal Version,Occupied Cell},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@incollection{chaslot_parallel_2008,
  title = {Parallel {{Monte}}-{{Carlo Tree Search}}},
  booktitle = {Computers and {{Games}}},
  author = {Chaslot, Guillaume M. J. -B. and Winands, Mark H. M. and {van den Herik}, H. Jaap},
  year = {2008},
  volume = {5131},
  pages = {60--71},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-87608-3_6},
  abstract = {Monte-Carlo Tree Search (MCTS) is a new best-first search method that started a revolution in the field of Computer Go. Parallelizing MCTS is an important way to increase the strength of any Go program. In this article, we discuss three parallelization methods for MCTS: leaf parallelization, root parallelization, and tree parallelization. To be effective tree parallelization requires two techniques: adequately handling of (1) local mutexes and (2) virtual loss. Experiments in 13 \texttimes{} 13 Go reveal that in the program Mango root parallelization may lead to the best results for a specific time setting and specific program parameters. However, as soon as the selection mechanism is able to handle more adequately the balance of exploitation and exploration, tree parallelization should have attention too and could become a second choice for parallelizing MCTS. Preliminary experiments on the smaller 9 \texttimes{} 9 board provide promising prospects for tree parallelization.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Chaslot et al. - 2008 - Parallel Monte-Carlo Tree Search.pdf},
  isbn = {978-3-540-87607-6 978-3-540-87608-3},
  language = {en}
}

@incollection{chen_abstracting_2007,
  title = {Abstracting {{Knowledge}} from {{Annotated Chinese}}-{{Chess Game Records}}},
  booktitle = {Computers and {{Games}}},
  author = {Chen, Bo-Nian and Liu, Pangfang and Hsu, Shun-Chin and Hsu, Tsan-sheng},
  year = {2007},
  volume = {4630},
  pages = {100--111},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-75538-8_9},
  abstract = {Text data used to annotate game records is written in a special form of natural language. Instead of using classical natural language processing techniques, with emphasis on efficiency and accuracy, we use a novel pattern matching algorithm (see Figure 1) to parse the annotations of game records. The process uses a processing algorithm and a keyword set. The latter is separated into several classes, each of which is mapped to a position value. When the sentences of annotations are recognized by one of the grammar rules or several consistent rules, the score-related element of the current state is mapped to a class of position values.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2006\\14 Abstracting Knowledge from Annotated Chinese-Chess Game Records.pdf},
  isbn = {978-3-540-75537-1 978-3-540-75538-8},
  language = {en}
}

@incollection{chen_automatic_2014,
  title = {Automatic {{Generation}} of {{Opening Books}} for {{Dark Chess}}},
  booktitle = {Computers and {{Games}}},
  author = {Chen, Bo-Nian and Hsu, Tsan-sheng},
  year = {2014},
  volume = {8427},
  pages = {221--232},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-09165-5_19},
  abstract = {Playing the opening game of dark chess well is a challenge that depends to a large extent on probability. There are no known studies or published results for opening games, although automatic generation of opening books for many games is a popular research topic. Some researchers collect masters' games to compile an opening book; while others automatically collect computer-played games as their opening books. However, it is difficult to obtain a strong opening book via the above strategies because few games played by masters have been recorded. In this paper, we propose a policy-oriented search strategy to build automatically a selective opening book that is helpful in practical game playing. The constructed book provides positive feedback for computer programs that play dark chess.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2013\\23 Automatic Generation of Opening Books for Dark Chess.pdf},
  isbn = {978-3-319-09164-8 978-3-319-09165-5},
  language = {en}
}

@inproceedings{chen_combinatorial_2013,
  title = {Combinatorial {{Multi}}-{{Armed Bandit}}:  {{General Framework}}, {{Results}} and {{Applications}}},
  booktitle = {Proceedings of the 30 Th {{International Conference}} on {{Ma}}- Chine {{Learning}}, {{Atlanta}}, {{Georgia}}, {{USA}}, 2013. {{JMLR}}: {{W}}\&{{CP}} Volume 28.},
  author = {Chen, Wei and Wang, Yajun and Yuan, Yang},
  year = {2013},
  pages = {9},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\VTFFN5QI\\Chen et al. - Combinatorial Multi-Armed Bandit  General Framewo.pdf},
  language = {en}
}

@incollection{chen_fast_2008,
  title = {A {{Fast Indexing Method}} for {{Monte}}-{{Carlo Go}}},
  booktitle = {Computers and {{Games}}},
  author = {Chen, Keh-Hsun and Du, Dawei and Zhang, Peigang},
  year = {2008},
  volume = {5131},
  pages = {92--101},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-87608-3_9},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2008\\14 A Fast Indexing Method for Monte-Carlo Go.pdf},
  isbn = {978-3-540-87607-6 978-3-540-87608-3},
  language = {en}
}

@incollection{chen_knowledge_2008,
  title = {Knowledge {{Inferencing}} on {{Chinese Chess Endgames}}},
  booktitle = {Computers and {{Games}}},
  author = {Chen, Bo-Nian and Liu, Pangfeng and Hsu, Shun-Chin and Hsu, Tsan-sheng},
  year = {2008},
  volume = {5131},
  pages = {180--191},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-87608-3_17},
  abstract = {Several Chinese chess programs exhibit grandmaster playing skills in the opening and middle game. However, in the endgame phase, the programs only apply ordinal search algorithms; hence, they usually cannot exchange pieces correctly. Some researchers use retrograde algorithms to solve endgames with a limited number of attack pieces, but this approach is often not practical in a real tournament. In a grandmaster game, the players typically perform a sequence of material exchanges between the middle game and the endgame, so computer programs can be useful. However, there are about 185 million possible combinations of material in Chinese chess, and many hard endgames are inconclusive even to human masters. To resolve this problem, we propose a novel strategy that applies a knowledge-inferencing algorithm on a sufficiently small database to determine whether endgames with a certain combination of material are advantageous to a player. Our experimental results show that the performance of the algorithm is good and reliable. Therefore, building a large knowledge database of material combinations is recommended.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2008\\22 Knowledge Inferencing on Chinese Chess Endgames.pdf},
  isbn = {978-3-540-87607-6 978-3-540-87608-3},
  language = {en}
}

@incollection{chen_knowledge_2011,
  title = {Knowledge {{Abstraction}} in {{Chinese Chess Endgame Databases}}},
  booktitle = {Computers and {{Games}}},
  author = {Chen, Bo-Nian and Liu, Pangfeng and Hsu, Shun-Chin and Hsu, Tsan-sheng},
  year = {2011},
  volume = {6515},
  pages = {176--187},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-17928-0_17},
  abstract = {Retrograde analysis is a well known approach to construct endgame databases. However, the size of the endgame databases are too large to be loaded into the main memory of a computer during tournaments. In this paper, a novel knowledge abstraction strategy is proposed to compress endgame databases. The goal is to obtain succinct knowledge for practical endgames. A specialized goal-oriented search method is described and applied on the important endgame KRKNMM. The method of combining a search algorithm with a small size of knowledge is used to handle endgame positions up to a limited depth, but with a high degree of correctness.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2010\\22 Knowledge Abstraction in Chinese Chess Endgame Databases.pdf},
  isbn = {978-3-642-17927-3 978-3-642-17928-0},
  language = {en}
}

@inproceedings{chen_new_2007,
  title = {A {{New Heuristic Search Algorithm}} for {{Capturing Problems}} in {{Go}}},
  booktitle = {Computers and {{Games}}},
  author = {Chen, Keh-Hsun and Zhang, Peigang},
  editor = {{van den Herik}, H. Jaap and Ciancarini, Paolo and Donkers, H. H. L. M. (Jeroen)},
  year = {2007},
  pages = {26--36},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-75538-8_3},
  abstract = {We propose a highly selective heuristic search algorithm for capturing problems in Go. This iterative deepening search works on the crucial chain in which the prey block is located. The algorithm starts using three order liberties of the chain as the basis of the position evaluation, the value is then adjusted by the presence of few liberty-surrounding opponent blocks. The algorithm solved most capturing problems in Kano's four volumes of graded Go problems. Moreover, it is fast enough to be used by Go programs in real time.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2006\\8 A New Heuristic Search Algorithm for Capturing Problems in Go.pdf},
  isbn = {978-3-540-75538-8},
  keywords = {Adjacent Block,Hash Code,Hash Table,Search Depth,Search Tree},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{chu_contextual_2011,
  title = {Contextual {{Bandits}} with {{Linear Payoff Functions}}},
  booktitle = {Proceedings of the {{Fourteenth International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Chu, Wei and Li, Lihong and Reyzin, Lev and Schapire, Robert},
  year = {2011},
  month = jun,
  pages = {208--214},
  issn = {1938-7228},
  abstract = {In this paper we study the contextual bandit problem (also known as the multi-armed bandit problem with expert advice) for linear payoff functions.  For T rounds, K actions, and d dimensional featu...},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\PSG2QDJN\\Chu et al. - 2011 - Contextual Bandits with Linear Payoff Functions.pdf;C\:\\Users\\aesou\\Zotero\\storage\\YZW4B3BJ\\chu11a.html},
  language = {en}
}

@inproceedings{chung_monte_2005,
  title = {Monte {{Carlo Planning}} in {{RTS Games}}},
  booktitle = {{{IEEE Symposium}} on {{Computational Intelligence}} and {{Games}}},
  author = {Chung, Michael and Buro, Michael and Schaeffer, Jonathan},
  year = {2005},
  pages = {8},
  abstract = {Monte Carlo simulations have been successfully used in classic turn\textendash{}based games such as backgammon, bridge, poker, and Scrabble. In this paper, we apply the ideas to the problem of planning in games with
imperfect information, stochasticity, and simultaneous
moves. The domain we consider is real\textendash{}time strategy
games. We present a framework \textemdash{} MCPlan \textemdash{} for Monte
Carlo planning, identify its performance parameters, and
analyze the results of an implementation in a capture\textendash{}
the\textendash{}flag game.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Chung et al. - 2005 - Monte Carlo Planning in RTS Games.pdf},
  language = {en}
}

@inproceedings{churchill_fast_2012,
  title = {Fast {{Heuristic Search}} for {{RTS Game Combat Scenarios}}},
  booktitle = {Proceedings, {{The Eighth AAAI Conference}} on {{Artificial Intelligence}} and {{Interactive Digital Entertainment}}},
  author = {Churchill, David and Saffidine, Abdallah and Buro, Michael},
  year = {2012},
  pages = {112--117},
  abstract = {Heuristic search has been very successful in abstract game domains such as Chess and Go. In video games, however, adoption has been slow due to the fact that state and move spaces are much larger, real-time constraints are harsher, and constraints on computational resources are tighter. In this paper we present a fast search method \textemdash{} Alpha-Beta search for durative moves \textemdash{} that can defeat commonly used AI scripts in RTS game combat scenarios of up to 8 vs. 8 units running on a single core in under 5ms per search episode. This performance is achieved by using standard search enhancements such as transposition tables and iterative deepening, and novel usage of combat AI scripts for sorting moves and state evaluation via playouts. We also present evidence that commonly used combat scripts are highly exploitable \textemdash{} opening the door for a promising line of research on opponent combat modelling.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\JKYRY585\\Churchill et al. - 2012 - Fast Heuristic Search for RTS Game Combat Scenario.pdf},
  language = {en}
}

@phdthesis{churchill_heuristic_2016,
  title = {Heuristic {{Search Techniques}} for {{Real}}-{{Time Strategy Games}}},
  author = {Churchill, David},
  year = {2016},
  abstract = {Real-time strategy (RTS) video games are known for being one of the most complex and strategic games for humans to play. With a unique combination of strategic thinking and dexterous mouse movements, RTS games make for
a very intense and exciting game-play experience. In recent years the games AI research community has been increasingly drawn to the field of RTS AI research due to its challenging sub-problems and harsh real-time computing
constraints. With the rise of e-Sports and professional human RTS gaming, the games industry has become very interested in AI techniques for helping design, balance, and test such complex games. In this thesis we will introduce and motivate the main topics of RTS AI research, and identify which areas need the most improvement. We then describe the RTS AI research we have conducted, which consists of five major contributions. First, our depth-first branch and bound build-order search algorithm, which is capable of producing professional human-quality build-orders in real-time, and was the first heuristic search algorithm to be used on-line in a starcraft AI competition setting. Second, our RTS combat simulation system: SparCraft, which contains three
new algorithms for unit micromanagement (Alpha-Beta Considering Durations (ABCD), UCT Considering Durations (UCT-CD) and Portfolio Greedy Search), each outperforming the previous state-of-the-art. Third, Hierarchical Portfolio Search for games with large search spaces, which was implemented as the AI system for the online strategy game Prismata by Lunarch Studios.
Fourth, UAlbertaBot: our starcraft AI bot which won the 2013 AIIDE starcraft AI competition. And fifth: our tournament managing software which is currently used in all three major starcraft AI competitions.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\ZAABN4TA\\Churchill - 2016 - Heuristic Search Techniques for Real-Time Strategy.pdf},
  language = {en},
  school = {University of Alberta},
  type = {{{PhD}}}
}

@inproceedings{churchill_hierarchical_2015,
  title = {Hierarchical {{Portfolio Search}}: {{Prismata}}'s {{Robust AI Architecture}} for {{Games}} with {{Large Search Spaces}}},
  booktitle = {The {{Eleventh AAAI Conference}} on {{Artificial Intelligence}} and {{Interactive Digital Entertainment}} ({{AIIDE}}-15)},
  author = {Churchill, David and Buro, Michael},
  year = {2015},
  pages = {7},
  abstract = {Online strategy video games offer several unique challenges to the field of AI research. Due to their large state and action spaces, existing search algorithms have difficulties in making strategically strong decisions. Additionally, the nature of competitive on-line video games adds the requirement that game designers be able to tweak game properties regularly when strategic imbalances are found. This means that an AI system for a game like this needs to be robust to such changes and less reliant on expert knowledge. This paper makes two main contributions to advancing the state of the art for AI in modern strategy video games which have large state and action spaces. The first is a novel method for performing hierarchical search using a portfolio of algorithms to reduce the search space while maintaining strong action candidates. The second contribution is an overall AI architecture for strategy video games using this portfolio search method. The proposed methods are used as the AI system for Prismata, an online turn-based strategy game by Lunarch Studios. This system is evaluated using three experiments: on-line play vs. human players, off-line AI tournaments to test the relative strengths of the AI bots, and a survey to determine user satisfaction of the system so far. Our result show that this system achieves a skill level in the top 25\% of human players on the ranked ladder, can be modified quickly to create different difficulty settings, is robust to changes in game unit properties, and creates an overall AI experience which is user rated more enjoyable than those currently found in similar video games.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\49MU5TZW\\Churchill and Buro - 2015 - Hierarchical Portfolio Search Prismata's Robust A.pdf},
  language = {en}
}

@inproceedings{churchill_portfolio_2013,
  title = {Portfolio Greedy Search and Simulation for Large-Scale Combat in Starcraft},
  booktitle = {2013 {{IEEE Conference}} on {{Computational Inteligence}} in {{Games}} ({{CIG}})},
  author = {Churchill, David and Buro, Michael},
  year = {2013},
  month = aug,
  publisher = {{IEEE}},
  address = {{Niagara Falls, ON, Canada}},
  doi = {10.1109/CIG.2013.6633643},
  abstract = {Real-time strategy video games have proven to be a very challenging area for applications of artificial intelligence research. With their vast state and action spaces and real-time constraints, existing AI solutions have been shown to be too slow, or only able to be applied to small problem sets, while human players still dominate RTS AI systems. This paper makes three contributions to advancing the state of AI for popular commercial RTS game combat, which can consist of battles of dozens of units. First, we present an efficient system for modelling abstract RTS combat called SparCraft, which can perform millions of unit actions per second and visualize them. We then present a modification of the UCT algorithm capable of performing search in games with simultaneous and durative actions. Finally, a novel greedy search algorithm called Portfolio Greedy Search is presented which uses hill climbing and accurate playout-based evaluations to efficiently search even the largest combat scenarios. We demonstrate that Portfolio Greedy Search outperforms state of the art Alpha-Beta and UCT search methods for large StarCraft combat scenarios of up to 50 vs. 50 units under real-time search constraints of 40 ms per search episode.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\2XZSIVMN\\Churchill and Buro - 2013 - Portfolio greedy search and simulation for large-s.pdf},
  isbn = {978-1-4673-5311-3 978-1-4673-5308-3},
  language = {en}
}

@inproceedings{churchill_robust_2019,
  title = {Robust {{Continuous Build}}-{{Order Optimization}} in {{StarCraft}}},
  booktitle = {{{IEEEE Conference}} on {{Games}} 2019},
  author = {Churchill, David and Buro, Michael and Kelly, Richard},
  year = {2019},
  pages = {8},
  abstract = {To solve complex real-world planning problems it is often beneficial to decompose tasks into high-level and low-level components and optimize actions separately. Examples of such modularization include car navigation (a high-level path planning problem) and obstacle avoidance (a lower-level control problem), and decomposing playing policies in modern video games into strategic (``macro'') and tactical (``micro'') components. In realtime strategy (RTS) video games such as StarCraft, players face decision problems ranging from economic development to maneuvering units in combat situations. A popular strategy employed in building AI agents for complex games like StarCraft is to use this strategy of task decomposition to construct separate AI systems for each of these sub-problems, combining them to form a complete game-playing agent. Existing AI systems for such games often contain build-order planning systems that attempt to minimize makespans for constructing specific sets of units, which are typically decided by hand-coded human expert knowledge rules. Drawbacks of this approach include the human expert effort involved in constructing these rules, as well as a lack of online adaptability to unforeseen circumstances, which can lead to brittle behavior that can be exploited by more advanced opponents. In this paper we introduce a new robust build-order planning system for RTS games that automatically produces build-orders which optimize unit compositions toward strategic game concepts (such as total unit firepower), without the need for specific unit goals. When incorporated into an existing StarCraft AI agent in a real tournament setting, it outperformed the previous state-of-the-art planning system which relied on human expert knowledge rules for deciding unit compositions.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Churchill et al. - 2019 - Robust Continuous Build-Order Optimization in Star.pdf},
  language = {en}
}

@incollection{churchill_starcraft_2016,
  title = {{{StarCraft Bots}} and {{Competitions}}},
  booktitle = {Encyclopedia of {{Computer Graphics}} and {{Games}}},
  author = {Churchill, David and Preuss, Mike and Richoux, Florian and Synnaeve, Gabriel and Uriarte, Alberto and Onta{\~n}n{\'o}n, Santiago and {\v C}ertick{\'y}, Michal},
  editor = {Lee, Newton},
  year = {2016},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-08234-9_18-1},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\SKDH9G6J\\Churchill et al. - 2016 - StarCraft Bots and Competitions.pdf},
  isbn = {978-3-319-08234-9},
  language = {en}
}

@incollection{cincotti_counting_2007,
  title = {Counting the {{Number}} of {{Three}}-{{Player Partizan Cold Games}}},
  booktitle = {Computers and {{Games}}},
  author = {Cincotti, Alessandro},
  year = {2007},
  volume = {4630},
  pages = {181--189},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-75538-8_16},
  abstract = {We give upper and lower bounds on S3[n] equal to the number of three-player partizan cold games born by day n. In particular, we give an upper bound of O(S2[n]3) and a lower bound of {$\Omega$}(S2[n]) where S2[n] is the number of surreal numbers born by day n.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2006\\21 Counting the Number of Three-Player Partizan Cold Games.pdf},
  isbn = {978-3-540-75537-1 978-3-540-75538-8},
  language = {en}
}

@incollection{cincotti_game_2008,
  title = {The {{Game}} of {{Synchronized Domineering}}},
  booktitle = {Computers and {{Games}}},
  author = {Cincotti, Alessandro and Iida, Hiroyuki},
  year = {2008},
  volume = {5131},
  pages = {241--251},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-87608-3_22},
  abstract = {In synchronized games players make their moves simultaneously rather than alternately. Synchronized Domineering is the synchronized version of Domineering, a classic two-player combinatorial game. We present the solutions for all the m \texttimes{} n boards with m {$\leq$} 6 and n {$\leq$} 6. Also, we give results for the n \texttimes{} 3 boards, n \texttimes{} 5 boards, and some partial results for the n \texttimes{} 2 boards. Future research is indicated.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2008\\27 The Game of Synchronized Domineering.pdf},
  isbn = {978-3-540-87607-6 978-3-540-87608-3},
  language = {en}
}

@incollection{cincotti_lattice_2011,
  title = {The {{Lattice Structure}} of {{Three}}-{{Player Games}}},
  booktitle = {Computers and {{Games}}},
  author = {Cincotti, Alessandro},
  year = {2011},
  volume = {6515},
  pages = {230--237},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-17928-0_21},
  abstract = {In combinatorial games, few results are known about the overall structure of three-player games. We prove that three-player games born by day d form a distributive lattice with respect to every partial order relation, but that the collection of all finite three-player games does not form a lattice.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2010\\26 The Lattice Structure of Three-Player Games.pdf},
  isbn = {978-3-642-17927-3 978-3-642-17928-0},
  language = {en}
}

@article{claypool_effect_2005,
  title = {The Effect of Latency on User Performance in {{Real}}-{{Time Strategy}} Games},
  author = {Claypool, Mark},
  year = {2005},
  month = sep,
  volume = {49},
  pages = {52--70},
  issn = {13891286},
  doi = {10.1016/j.comnet.2005.04.008},
  abstract = {Latency on the Internet is a well-known problem for interactive applications. The growth in interactive network games brings an increased importance in understanding the effects of latency on user performance. Classes of network games such as First Person Shooters (FPS) and Real-Time Strategy (RTS) differ in their user interaction model and hence susceptibility to latency. While previous work has measured the effects of latency on FPS games, there has been no systematic investigation of the effects of latency on RTS games. In this work, we design and conduct user studies that measure the impact of latency on user performance on three of the most popular RTS games. As a foundation for the research, we separated typical RTS user interactions into the basic components of explore, build and combat, and analyzed each individually. We find modest statistical correlations between user performance and latency for exploration, but very weak correlations for building and combat. Overall, the effect of even very high latency, while noticeable to users, has a negligible effect on the outcome of the game. We attribute this somewhat surprising result to the nature of RTS game play that clearly favors strategy over the real-time aspects.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\9YL5FBPF\\Claypool - 2005 - The effect of latency on user performance in Real-.pdf},
  journal = {Computer Networks},
  language = {en},
  number = {1}
}

@incollection{collette_symbolic_2007,
  title = {On the {{Symbolic Computation}} of the {{Hardest Configurations}} of the {{RUSH HOUR Game}}},
  booktitle = {Computers and {{Games}}},
  author = {Collette, S{\'e}bastien and Raskin, Jean-Fran{\c c}ois and Servais, Fr{\'e}d{\'e}ric},
  year = {2007},
  volume = {4630},
  pages = {220--233},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-75538-8_20},
  abstract = {Rush Hour is a sliding blocks game where blocks represent cars stuck in a traffic jam on a 6 \texttimes{} 6 board. The goal of the game is to allow one of the cars (the target car) to exit this traffic jam by moving the other cars out of its way. In this paper, we study the problem of finding difficult initial configurations for this game. An initial configuration is difficult if the number of car moves necessary to exit the target car is high. To solve the problem, we model the game in propositional logic and we apply symbolic model-checking techniques to study the huge graph of configurations that underlies the game. On the positive side, we show that this huge graph (containing 3.6 {$\cdot$} 1010 vertices) can be completely analyzed using symbolic model-checking techniques with reasonable computing resources. We have classified every possible initial configuration of the game according to the length of its shortest solution. On the negative side, we prove a general theorem that shows some limits of symbolic model-checking methods for board games. The result explains why some natural modeling of board games leads to the explosion of the size of symbolic data-structures.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2006\\25 On the Symbolic Computation of the Hardest Configurations of the RUSH HOUR Game.pdf},
  isbn = {978-3-540-75537-1 978-3-540-75538-8},
  language = {en}
}

@incollection{cook_human-computer_2011,
  title = {A {{Human}}-{{Computer Team Experiment}} for 9x9 {{Go}}},
  booktitle = {Computers and {{Games}}},
  author = {Cook, Darren},
  year = {2011},
  volume = {6515},
  pages = {145--155},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-17928-0_14},
  abstract = {Monte-Carlo Tree Search has given computer Go a significant boost in strength in the past few years, but progress seems to have slowed, and once again we have to ask ourselves how can computers make effective use of the ever-increasing computer power. In 2002, we started a human-computer team experiment with very long thinking times and no restrictions on the procedure, to see how strong such a team could be. We will introduce our experimental method and show the results so far.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2010\\19 A Human-Computer Team Experiment for 9x9 Go.pdf},
  isbn = {978-3-642-17927-3 978-3-642-17928-0},
  language = {en}
}

@book{cooper_computer_2015,
  title = {Computer Games and Software Engineering},
  author = {Cooper, Kendra M. L and Scacchi, Walt},
  year = {2015},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\H4K6KY3L\\Cooper and Scacchi - Computer Games and Software Engineering.pdf},
  isbn = {978-1-4822-2669-0},
  language = {English}
}

@article{coquelin_bandit_2007,
  title = {Bandit {{Algorithms}} for {{Tree Search}}},
  author = {Coquelin, Pierre-Arnaud and Munos, R{\'e}mi},
  year = {2007},
  month = mar,
  abstract = {Bandit based methods for tree search have recently gained popularity when applied to huge trees, e.g. in the game of go (Gelly et al., 2006). The UCT algorithm (Kocsis and Szepesvari, 2006), a tree search method based on Upper Confidence Bounds (UCB) (Auer et al., 2002), is believed to adapt locally to the effective smoothness of the tree. However, we show that UCT is too ``optimistic'' in some cases, leading to a regret O(exp(exp(D))) where D is the depth of the tree. We propose alternative bandit algorithms for tree search. First, a modification of UCT using a confidence sequence that scales exponentially with the horizon depth is proven to have a regret O(2\^D \textbackslash{}sqrt\{n\}), but does not adapt to possible smoothness in the tree. We then analyze Flat-UCB performed on the leaves and provide a finite regret bound with high probability. Then, we introduce a UCB-based Bandit Algorithm for Smooth Trees which takes into account actual smoothness of the rewards for performing efficient ``cuts'' of sub-optimal branches with high confidence. Finally, we present an incremental tree search version which applies when the full tree is too big (possibly infinite) to be entirely represented and show that with high probability, essentially only the optimal branches is indefinitely developed. We illustrate these methods on a global optimization problem of a Lipschitz function, given noisy data.},
  archivePrefix = {arXiv},
  eprint = {cs/0703062},
  eprinttype = {arxiv},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Coquelin and Munos - 2007 - Bandit Algorithms for Tree Search.pdf},
  journal = {arXiv:cs/0703062},
  keywords = {Computer Science - Machine Learning},
  language = {en}
}

@inproceedings{coulom_efficient_2006,
  title = {Efficient {{Selectivity}} and {{Backup Operators}} in {{Monte}}-{{Carlo Tree Search}}},
  booktitle = {5th {{International Conference}} on {{Computer}} and {{Games}}},
  author = {Coulom, R{\'e}mi},
  year = {2006},
  pages = {72--83},
  address = {{Turin, Italy.}},
  doi = {10.1007/978-3-540-75538-8_7},
  abstract = {Monte-Carlo evaluation consists in estimating a position by averaging the outcome of several random continuations, and can serve as an evaluation function at the leaves of a min-max tree. This paper presents a new framework to combine tree search with Monte-Carlo evaluation, that does not separate between a min-max phase and a MonteCarlo phase. Instead of backing-up the min-max value close to the root, and the average value at some depth, a more general backup operator is defined that progressively changes from averaging to min-max as the number of simulations grows. This approach provides a fine-grained control of the tree growth, at the level of individual simulations, and allows efficient selectivity methods. This algorithm was implemented in a 9 \texttimes{} 9 Go-playing program, Crazy Stone, that won the 10th KGS computer-Go tournament.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\HMI8QD78\\Coulom - 2006 - Efficient Selectivity and Backup Operators in Mont.pdf},
  isbn = {978-3-540-75537-1 978-3-540-75538-8},
  language = {en}
}

@incollection{coulom_efficient_2007,
  title = {Efficient {{Selectivity}} and {{Backup Operators}} in {{Monte}}-{{Carlo Tree Search}}},
  booktitle = {Computers and {{Games}}},
  author = {Coulom, R{\'e}mi},
  year = {2007},
  volume = {4630},
  pages = {72--83},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-75538-8_7},
  abstract = {A Monte-Carlo evaluation consists in estimating a position by averaging the outcome of several random continuations. The method can serve as an evaluation function at the leaves of a min-max tree. This paper presents a new framework to combine tree search with MonteCarlo evaluation, that does not separate between a min-max phase and a Monte-Carlo phase. Instead of backing-up the min-max value close to the root, and the average value at some depth, a more general backup operator is defined that progressively changes from averaging to minmax as the number of simulations grows. This approach provides a finegrained control of the tree growth, at the level of individual simulations, and allows efficient selectivity. The resulting algorithm was implemented in a 9 \texttimes{} 9 Go-playing program, Crazy Stone, that won the 10th KGS computer-Go tournament.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2006\\12 Efficient Selectivity and Backup Operators in Monte-Carlo Tree Search.pdf},
  isbn = {978-3-540-75537-1 978-3-540-75538-8},
  language = {en}
}

@incollection{coulom_whole-history_2008,
  title = {Whole-{{History Rating}}: {{A Bayesian Rating System}} for {{Players}} of {{Time}}-{{Varying Strength}}},
  shorttitle = {Whole-{{History Rating}}},
  booktitle = {Computers and {{Games}}},
  author = {Coulom, R{\'e}mi},
  year = {2008},
  volume = {5131},
  pages = {113--124},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-87608-3_11},
  abstract = {Whole-History Rating (WHR) is a new method to estimate the time-varying strengths of players involved in paired comparisons. Like many variations of the Elo rating system, the whole-history approach is based on the dynamic Bradley-Terry model. But, instead of using incremental approximations, WHR directly computes the exact maximum a posteriori over the whole rating history of all players. This additional accuracy comes at a higher computational cost than traditional methods, but computation is still fast enough to be easily applied in real time to large-scale game servers (a new game is added in less than 0.001 second). Experiments demonstrate that, in comparison to Elo, Glicko, TrueSkill, and decayed-history algorithms, WHR produces better predictions.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2008\\16 Whole-History Rating A Bayesian Rating System for Players of Time-Varying Strength.pdf},
  isbn = {978-3-540-87607-6 978-3-540-87608-3},
  language = {en}
}

@book{dagraca_practical_2017,
  title = {Practical Game {{AI}} Programming: Create Game {{AI}} and Implement Cutting Edge {{AI}} Algorithms from Scratch},
  shorttitle = {Practical Game {{AI}} Programming},
  author = {DaGra{\c c}a, Micael},
  year = {2017},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\GM2TUB5P\\DaGraça - 2017 - Practical game AI programming create game AI and .pdf},
  isbn = {978-1-78712-946-7},
  language = {en}
}

@book{dagraca_practical_2017-1,
  title = {Practical Game {{AI}} Programming Create Game {{AI}} and Implement Cutting Edge {{AI}} Algorithms from Scratch},
  author = {DaGra{\c c}a, Micael},
  year = {2017},
  publisher = {{Packt Publishing}},
  address = {{Birmingham, UK}},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\MKYW7I2V\\DaGraça - 2017 - Practical game AI programming create game AI and i.pdf},
  isbn = {978-1-78712-946-7},
  language = {en}
}

@book{dalmau_core_2003,
  title = {Core {{Techniques}} and {{Algorithms}} in {{Game Programming}}},
  author = {Dalmau, Daniel Sanchez-Crespo},
  year = {2003},
  month = sep,
  publisher = {{New Riders Games}},
  address = {{Indianapolis, Ind}},
  abstract = {To even try to keep pace with the rapid evolution of game development, you need a strong foundation in core programming techniques-not a hefty volume on one narrow topic or one that devotes itself to API-specific implementations. Finally, there's a guide that delivers! As a professor at the Spanish university that offered that country's first master's degree in video game creation, author Daniel Sanchez-Crespo recognizes that there's a core programming curriculum every game designer should be well versed in-and he's outlined it in these pages! By focusing on time-tested coding techniques-and providing code samples that use C++, and the OpenGL and DirectX APIs-Daniel has produced a guide whose shelf life will extend long beyond the latest industry trend. Code design, data structures, design patterns, AI, scripting engines, 3D pipelines, texture mapping, and more: They're all covered here-in clear, coherent fashion and with a focus on the essentials that will have you referring back to this volume for years to come.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\YIQX7WKP\\Dalmau - 2003 - Core Techniques and Algorithms in Game Programming.pdf},
  isbn = {978-0-13-102009-2},
  language = {English}
}

@inproceedings{david-tabibi_extended_2008,
  title = {Extended {{Null}}-{{Move Reductions}}},
  booktitle = {Computers and {{Games}}},
  author = {{David-Tabibi}, Omid and Netanyahu, Nathan S.},
  editor = {{van den Herik}, H. Jaap and Xu, Xinhe and Ma, Zongmin and Winands, Mark H. M.},
  year = {2008},
  pages = {205--216},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-87608-3_19},
  abstract = {In this paper we review the conventional versions of null-move pruning, and present our enhancements which allow for a deeper search with greater accuracy. While the conventional versions of null-move pruning use reduction values of R {$\leq$} 3, we use an aggressive reduction value of R = 4 within a verified adaptive configuration which maximizes the benefit from the more aggressive pruning, while limiting its tactical liabilities. Our experimental results using our grandmaster-level chess program, Falcon, show that our null-move reductions (NMR) outperform the conventional methods, with the tactical benefits of the deeper search dominating the deficiencies. Moreover, unlike standard null-move pruning, which fails badly in zugzwang positions, NMR is impervious to zugzwangs. Finally, the implementation of NMR in any program already using null-move pruning requires a modification of only a few lines of code.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\David-Tabibi and Netanyahu - 2008 - Extended Null-Move Reductions.pdf;C\:\\Users\\aesou\\Zotero\\storage\\HAZBQESM\\David-Tabibi and Netanyahu - 2008 - Extended Null-Move Reductions.pdf},
  isbn = {978-3-540-87608-3},
  keywords = {Aggressive Reduction,Conventional Version,Depth Reduction,Test Suite,Total Node Count},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@incollection{david-tabibi_extended_2008-1,
  title = {Extended {{Null}}-{{Move Reductions}}},
  booktitle = {Computers and {{Games}}},
  author = {{David-Tabibi}, Omid and Netanyahu, Nathan S.},
  year = {2008},
  volume = {5131},
  pages = {205--216},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-87608-3_19},
  abstract = {In this paper we review the conventional versions of nullmove pruning, and present our enhancements which allow for a deeper search with greater accuracy. While the conventional versions of nullmove pruning use reduction values of R {$\leq$} 3, we use an aggressive reduction value of R = 4 within a verified adaptive configuration which maximizes the benefit from the more aggressive pruning, while limiting its tactical liabilities. Our experimental results using our grandmasterlevel chess program, Falcon, show that our null-move reductions (NMR) outperform the conventional methods, with the tactical benefits of the deeper search dominating the deficiencies. Moreover, unlike standard null-move pruning, which fails badly in zugzwang positions, NMR is impervious to zugzwangs. Finally, the implementation of NMR in any program already using null-move pruning requires a modification of only a few lines of code.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2008\\24 Extended Null-Move Reductions.pdf},
  isbn = {978-3-540-87607-6 978-3-540-87608-3},
  language = {en}
}

@article{daylamani-zad_chain_2018,
  title = {Chain of Command in Autonomous Cooperative Agents for Battles in Real-Time Strategy Games},
  author = {{Daylamani-Zad}, Damon and Graham, Letitia B. and Paraskevopoulos, Ioannis Th.},
  year = {2018},
  month = sep,
  issn = {2197-9987, 2197-9995},
  doi = {10.1007/s40692-018-0119-8},
  abstract = {This paper investigates incorporating chain of command in swarm intelligence of honey bees to create groups of ranked co-operative autonomous agents for an RTS game in to create and re-enact battle simulations. The behaviour of the agents are based on the foraging and defensive behaviours of honey bees, adapted to a human environment. The chain of command is implemented using a hierarchical decision model. The groups consist of multiple model-based reflex agents, with individual blackboards for working memory, with a colony level blackboard to mimic the foraging patterns and include commands received from ranking agents. An agent architecture and environment are proposed that allows for creation of autonomous cooperative agents. The behaviour of agents is then evaluated both mathematically and empirically using an adaptation of anytime universal intelligence test and agent believability metric.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\DMTUUMFG\\Daylamani-Zad et al. - 2018 - Chain of command in autonomous cooperative agents .pdf},
  journal = {Journal of Computers in Education},
  language = {en}
}

@incollection{de_wet_gender_2007,
  title = {Gender and {{Cultural Differences}} ({{If Any}}!): {{South African School Children}} and {{Computer Games}}},
  shorttitle = {Gender and {{Cultural Differences}} ({{If Any}}!)},
  booktitle = {Computers and {{Games}}},
  author = {{de Wet}, Lizette and McDonald, Theo},
  year = {2007},
  volume = {4630},
  pages = {271--282},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-75538-8_24},
  abstract = {When studying computer games several factors come into play. The issue of gender inequality has been a topic of many research projects in the past. The issue of culture is still in its infancy. Previous research regarding gameplaying and gender issues seem to indicate that boys play more computer games than girls, that boys prefer more violent, action-oriented games in comparison to girls and that girls would prefer to play games with a feminine appeal. Intuitively it can be assumed that different cultures play different existing games at different frequencies. In this study grade ten school children (ages sixteen to seventeen) from one city in South Africa were questioned in order to establish if the same results hold true. The results indicate that there are no major differences in game playing between genders and cultures for this group. The conclusion is reached that especially with regard to gender the situation changed quite a bit over the past few years in comparison to research results found in the literature.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2006\\29 Gender and Cultural Differences (If Any!) South African School Children and Computer Games.pdf},
  isbn = {978-3-540-75537-1 978-3-540-75538-8},
  language = {en}
}

@inproceedings{dereszynski_learning_2011,
  title = {Learning {{Probabilistic Behavior Models}} in {{Real}}-{{Time Strategy Games}}},
  booktitle = {Proceedings of the {{Seventh AAAI Conference}} on {{Artificial Intelligence}} and {{Interactive Digital Entertainment}}},
  author = {Dereszynski, Ethan and Hostetler, Jesse and Fern, Alan and Dietterich, Tom and Hoang, Thao-Trang and Udarbe, Mark},
  year = {2011},
  pages = {6},
  abstract = {We study the problem of learning probabilistic models of high-level strategic behavior in the real-time strategy (RTS) game StarCraft. The models are automatically learned from sets of game logs and aim to capture the common strategic states and decision points that arise in those games. Unlike most work on behavior/strategy learning and prediction in RTS games, our data-centric approach is not biased by or limited to any set of preconceived strategic concepts. Further, since our behavior model is based on the well-developed and generic paradigm of hidden Markov models, it supports a variety of uses for the design of AI players and human assistants. For example, the learned models can be used to make probabilistic predictions of a player's future actions based on observations, to simulate possible future trajectories of a player, or to identify uncharacteristic or novel strategies in a game database. In addition, the learned qualitative structure of the model can be analyzed by humans in order to categorize common strategic elements. We demonstrate our approach by learning models from 331 expert-level games and provide both a qualitative and quantitative assessment of the learned model's utility.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\RWLADAY2\\Dereszynski et al. - 2011 - Learning Probabilistic Behavior Models in Real-Tim.pdf},
  language = {en}
}

@article{dodge_how_2017,
  title = {How the {{Experts Do It}}: {{Assessing}} and {{Explaining Agent Behaviors}} in {{Real}}-{{Time Strategy Games}}},
  shorttitle = {How the {{Experts Do It}}},
  author = {Dodge, Jonathan and Penney, Sean and Hilderbrand, Claudia and Anderson, Andrew and Burnett, Margaret},
  year = {2017},
  month = nov,
  abstract = {How should an AI-based explanation system explain an agent's complex behavior to ordinary end users who have no background in AI? Answering this question is an active research area, for if an AI-based explanation system could effectively explain intelligent agents' behavior, it could enable the end users to understand, assess, and appropriately trust (or distrust) the agents attempting to help them. To provide insights into this question, we turned to human expert explainers in the real-time strategy domain \textendash{} ``shoutcasters'' \textendash{} to understand (1) how they foraged in an evolving strategy game in real time, (2) how they assessed the players' behaviors, and (3) how they constructed pertinent and timely explanations out of their insights and delivered them to their audience. The results provided insights into shoutcasters' foraging strategies for gleaning information necessary to assess and explain the players; a characterization of the types of implicit questions shoutcasters answered; and implications for creating explanations by using the patterns and abstraction levels these human experts revealed.},
  archivePrefix = {arXiv},
  eprint = {1711.06953},
  eprinttype = {arxiv},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\768VQHYC\\Dodge et al. - 2017 - How the Experts Do It Assessing and Explaining Ag.pdf},
  journal = {arXiv:1711.06953 [cs]},
  keywords = {Computer Science - Human-Computer Interaction},
  language = {en},
  primaryClass = {cs}
}

@article{donninger_null_1993,
  title = {Null {{Move}} and {{Deep Search}}},
  author = {Donninger, Christian},
  year = {1993},
  month = jan,
  volume = {16},
  pages = {137--143},
  publisher = {{IOS Press}},
  issn = {1389-6911},
  doi = {10.3233/ICG-1993-16304},
  abstract = {This article describes in detail a selective-search heuristic which uses a null-move approach recursively. A variety of empirical data, ranging from tournament results against strong human players to special test positions, are presented. These resul},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Donninger - 1993 - Null Move and Deep Search.pdf;C\:\\Users\\aesou\\Zotero\\storage\\QWFWIGHQ\\icg16-3-04.html},
  journal = {ICGA Journal},
  language = {en},
  number = {3}
}

@article{dor_heuristic_2014,
  title = {The {{Heuristic Circle}} of {{Real}}-{{Time Strategy Process}}: {{A StarCraft}}: {{Brood War Case Study}}},
  shorttitle = {The {{Heuristic Circle}} of {{Real}}-{{Time Strategy Process}}},
  author = {Dor, Simon},
  year = {2014},
  month = aug,
  volume = {14},
  issn = {1604-7982},
  abstract = {This article aims to describe competitive playing experience in StarCraft: Brood War. Strategy is defined as a process using game plans (strategies) and game states. By using cognitive psychology works, as well as their applications to chess and in film studies, the goal of this article is to summarize cognitive and perceptive processes in the heuristic circle of real-time strategy process. This model is based on three levels of strategic plans (operational, mobilized and projected plans) as well as on three levels of game states in the player's mind (immediate, inferred and anticipated game states). This conceptualization of strategy as a process and its usefulness for the understanding of real-time strategy games is then illustrated by a specific StarCraft game session analysis [1].},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\NY2K55VS\\dor.html},
  journal = {Game Studies},
  keywords = {brood war,cognition,gameplay,perception,real-time strategy,schema,starcraft,strategy},
  number = {1}
}

@article{drugan_reinforcement_2019,
  title = {Reinforcement Learning versus Evolutionary Computation: {{A}} Survey on Hybrid Algorithms},
  shorttitle = {Reinforcement Learning versus Evolutionary Computation},
  author = {Drugan, Madalina M.},
  year = {2019},
  month = feb,
  volume = {44},
  pages = {228--246},
  issn = {2210-6502},
  doi = {10.1016/j.swevo.2018.03.011},
  abstract = {A variety of Reinforcement Learning (RL) techniques blends with one or more techniques from Evolutionary Computation (EC) resulting in hybrid methods classified according to their goal, new focus, and their component methodologies. We denote this class of hybrid algorithmic techniques as the evolutionary computation versus reinforcement learning (ECRL) paradigm. This overview considers the entire spectrum of algorithmic aspects and proposes a novel methodology that analyses the technical resemblances and differences in ECRL. Our design analyses the motivation for each ECRL paradigm, the underlying natural models, the sub-component algorithmic techniques, as well as the properties of their ensemble.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Drugan - 2019 - Reinforcement learning versus evolutionary computa.pdf;C\:\\Users\\aesou\\Zotero\\storage\\TN9ZDNRJ\\S2210650217302766.html},
  journal = {Swarm and Evolutionary Computation},
  keywords = {Evolutionary computation,Hybrid algorithms,Natural paradigms,Reinforcement learning,Survey},
  language = {en}
}

@inproceedings{dubey_comparing_2019,
  title = {Comparing {{Three Approaches}} to {{Micro}} in {{RTS Games}}},
  booktitle = {2019 {{IEEE Congress}} on {{Evolutionary Computation}} ({{CEC}})},
  author = {Dubey, Rahul and Louis, Sushil and Gajurel, Aavaas and Liu, Siming},
  year = {2019},
  month = jun,
  pages = {777--784},
  doi = {10.1109/CEC.2019.8790308},
  abstract = {We compare three promising approaches to micromanaging units in real-time strategy games. These approaches span the range from easily understandable meta-search, which uses genetic algorithms to search through the space of parameters of a human specified control algorithm to pure potential fields, which searches through a space of less human understandable potential field parameter values, to neuro-evolution of augmented topologies which evolves an opaque difficult to understand neural network. All three approaches use a two-objective pareto optimal fitness function that maximizes damage done to opponent units and minimizes damage received by friendly units. We first show that all three approaches can quickly evolve micro superior to the default AI for Starcraft 2, a popular real-time strategy game and research testbed. We then manually co-evolve micro against previously evolved micro to produce micro that plays well against good (gold level) human Starcraft 2 players. Furthermore, we can integrate the micro produced by different approaches to control a single group of units composed from multiple types. These results indicate that we may choose our approach based on our need to understand unit control behavior and thus provides another bridge to transferring research results from autonomous units in simulation games to autonomous agents (robots) in the real world.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Dubey et al. - 2019 - Comparing Three Approaches to Micro in RTS Games.pdf;C\:\\Users\\aesou\\Zotero\\storage\\IIG9FRWD\\8790308.html},
  keywords = {Artificial intelligence,augmented topologies,autonomous units,computer games,friendly units,Games,genetic algorithms,Genetic algorithms,good human Starcraft 2 players,human specified control algorithm,human understandable potential field parameter values,meta-search,Metasearch,micromanaging units,multi-objective optimization,NEAT,neural nets,neural network,Neural networks,neuro-evolution,opponent units,Pareto optimisation,Planning,pure potential fields,real-time strategy game,Real-time systems,RTS games,simulation games,two-objective pareto optimal fitness function,understandable meta-search,unit control behavior}
}

@article{dubey_evolutionary_2018,
  title = {Evolutionary {{Multi}}-Objective {{Optimization}} of {{Real}}-{{Time Strategy Micro}}},
  author = {Dubey, Rahul and Ghantous, Joseph and Louis, Sushil and Liu, Siming},
  year = {2018},
  month = mar,
  abstract = {We investigate an evolutionary multi-objective approach to good micro for real-time strategy games. Good micro helps a player win skirmishes and is one of the keys to developing better real-time strategy game play. In prior work, the same multi-objective approach of maximizing damage done while minimizing damage received was used to evolve micro for a group of ranged units versus a group of melee units. We extend this work to consider groups composed from two types of units. Specifically, this paper uses evolutionary multi-objective optimization to generate micro for one group composed from both ranged and melee units versus another group of ranged and melee units. Our micro behavior representation uses influence maps to represent enemy spatial information and potential fields generated from distance, health, and weapons cool down to guide unit movement. Experimental results indicate that our multiobjective approach leads to a Pareto front of diverse high-quality micro encapsulating multiple possible tactics. This range of micro provided by the Pareto front enables a human or AI player to trade-off among short term tactics that better suit the player's longer term strategy - for example, choosing to minimize friendly unit damage at the cost of only lightly damaging the enemy versus maximizing damage to the enemy units at the cost of increased damage to friendly units. We believe that our results indicate the usefulness of potential fields as a representation, and of evolutionary multi-objective optimization as an approach, for generating good micro.},
  archivePrefix = {arXiv},
  eprint = {1803.10316},
  eprinttype = {arxiv},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\646LF7PH\\Dubey et al. - 2018 - Evolutionary Multi-objective Optimization of Real-.pdf},
  journal = {arXiv:1803.10316 [cs]},
  keywords = {Computer Science - Neural and Evolutionary Computing},
  language = {en},
  primaryClass = {cs}
}

@inproceedings{dubey_evolutionary_2018-1,
  title = {Evolutionary {{Multi}}-Objective {{Optimization}} of {{Real}}-{{Time Strategy Micro}}},
  booktitle = {2018 {{IEEE Conference}} on {{Computational Intelligence}} and {{Games}} ({{CIG}})},
  author = {Dubey, Rahul and Ghantous, Joseph and Louis, Sushil and Liu, Siming},
  year = {2018},
  month = aug,
  pages = {1--8},
  issn = {2325-4289},
  doi = {10.1109/CIG.2018.8490375},
  abstract = {We investigate an evolutionary multi-objective approach to generating micro for real-time strategy games. Good micro helps a player win skirmishes and is one of the keys to developing better real-time strategy game play. In prior work, the same multi-objective approach of maximizing damage done while minimizing damage received was used to evolve micro for a group of ranged units versus a group of melee units. We extend this work to consider groups composed from two types of units. Specifically, this paper uses evolutionary multi-objective optimization to generate micro for one group composed from both ranged and melee units versus another group of ranged and melee units. Our micro behavior representation uses influence maps to represent enemy spatial information and potential fields generated from distance, health, and weapons cool down to guide unit movement. Experimental results indicate that our multi-objective approach leads to a Pareto front of diverse high-quality micro encapsulating multiple possible tactics. This range of micro provided by the Pareto front enables a human or AI player to trade-off among short term tactics that better suit the player's longer term strategy - for example, choosing to minimize friendly unit damage at the cost of only lightly damaging the enemy versus maximizing damage to the enemy units at the cost of increased damage to friendly units. We believe that our results indicate the usefulness of potential fields as a representation, and of evolutionary multi-objective optimization as an approach, for generating micro.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Dubey et al. - 2018 - Evolutionary Multi-objective Optimization of Real- 2.pdf;C\:\\Users\\aesou\\Zotero\\storage\\G8P6DTD5\\8490375.html},
  keywords = {Acceleration,AI player,artificial intelligence,Artificial intelligence,computer games,enemy units,evolutionary multiobjective optimization,game AI,Games,genetic algorithms,high-quality microencapsulating,influence maps,melee units,microbehavior representation,multiobjective approach,NSGA-II,Optimization,Pareto front,potential fields,ranged units,real-time strategy game play,real-time strategy micro,Real-time systems,Three-dimensional displays,Weapons}
}

@book{duffy_monte_2009,
  title = {Monte {{Carlo}} Frameworks: Building Customisable High Performance {{C}}++ Applications},
  shorttitle = {Monte {{Carlo}} Frameworks},
  author = {Duffy, Daniel J. and Kienitz, Joerg},
  year = {2009},
  publisher = {{Wiley}},
  address = {{Chichester, U.K}},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Duffy and Kienitz - 2009 - Monte Carlo frameworks building customisable high.pdf},
  isbn = {978-0-470-06069-8},
  keywords = {C++ (Computer program language),Finance,Mathematical models,Monte Carlo method},
  language = {en},
  lccn = {HG106 .D84 2009},
  series = {Wiley Finance}
}

@incollection{dugueperoux_pruning_2016,
  title = {Pruning {{Playouts}} in {{Monte}}-{{Carlo Tree Search}} for the {{Game}} of {{Havannah}}},
  booktitle = {Computers and {{Games}}},
  author = {Dugu{\'e}p{\'e}roux, Joris and Mazyad, Ahmad and Teytaud, Fabien and Dehos, Julien},
  year = {2016},
  volume = {10068},
  pages = {47--57},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-50935-8_5},
  abstract = {Monte-Carlo Tree Search (MCTS) is a popular technique for playing multi-player games. In this paper, we propose a new method to bias the playout policy of MCTS. The idea is to prune the decisions which seem ``bad'' (according to the previous iterations of the algorithm) before computing each playout. Thus, the method evaluates the estimated ``good'' moves more precisely. We have tested our improvement for the game of Havannah and compared it to several classic improvements. Our method outperforms the classic version of MCTS (with the RAVE improvement) and the different playout policies of MCTS that we have experimented.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2016\\9 Pruning Playouts in Monte-Carlo Tree Search for the Game of Havannah.pdf},
  isbn = {978-3-319-50934-1 978-3-319-50935-8},
  language = {en}
}

@incollection{dussault_optimization_2007,
  title = {Optimization of a {{Billiard Player}} \textendash{} {{Tactical Play}}},
  booktitle = {Computers and {{Games}}},
  author = {Dussault, Jean-Pierre and Landry, Jean-Fran{\c c}ois},
  year = {2007},
  volume = {4630},
  pages = {256--270},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-75538-8_23},
  abstract = {In this paper we explore the tactical aspects needed for the creation of an intelligent computer-pool player. The research results in three modifications to our previous model. An optimization procedure computes the shot parameters and repositions the cue ball on a given target. Moreover, we take a look at possible heuristics to generate a sound selection of targets repositioning. We thus obtain a greedy but rather good billiard player.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2006\\28 Optimization of a Billiard Player – Tactical Play.pdf},
  isbn = {978-3-540-75537-1 978-3-540-75538-8},
  language = {en}
}

@book{egenfeldt-nielsen_understanding_2008,
  title = {Understanding Video Games: The Essential Introduction},
  shorttitle = {Understanding Video Games},
  author = {{Egenfeldt-Nielsen}, Simon and Smith, Jonas Heide and Tosca, Susana Pajares},
  year = {2008},
  publisher = {{Routledge}},
  address = {{New York}},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\9JZ6AA5J\\Egenfeldt-Nielsen et al. - 2008 - Understanding video games the essential introduct.pdf},
  isbn = {978-0-415-97720-3 978-0-415-97721-0 978-0-203-93074-8},
  keywords = {Video games},
  language = {en},
  lccn = {GV1469.3 .E44 2008}
}

@misc{ell_video_2018,
  title = {Video Game Industry Is Booming with Continued Revenue},
  author = {Ell, Kellie},
  year = {2018},
  month = jul,
  abstract = {The video game industry is growing with new products and technology and could be worth nearly \$138 billion by the end of the year.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\7VL576EG\\video-game-industry-is-booming-with-continued-revenue.html},
  howpublished = {https://www.cnbc.com/2018/07/18/video-game-industry-is-booming-with-continued-revenue.html}
}

@inproceedings{enzenberger_lock-free_2010,
  title = {A {{Lock}}-{{Free Multithreaded Monte}}-{{Carlo Tree Search Algorithm}}},
  booktitle = {Advances in {{Computer Games}}},
  author = {Enzenberger, Markus and M{\"u}ller, Martin},
  editor = {{van den Herik}, H. Jaap and Spronck, Pieter},
  year = {2010},
  pages = {14--20},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-12993-3_2},
  abstract = {With the recent success of Monte-Carlo tree search algorithms in Go and other games, and the increasing number of cores in standard CPUs, the efficient parallelization of the search has become an important issue. We present a new lock-free parallel algorithm for Monte-Carlo tree search which takes advantage of the memory model of the IA-32 and Intel-64 CPU architectures and intentionally ignores rare faulty updates of node values. We show that this algorithm significantly improves the scalability of the Fuego Go program.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Enzenberger and Müller - 2010 - A Lock-Free Multithreaded Monte-Carlo Tree Search .pdf;C\:\\Users\\aesou\\Zotero\\storage\\QQLP446I\\Enzenberger and Müller - 2010 - A Lock-Free Multithreaded Monte-Carlo Tree Search .pdf},
  isbn = {978-3-642-12993-3},
  keywords = {Board Size,Heuristic Evaluation,Memory Array,Memory Model,Number Thread},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{erickson_global_2014,
  title = {Global {{State Evaluation}} in {{StarCraft}}},
  author = {Erickson, Graham and Buro, Michael},
  year = {2014},
  pages = {7},
  abstract = {State evaluation and opponent modelling are important areas to consider when designing game-playing Artificial Intelligence. This paper presents a model for predicting which player will win in the real-time strategy game StarCraft. Model weights are learned from replays using logistic regression. We also present some metrics for estimating player skill which can be used a features in the predictive model, including using a battle simulation as a baseline to compare player performance against.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\BWUJNSMX\\Erickson and Buro - 2014 - Global State Evaluation in StarCraft.pdf},
  language = {en}
}

@book{ernest_w_adams_fundamentals_2014,
  title = {Fundamentals of Construction and Simulation Game Design},
  author = {{Ernest W Adams}},
  year = {2014},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\SWYNIXXL\\Adams - 2014 - Fundamentals of construction and simulation game d.pdf},
  isbn = {978-0-13-381262-6 978-0-13-381197-1},
  language = {English}
}

@inproceedings{erol_htn_1994,
  title = {{{HTN Planning}}: {{Complexity}} and {{Expressivity}}},
  booktitle = {Proceedings of the 12th {{National Conference}} on {{Artificial Intelligence}}},
  author = {Erol, Kutluhan and Hendler, James and Nau, Dana S.},
  year = {1994},
  pages = {1123--1128},
  publisher = {{AAAI Press}},
  address = {{Menlo Park, CA}},
  abstract = {Most practical work on AI planning systems during thelast fifteen years has been based on hierarchical tasknetwork (HTN) d ecomposition, but until now, therehas been very little analytical work on the propertiesof HTN planners. This paper describes how the complexity of HTN planning varies with various conditionson the task networks.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Erol et al. - 1994 - HTN Planning Complexity and Expressivity.pdf}
}

@inproceedings{erticky_evolving_2015,
  title = {Evolving {{Reactive Micromanagement Controller}} for {{Real}}-{{Time Strategy Games}}},
  booktitle = {Scientific {{Conference}} of {{Young Researchers}}, {{At Herlany}}, {{Slovakia}}},
  author = {Erticky, Martin C\textasciicaron{} and Erticky, Michal C\textasciicaron{}},
  year = {2015},
  pages = {5},
  abstract = {Real-Time Strategy (RTS) games are a genre of video games representing an interesting, well-defined adversarial domain for Artificial Intelligence (AI) research. One of many subproblems that RTS players need to solve is the micromanagement of individual units (simple agents carrying out player's commands) during combat. Numerous multi-agent reactive control mechanisms have already been developed to maximize the combat efficiency of controlled units. Majority of these mechanisms make use of numeric parameters that need to be fine-tuned in order to achieve desired behavior. Due to a large number of these parameters, assigning them manually is inconvenient and training them by machine learning methods usually takes a long time (search space is too large). To reduce the search space and accelerate the training, we propose a simple reactive controller with only eight parameters. We implement it for a classic RTS game StarCraft: Brood War and train the parameters using genetic algorithms. Our experiments demonstrate an impressive combat performance after only a small number of generations.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\EJA6NLRC\\Erticky and Erticky - 2015 - Evolving Reactive Micromanagement Controller for R.pdf},
  language = {en}
}

@inproceedings{esparcia-alcazar_fast_2014,
  title = {Fast {{Evolutionary Adaptation}} for {{Monte Carlo Tree Search}}},
  booktitle = {European {{Conference}} on the {{Applications}} of {{Evolutionary Computation}}},
  author = {Lucas, Simon M. and Samothrakis, Spyridon and P{\'e}rez, Diego},
  editor = {{Esparcia-Alc{\'a}zar}, Anna I. and Mora, Antonio M.},
  year = {2014},
  volume = {8602},
  pages = {349--360},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-662-45523-4_29},
  abstract = {This paper describes a new adaptive Monte Carlo Tree Search (MCTS) algorithm that uses evolution to rapidly optimise its performance. An evolutionary algorithm is used as a source of control parameters to modify the behaviour of each iteration (i.e. each simulation or roll-out) of the MCTS algorithm; in this paper we largely restrict this to modifying the behaviour of the random default policy, though it can also be applied to modify the tree policy.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Lucas et al. - 2014 - Fast Evolutionary Adaptation for Monte Carlo Tree .pdf},
  isbn = {978-3-662-45522-7 978-3-662-45523-4},
  language = {en}
}

@incollection{esser_improving_2014,
  title = {Improving {{Best}}-{{Reply Search}}},
  booktitle = {Computers and {{Games}}},
  author = {Esser, Markus and Gras, Michael and Winands, Mark H. M. and Schadd, Maarten P. D. and Lanctot, Marc},
  year = {2014},
  volume = {8427},
  pages = {125--137},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-09165-5_11},
  abstract = {Best-Reply Search (BRS) is a new search technique for gametree search in multi-player games. In BRS, the exponentially many possibilities that can be considered by opponent players is flattened so that only a single move, the best one among all opponents, is chosen. BRS has been shown to outperform the classic search techniques in several domains. However, BRS may consider invalid game states. In this paper, we improve the BRS search technique such that it preserves the proper turn order during the search and does not lead to invalid states. The new technique, BRS+, uses the move ordering to select moves at opponent nodes that are not searched. Empirically, we show that BRS+ significantly improves the performance of BRS in Four-Player Chess, leading to winning 8.3 \%\textendash{}11.1 \% more games against the classic techniques maxn and Paranoid, respectively. When BRS+ plays against maxn, Paranoid, and BRS at once, it wins the most games as well.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2013\\15 Improving Best-Reply Search.pdf},
  isbn = {978-3-319-09164-8 978-3-319-09165-5},
  language = {en}
}

@article{farooq_starcraft_2016,
  title = {{{StarCraft AI Competition}}: {{A Step Toward Human}}-{{Level AI}} for {{Real}}-{{Time Strategy Games}}},
  author = {Farooq, Sehar Shahzad and Oh, In-Suk and Kim, Man-Jae and Kim, Kyung Joong},
  year = {2016},
  pages = {6},
  issn = {0738-4602},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\KEU3GL45\\Farooq et al. - 2016 - StarCraft AI Competition A Step Toward Human-Leve.pdf},
  journal = {AI Magazine},
  language = {en}
}

@book{fencott_game_2012,
  title = {Game Invaders: The Theory and Understanding of Computer Games},
  shorttitle = {Game Invaders},
  editor = {Fencott, P. C.},
  year = {2012},
  publisher = {{Wiley}},
  address = {{Hoboken, N.J}},
  abstract = {"Introduces a practical critical method for analyzing existing games and designing future games"--},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\HGKTAKSW\\Fencott - 2012 - Game invaders the theory and understanding of com.pdf},
  isbn = {978-0-470-59718-7},
  keywords = {Computer games,COMPUTERS / Programming / Games,Design,Video games},
  language = {en},
  lccn = {GV1469.3 .G365 2012}
}

@article{fern_ensemble_2011,
  title = {Ensemble {{Monte}}-{{Carlo Planning}}: {{An Empirical Study}}},
  author = {Fern, Alan and Lewis, Paul},
  year = {2011},
  pages = {8},
  abstract = {Monte-Carlo planning algorithms, such as UCT, select actions at each decision epoch by intelligently expanding a single search tree given the available time and then selecting the best root action. Recent work has provided evidence that it can be advantageous to instead construct an ensemble of search trees and to make a decision according to a weighted vote. However, these prior investigations have only considered the application domains of Go and Solitaire and were limited in the scope of ensemble configurations considered. In this paper, we conduct a more exhaustive empirical study of ensemble Monte-Carlo planning using the UCT algorithm in a set of six additional domains. In particular, we evaluate the advantages of a broad set of ensemble configurations in terms of space and time efficiency in both parallel and singlecore models. Our results demonstrate that ensembles are an effective way to improve performance per unit time given a parallel time model and performance per unit space in a single-core model. However, contrary to prior isolated observations, we did not find significant evidence that ensembles improve performance per unit time in a single-core model.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\PYCZRSPE\\Fern and Lewis - 2011 - Ensemble Monte-Carlo Planning An Empirical Study.pdf},
  language = {en}
}

@article{fernandez-ares_analysing_2017,
  title = {Analysing the Influence of the Fitness Function on Genetically Programmed Bots for a Real-Time Strategy Game},
  author = {{Fern{\'a}ndez-Ares}, A. and Mora, A.M. and {Garc{\'i}a-S{\'a}nchez}, P. and Castillo, P.A. and Merelo, J.J.},
  year = {2017},
  month = jan,
  volume = {18},
  pages = {15--29},
  issn = {18759521},
  doi = {10.1016/j.entcom.2016.08.001},
  abstract = {Finding the global best strategy for an autonomous agent (bot) in a RTS game is a hard problem, mainly because the techniques applied to do this must deal with uncertainty and real-time planning in order to control the game agents. This work describes an approach applying a Genetic Programming (GP) algorithm to create the behavioural engine of bots able to play a simple RTS. Normally it is impossible to know in advance what kind of strategies will be the best in the most general case of this problem. So GP, which searches the general decision tree space, has been introduced and used successfully. However, it is not straightforward what fitness function would be the most convenient to guide the evolutionary process in order to reach the best solutions and also being less sensitive to the uncertainty present in the context of games. Thus, in this paper three different evaluation functions have been proposed, and a detailed analysis of their performance has been conducted. The paper also analyses several aspects of the obtained bots, in addition to their final performance on battles, such as the evolution of the decision trees (behavioural models) themselves, or the influence on the results of noise or uncertainty. The results show that a victory-based fitness, which prioritises the number of victories, contributes to generate better bots, on average, than other functions based on other numerical aspects of the battles, such as the number of resources gathered, or the number of units generated.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\RB4K73XX\\Fernández-Ares et al. - 2017 - Analysing the influence of the fitness function on.pdf},
  journal = {Entertainment Computing},
  language = {en}
}

@inproceedings{fernandez-ares_designing_2014,
  title = {Designing {{Competitive Bots}} for a {{Real Time Strategy Game}} Using {{Genetic Programming}}},
  booktitle = {Proceedings of the 1st {{Congress}} of the {{Spanish Society}} for the {{Sciences}} of the {{Videogame}}},
  author = {{Fernandez-Ares}, A and {Garc\i{}a-Sanchez}, P and Mora, A M and Castillo, P A and Merelo, J J},
  year = {2014},
  pages = {14},
  abstract = {The design of the Artificial Intelligence (AI) engine for an autonomous agent (bot) in a game is always a difficult task mainly done by an expert human player, who has to transform his/her knowledge into a behavioural engine. This paper presents an approach for conducting this task by means of Genetic Programming (GP) application. This algorithm is applied to design decision trees to be used as bot's AI in 1 vs 1 battles inside the RTS game Planet Wars. Using this method it is possible to create rule-based systems defining decisions and actions, in an automatic way, completely different from a human designer doing them from scratch. These rules will be optimised along the algorithm run, considering the bot's performance during evaluation matches. As GP can generate and evolve behavioural rules not taken into account by an expert, the obtained bots could perform better than human-defined ones. Due to the difficulties when applying Computational Intelligence techniques in the videogames scope, such as noise factor in the evaluation functions, three different fitness approaches have been implemented and tested in this work. Two of them try to minimize this factor by considering additional dynamic information about the evaluation matches, rather than just the final result (the winner), as the other function does. In order to prove them, the best obtained agents have been compared with a previous bot, created by an expert player (from scratch) and then optimised by means of Genetic Algorithms. The experiments show that the three used fitness functions generate bots that outperform the optimized human-defined one, being the area-based fitness function the one that produces better results.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\BHESD4MK\\Fernandez-Ares et al. - 2014 - Designing Competitive Bots for a Real Time Strateg.pdf},
  language = {en}
}

@incollection{fernando_analyzing_2014,
  title = {Analyzing {{Simulations}} in {{Monte}}-{{Carlo Tree Search}} for the {{Game}} of {{Go}}},
  booktitle = {Computers and {{Games}}},
  author = {Fernando, Sumudu and M{\"u}ller, Martin},
  year = {2014},
  volume = {8427},
  pages = {72--83},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-09165-5_7},
  abstract = {In Monte-Carlo Tree Search, simulations play a crucial role since they replace the evaluation function used in classical game-tree search and guide the development of the game tree. Despite their importance, not too much is known about the details of how they work. This paper starts a more in-depth study of simulations, using the game of Go, and in particular the program Fuego, as an example. Playout policies are investigated in terms of the number of blunders they make, and in terms of how many points they lose over the course of a simulation. The result is a deeper understanding of the different components of the Fuego playout policy, as well as an analysis of the shortcomings of current methods for evaluating playouts.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2013\\11 Analyzing Simulations in Monte-Carlo Tree Search for the Game of Go.pdf},
  isbn = {978-3-319-09164-8 978-3-319-09165-5},
  language = {en}
}

@article{forbus_how_2002,
  title = {How Qualitative Spatial Reasoning Can Improve Strategy Game {{AIs}}},
  author = {Forbus, K.D. and Mahoney, J.V. and Dill, K.},
  year = {2002},
  month = jul,
  volume = {17},
  pages = {25--30},
  issn = {1541-1672},
  doi = {10.1109/MIS.2002.1024748},
  abstract = {Spatial reasoning is a majorsource of difficulties for strategy gameAIs. Weconjecturethat qualitative spatial reasoningtechniquescan help overcomtehese difficulties.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\HMX8TDCE\\Forbus et al. - 2002 - How qualitative spatial reasoning can improve stra.pdf},
  journal = {IEEE Intelligent Systems},
  language = {en},
  number = {4}
}

@article{frutos-pascual_review_2017,
  title = {Review of the {{Use}} of {{AI Techniques}} in {{Serious Games}}: {{Decision Making}} and {{Machine Learning}}},
  shorttitle = {Review of the {{Use}} of {{AI Techniques}} in {{Serious Games}}},
  author = {{Frutos-Pascual}, Maite and Zapirain, Begonya Garcia},
  year = {2017},
  month = jun,
  volume = {9},
  pages = {133--152},
  issn = {1943-068X, 1943-0698},
  doi = {10.1109/TCIAIG.2015.2512592},
  abstract = {The video-games market has become an established and ever-growing global industry. The health of the video and computer games industry, together with the variety of genres and technologies available, mean that videogame concepts and programmes are being applied in numerous different disciplines. One of these is the field known as serious games. The main goal of this article is to collect all the relevant articles published during the last decade and create a trend analysis about the use of certain artificial intelligence algorithms related to decision making and learning in the field of serious games. A categorization framework was designed and outlined to classify the 129 papers that met the inclusion criteria. The authors made use of this categorization framework for drawing some conclusions regarding the actual use of intelligent serious games. The authors consider that over recent years enough knowledge has been gathered to create new intelligent serious games to consider not only the final aim but also the technologies and techniques used to provide players with a nearly real experience. However, researchers may need to improve their testing methodology for developed serious games, so as to ensure they meet their final purposes.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\2USJT4GU\\Frutos-Pascual and Zapirain - 2017 - Review of the Use of AI Techniques in Serious Game.pdf},
  journal = {IEEE Transactions on Computational Intelligence and AI in Games},
  language = {en},
  number = {2}
}

@inproceedings{fu_monte_2018,
  title = {{{MONTE CARLO TREE SEARCH}}: {{A TUTORIAL}}},
  shorttitle = {{{MONTE CARLO TREE SEARCH}}},
  booktitle = {2018 {{Winter Simulation Conference}} ({{WSC}})},
  author = {Fu, Michael C.},
  year = {2018},
  month = dec,
  pages = {222--236},
  issn = {0891-7736},
  doi = {10.1109/WSC.2018.8632344},
  abstract = {Monte Carlo tree search (MCTS) is a general approach to solving game problems, playing a central role in Google DeepMind's AlphaZero and its predecessor AlphaGo, which famously defeated the (human) world Go champion Lee Sedol in 2016 and world \#1 Go player Ke Jie in 2017. Starting from scratch without using any domain-specific knowledge (other than the game rules), AlphaZero defeated not only its ancestors in Go but also the best computer programs in chess (Stockfish) and shogi (Elmo). In this tutorial, we provide an introduction to MCTS, including a review of its history and relationship to a more general simulation-based algorithm for Markov decision processes (MDPs) published in a 2005 Operations Research article; a demonstration of the basic mechanics of the algorithms via decision trees and the game of tic-tac-toe; and its use in AlphaGo and AlphaZero.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Fu - 2018 - MONTE CARLO TREE SEARCH A TUTORIAL.pdf;C\:\\Users\\aesou\\Zotero\\storage\\NXXYFFRP\\8632344.html},
  keywords = {AlphaGo,decision trees,Decision trees,game theory,Games,Go-playing programs,Google DeepMind AlphaZero,Markov decision processes,Markov processes,MCTS,Monte Carlo methods,Monte Carlo tree search,Neural networks,Operations research,simulation-based algorithm,Training,tree searching,Tutorials}
}

@article{fu_simulation-based_2019,
  title = {Simulation-{{Based Algorithms}} for {{Markov Decision Processes}}: {{Monte Carlo Tree Search}} from {{AlphaGo}} to {{AlphaZero}}},
  shorttitle = {Simulation-{{Based Algorithms}} for {{Markov Decision Processes}}},
  author = {Fu, Michael C.},
  year = {2019},
  month = dec,
  volume = {36},
  pages = {1940009},
  publisher = {{World Scientific Publishing Co.}},
  issn = {0217-5959},
  doi = {10.1142/S0217595919400098},
  abstract = {AlphaGo and its successors AlphaGo Zero and AlphaZero made international headlines with their incredible successes in game playing, which have been touted as further evidence of the immense potential of artificial intelligence, and in particular, machine learning. AlphaGo defeated the reigning human world champion Go player Lee Sedol 4 games to 1, in March 2016 in Seoul, Korea, an achievement that surpassed previous computer game-playing program milestones by IBM's Deep Blue in chess and by IBM's Watson in the U.S. TV game show Jeopardy. AlphaGo then followed this up by defeating the world's number one Go player Ke Jie 3-0 at the Future of Go Summit in Wuzhen, China in May 2017. Then, in December 2017, AlphaZero stunned the chess world by dominating the top computer chess program Stockfish (which has a far higher rating than any human) in a 100-game match by winning 28 games and losing none (72 draws) after training from scratch for just four hours! The deep neural networks of AlphaGo, AlphaZero, and all their incarnations are trained using a technique called Monte Carlo tree search (MCTS), whose roots can be traced back to an adaptive multistage sampling (AMS) simulation-based algorithm for Markov decision processes (MDPs) published in Operations Research back in 2005 [Chang, HS, MC Fu, J Hu and SI Marcus (2005). An adaptive sampling algorithm for solving Markov decision processes. Operations Research, 53, 126\textendash{}139.] (and introduced even earlier in 2002). After reviewing the history and background of AlphaGo through AlphaZero, the origins of MCTS are traced back to simulation-based algorithms for MDPs, and its role in training the neural networks that essentially carry out the value/policy function approximation used in approximate dynamic programming, reinforcement learning, and neuro-dynamic programming is discussed, including some recently proposed enhancements building on statistical ranking \& selection research in the operations research simulation community.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Fu - 2019 - Simulation-Based Algorithms for Markov Decision Pr.pdf},
  journal = {Asia-Pacific Journal of Operational Research},
  number = {06}
}

@book{fullerton_game_2014,
  title = {Game {{Design Workshop}}: A {{Playcentric Approach}} to {{Creating Innovative Games}}, {{Third Edition}}.},
  shorttitle = {Game {{Design Workshop}}},
  author = {Fullerton, Tracy},
  year = {2014},
  publisher = {{CRC Press}},
  address = {{Natick}},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\2HFZAF3R\\Fullerton - Game Design Workshop.pdf},
  isbn = {978-1-4822-1717-9},
  language = {English}
}

@article{gabriel_neuroevolution_2014,
  title = {Neuroevolution {{Based Multi}}-{{Agent System}} with {{Ontology Based Template Creation}} for {{Micromanagement}} in {{Real}}-{{Time Strategy Games}}},
  author = {Gabriel, I. and Negru, V. and Zaharie, D.},
  year = {2014},
  month = mar,
  volume = {43},
  issn = {2335-884X, 1392-124X},
  doi = {10.5755/j01.itc.43.1.4600},
  abstract = {This paper presents a multi-agent system that handles unit micromanagement using online machine learning in real time strategy games. We used rtNEAT algorithm in order to obtain customized neural network topologies, thus avoiding to complex network architecture. We use an ontology based template to create suitable input and outputs for unit agents enabling them to cooperate and form teams for their mutual benefit and eliminating communication overhead. The AI system was implemented using the JADE framework and the BWAPI handled communication between our system and the game. We have chosen Starcraft as a testbed. As a baseline we compared the in game AI as well as several other AI solutions that use adaptive mechanisms.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\H6ZBQKLJ\\Gabriel et al. - 2014 - Neuroevolution Based Multi-Agent System with Ontol.pdf},
  journal = {Information Technology And Control},
  language = {en},
  number = {1}
}

@inproceedings{gai_learning_2010,
  title = {Learning {{Multiuser Channel Allocations}} in {{Cognitive Radio Networks}}: {{A Combinatorial Multi}}-{{Armed Bandit Formulation}}},
  shorttitle = {Learning {{Multiuser Channel Allocations}} in {{Cognitive Radio Networks}}},
  booktitle = {2010 {{IEEE Symposium}} on {{New Frontiers}} in {{Dynamic Spectrum}} ({{DySPAN}})},
  author = {Gai, Yi and Krishnamachari, Bhaskar and Jain, Rahul},
  year = {2010},
  month = apr,
  pages = {1--9},
  publisher = {{IEEE}},
  address = {{Singapore, Singapore}},
  doi = {10.1109/DYSPAN.2010.5457857},
  abstract = {We consider the following fundamental problem in the context of channelized dynamic spectrum access. There are M secondary users and N {$\geq$} M orthogonal channels. Each secondary user requires a single channel for operation that does not conflict with the channels assigned to the other users. Due to geographic dispersion, each secondary user can potentially see different primary user occupancy behavior on each channel. Time is divided into discrete decision rounds. The throughput obtainable from spectrum opportunities on each userchannel combination over a decision period is modeled as an arbitrarily-distributed random variable with bounded support but unknown mean, i.i.d. over time. The objective is to search for an allocation of channels for all users that maximizes the expected sum throughput. We formulate this problem as a combinatorial multi-armed bandit (MAB), in which each arm corresponds to a matching of the users to channels. Unlike most prior work on multi-armed bandits, this combinatorial formulation results in dependent arms. Moreover, the number of arms grows super-exponentially as the permutation P (N, M ). We present a novel matching-learning algorithm with polynomial storage and polynomial computation per decision period for this problem, and prove that it results in a regret (the gap between the expected sum-throughput obtained by a genie-aided perfect allocation and that obtained by this algorithm) that is uniformly upper-bounded for all time n by a function that grows as O(M 4N logn), i.e. polynomial in the number of unknown parameters and logarithmic in time. We also discuss how our results provide a non-trivial generalization of known theoretical results on multi-armed bandits.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\TEZM4G5C\\Gai et al. - 2010 - Learning Multiuser Channel Allocations in Cognitiv.pdf},
  isbn = {978-1-4244-5189-0},
  language = {en}
}

@inproceedings{gaina_rolling_2017,
  title = {Rolling Horizon Evolution Enhancements in General Video Game Playing},
  booktitle = {2017 {{IEEE Conference}} on {{Computational Intelligence}} and {{Games}} ({{CIG}})},
  author = {Gaina, Raluca D. and Lucas, Simon M. and {Perez-Liebana}, Diego},
  year = {2017},
  month = aug,
  pages = {88--95},
  issn = {2325-4289},
  doi = {10.1109/CIG.2017.8080420},
  abstract = {Game AI literature has looked at applying various enhancements to Rolling Horizon Evolutionary methods or creating hybrids with popular tree search methods for an improved performance. However, these techniques have not been analyzed in depth in a general setting under the same conditions and restrictions. This paper proposes a fair juxtaposition of four enhancements applied to different parts of the evolutionary process: bandit-based mutation, a statistical tree for action selection, a shift buffer for population management and additional Monte Carlo simulations at the end of an individual's evaluation. These methods are studied individually, as well as their hybrids, on a representative subset of 20 games of the General Video Game AI Framework and compared to the vanilla version of the Rolling Horizon Evolutionary Algorithm, in addition to the dominating Monte Carlo Tree Search. The results show that some of the enhancements are able to produce impressive results, while others fall short. Interesting hybrids also emerge, encouraging further research into this problem.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Gaina et al. - 2017 - Rolling horizon evolution enhancements in general .pdf;C\:\\Users\\aesou\\Zotero\\storage\\U5VUMQUQ\\8080420.html},
  keywords = {action selection,Artificial intelligence,computer games,evolutionary computation,Evolutionary computation,evolutionary process,Game AI literature,Games,General Video Game AI Framework,horizon evolution enhancements,Mathematical model,Monte Carlo methods,Monte Carlo simulations,Monte Carlo Tree Search,population management,Rolling Horizon Evolutionary Algorithm,shift buffer,Sociology,statistical tree,tree searching,video game playing}
}

@article{gaina_rolling_2020,
  title = {Rolling {{Horizon Evolutionary Algorithms}} for {{General Video Game Playing}}},
  author = {Gaina, Raluca D. and Devlin, Sam and Lucas, Simon M. and {Perez-Liebana}, Diego},
  year = {2020},
  month = mar,
  abstract = {Game-playing Evolutionary Algorithms, specifically Rolling Horizon Evolutionary Algorithms, have recently managed to beat the state of the art in performance across many games. However, the best results per game are highly dependent on the specific configuration of modifications and hybrids introduced over several works, each described as parameters in the algorithm. However, the search for the best parameters has been reduced to several human-picked combinations, as the possibility space has grown beyond exhaustive search. This paper presents the state of the art in Rolling Horizon Evolutionary algorithms, combining all modifications described in literature and some additional ones for a large resultant hybrid. It then uses a parameter optimiser, the N-Tuple Bandit Evolutionary Algorithm, to find the best combination of parameters in 20 games with various properties from the General Video Game AI Framework. We highlight the noisy optimisation problem resultant, as both the games and the algorithm being optimised are stochastic. We then analyse the algorithm's parameters and interesting combinations revealed through the parameter optimisation process. Lastly, we show that it is possible to automatically explore a large parameter space and find configurations which outperform the state of the art on several games.},
  archivePrefix = {arXiv},
  eprint = {2003.12331},
  eprinttype = {arxiv},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Gaina et al. - 2020 - Rolling Horizon Evolutionary Algorithms for Genera.pdf},
  journal = {arXiv:2003.12331 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Neural and Evolutionary Computing},
  language = {en},
  primaryClass = {cs}
}

@inproceedings{gajurel_multiobjective_2019,
  title = {Multiobjective Multi Unit-Type Neuroevolution for Micro in {{RTS}} Games},
  booktitle = {Proceedings of the {{Genetic}} and {{Evolutionary Computation Conference Companion}}},
  author = {Gajurel, Aavaas and Louis, Sushil J.},
  year = {2019},
  month = jul,
  pages = {169--170},
  publisher = {{Association for Computing Machinery}},
  address = {{Prague, Czech Republic}},
  doi = {10.1145/3319619.3322064},
  abstract = {We used multiobjective genetic algorithms with neuroevolution of augmenting topologies (NEAT) to evolve effective micro behaviors for opposing groups with heterogeneous compositions in StarCraft II, an RTS game. We used the Fast Nondominated Sorting Genetic Algorithm to maximize damage done and minimize damage received in a skirmish, and used this two objective fitness to guide NEAT to evolve the structure and weights of a neural network based controller. The evolved NEAT network controls the movement and attack commands for each unit. We show that non-dominated selection and NEAT can be used together to generate effective micro for groups with two types or three types of units on each side. The evolved micro also generalized well to random configurations, doing well along both objectives. We also manually co-evolved against the best performing individuals produced during a run for multiple cycles and show that this improves micro resulting in better performance against the default Starcraft II AI.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Gajurel and Louis - 2019 - Multiobjective multi unit-type neuroevolution for .pdf},
  isbn = {978-1-4503-6748-6},
  keywords = {evolution,multi objective,NEAT,neural networks,NSGAII,RTS},
  series = {{{GECCO}} '19}
}

@article{gajurel_neuroevolution_2018,
  title = {Neuroevolution for {{RTS Micro}}},
  author = {Gajurel, Aavaas and Louis, Sushil J. and Mendez, Daniel J. and Liu, Siming},
  year = {2018},
  month = mar,
  abstract = {This paper uses neuroevolution of augmenting topologies to evolve control tactics for groups of units in realtime strategy games. In such games, players build economies to generate armies composed of multiple types of units with different attack and movement characteristics to combat each other. This paper evolves neural networks to control movement and attack commands, also called micro, for a group of ranged units skirmishing with a group of melee units. Our results show that neuroevolution of augmenting topologies can effectively generate neural networks capable of good micro for our ranged units against a group of hand-coded melee units. The evolved neural networks lead to kiting behavior for the ranged units which is a common tactic used by professional players in ranged versus melee skirmishes in popular real-time strategy games like Starcraft. The evolved neural networks also generalized well to other starting positions and numbers of units. We believe these results indicate the potential of neuroevolution for generating effective micro in real-time strategy games.},
  archivePrefix = {arXiv},
  eprint = {1803.10288},
  eprinttype = {arxiv},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\IGYVI58Q\\Gajurel et al. - 2018 - Neuroevolution for RTS Micro.pdf},
  journal = {arXiv:1803.10288 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Neural and Evolutionary Computing},
  language = {en},
  primaryClass = {cs}
}

@misc{gamer_which_2020,
  title = {Which {{RTS}} Series Deserves a Comeback?},
  author = {Gamer, P. C.},
  year = {2020},
  month = jan,
  abstract = {There's certainly enough space for them to return.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\ZP4GYKBP\\which-rts-series-deserves-a-comeback.html},
  journal = {PC Gamer},
  language = {en-US}
}

@inproceedings{garcia-sanchez_towards_2015,
  title = {Towards Automatic {{StarCraft}} Strategy Generation Using Genetic Programming},
  booktitle = {2015 {{IEEE Conference}} on {{Computational Intelligence}} and {{Games}} ({{CIG}})},
  author = {{Garc{\'i}a-S{\'a}nchez}, Pablo and Tonda, Alberto and Mora, Antonio M. and Squillero, Giovanni and Merelo, J.J.},
  year = {2015},
  month = aug,
  pages = {284--291},
  publisher = {{IEEE}},
  address = {{Tainan, Taiwan}},
  doi = {10.1109/CIG.2015.7317940},
  abstract = {Among Real-Time Strategy games few titles have enjoyed the continued success of StarCraft. Many research lines aimed at developing Artificial Intelligences, or ``bots'', capable of challenging human players, use StarCraft as a platform. Several characteristics make this game particularly appealing for researchers, such as: asymmetric balanced factions, considerable complexity of the technology trees, large number of units with unique features, and potential for optimization both at the strategical and tactical level. In literature, various works exploit evolutionary computation to optimize particular aspects of the game, from squad formation to map exploration; but so far, no evolutionary approach has been applied to the development of a complete strategy from scratch. In this paper, we present the preliminary results of StarCraftGP, a framework able to evolve a complete strategy for StarCraft, from the building plan, to the composition of squads, up to the set of rules that define the bot's behavior during the game. The proposed approach generates strategies as C++ classes, that are then compiled and executed inside the OpprimoBot open-source framework. In a first set of runs, we demonstrate that StarCraftGP ultimately generates a competitive strategy for a Zerg bot, able to defeat several humandesigned bots.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\37TEKXLV\\Garcia-Sanchez et al. - 2015 - Towards automatic StarCraft strategy generation us.pdf},
  isbn = {978-1-4799-8622-4},
  language = {en}
}

@inproceedings{garcia-sanchez_tree_2014-1,
  title = {Tree {{Depth Influence}} in {{Genetic Programming}} for {{Generation}} of {{Competitive Agents}} for {{RTS Games}}},
  booktitle = {Applications of {{Evolutionary Computation}}},
  author = {{Garc{\'i}a-S{\'a}nchez}, Pablo and {Fern{\'a}ndez-Ares}, Antonio and Mora, Antonio M. and Castillo, Pedro A. and Gonz{\'a}lez, Jes{\'u}s and Guerv{\'o}s, Juan Juli{\'a}n Merelo},
  editor = {{Esparcia-Alc{\'a}zar}, Anna I. and Mora, Antonio M.},
  year = {2014},
  pages = {411--421},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {This work presents the results obtained from comparing different tree depths in a Genetic Programming Algorithm to create agents that play the Planet Wars game. Three different maximum levels of the tree have been used (3, 7 and Unlimited) and two bots available in the literature, based on human expertise, and optimized by a Genetic Algorithm have been used for training and comparison. Results show that in average, the bots obtained using our method equal or outperform the previous ones, being the maximum depth of the tree a relevant parameter for the algorithm.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\UM98N4NB\\García-Sánchez et al. - 2014 - Tree Depth Influence in Genetic Programming for Ge.pdf},
  isbn = {978-3-662-45523-4},
  keywords = {Competitive Agent,Genetic Program Algorithm,Genetic Programming,Good Individual,Tree Depth},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{garisto_google_2019,
  title = {Google {{AI}} Beats Top Human Players at Strategy Game {{StarCraft II}}},
  author = {Garisto, Dan},
  year = {2019},
  month = oct,
  publisher = {{Nature Publishing Group}},
  doi = {10.1038/d41586-019-03298-6},
  abstract = {DeepMind's AlphaStar beat all but the very best humans at the fast-paced sci-fi video game.},
  copyright = {2019 Nature},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\UFQ2SMFW\\d41586-019-03298-6.html},
  journal = {Nature},
  language = {en}
}

@incollection{gaudel_principled_2011,
  title = {A {{Principled Method}} for {{Exploiting Opening Books}}},
  booktitle = {Computers and {{Games}}},
  author = {Gaudel, Romaric and Hoock, Jean-Baptiste and P{\'e}rez, Julien and Sokolovska, Nataliya and Teytaud, Olivier},
  year = {2011},
  volume = {6515},
  pages = {136--144},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-17928-0_13},
  abstract = {In the past we used a great deal of computational power and human expertise for storing a rather big dataset of good 9x9 Go games, in order to build an opening book. We improved the algorithm used for generating and storing these games considerably. However, the results were not very robust, as (i) opening books are definitely not transitive, making the non-regression testing extremely difficult, (ii) different time settings lead to opposite conclusions, because a good opening for a game with 10s per move on a single core is quite different from a good opening for a game with 30s per move on a 32-cores machine, and (iii) some very bad moves sometimes still occur. In this paper, we formalize the optimization of an opening book as a matrix game, compute the Nash equilibrium, and conclude that a naturally randomized opening book provides optimal performance (in the sense of Nash equilibria). Moreover, our research showed that from a finite set of opening books, we can choose a distribution on these opening books so that the resultant randomly constructed opening book has a significantly better performance than each of the deterministic opening books.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2010\\18 A Principled Method for Exploiting Opening Books.pdf},
  isbn = {978-3-642-17927-3 978-3-642-17928-0},
  language = {en}
}

@inproceedings{geib_learning_2018,
  title = {Learning {{Combinatory Categorial Grammars}} for {{Plan Recognition}}},
  booktitle = {The {{Thirty}}-{{Second AAAI Conference}} on {{Artificial Intelligence}} ({{AAAI}}-18)},
  author = {Geib, Christopher W and Kantharaju, Pavan},
  year = {2018},
  pages = {3007--3014},
  abstract = {This paper defines a learning algorithm for plan grammars used for plan recognition. The algorithm learns Combinatory Categorial Grammars (CCGs) that capture the structure of plans from a set of successful plan execution traces paired with the goal of the actions. This work is motivated by past work on CCG learning algorithms for natural language processing, and is evaluated on five well know planning domains.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Geib and Kantharaju - 2018 - Learning Combinatory Categorial Grammars for Plan .pdf},
  language = {en}
}

@inproceedings{geib_recognizing_2011,
  title = {Recognizing {{Plans}} with {{Loops Represented}} in a {{Lexicalized Grammar}}},
  booktitle = {Proceedings of the {{Twenty}}-{{Fifth AAAI Conference}} on {{Artificial Intelligence}}, {{AAAI}}'11},
  author = {Geib, Christopher W and Goldman, Robert P},
  year = {2011},
  pages = {958--963},
  abstract = {This paper extends existing plan recognition research to handle plans containing loops. We supply an encoding of plans with loops for recognition, based on techniques used to parse lexicalized grammars, and demonstrate its effectiveness empirically. To do this, the paper first shows how encoding plan libraries as context free grammars permits the application of standard rewriting techniques to remove left recursion and -productions, thereby enabling polynomial time parsing. However, these techniques alone fail to provide efficient algorithms for plan recognition. We show how the loop-handling methods from formal grammars can be extended to the more general plan recognition problem and provide a method for encoding loops in an existing plan recognition system that scales linearly in the number of loop iterations.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Geib and Goldman - 2011 - Recognizing Plans with Loops Represented in a Lexi.pdf},
  language = {en}
}

@inproceedings{gelly_exploration_2006,
  title = {Exploration Exploitation in {{Go}}: {{UCT}} for {{Monte}}-{{Carlo Go}}},
  shorttitle = {Exploration Exploitation in {{Go}}},
  booktitle = {{{NIPS}}: {{Neural Information Processing Systems Conference On}}-Line Trading of {{Exploration}} and {{Exploitation Workshop}}},
  author = {Gelly, Sylvain and Wang, Yizao},
  year = {2006},
  month = dec,
  address = {{Canada}},
  abstract = {Algorithm UCB1 for multi-armed bandit problem has already been extended to Algorithm UCT which works for minimax tree search. We have developed a Monte-Carlo program, MoGo, which is the first computer Go program using UCT. We explain our modifications of UCT for Go application, among which efficient memory management, parametrization, ordering of non-visited nodes and parallelization. MoGo is now a top-level Computer-Go program on 9 x 9 Go board.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\6MNQQNYB\\Gelly and Wang - 2006 - Exploration exploitation in Go UCT for Monte-Carl.pdf}
}

@article{georgievski_overview_2014,
  title = {An {{Overview}} of {{Hierarchical Task Network Planning}}},
  author = {Georgievski, Ilche and Aiello, Marco},
  year = {2014},
  month = mar,
  abstract = {Hierarchies are the most common structure used to understand the world better. In galaxies, for instance, multiple-star systems are organised in a hierarchical system. Then, governmental and company organisations are structured using a hierarchy, while the Internet, which is used on a daily basis, has a space of domain names arranged hierarchically. Since Artificial Intelligence (AI) planning portrays information about the world and reasons to solve some of world's problems, Hierarchical Task Network (HTN) planning has been introduced almost 40 years ago to represent and deal with hierarchies. Its requirement for rich domain knowledge to characterise the world enables HTN planning to be very useful, but also to perform well. However, the history of almost 40 years obfuscates the current understanding of HTN planning in terms of accomplishments, planning models, similarities and differences among hierarchical planners, and its current and objective image. On top of these issues, attention attracts the ability of hierarchical planning to truly cope with the requirements of applications from the real world. We propose a framework-based approach to remedy this situation. First, we provide a basis for defining different formal models of hierarchical planning, and define two models that comprise a large portion of HTN planners. Second, we provide a set of concepts that helps to interpret HTN planners from the aspect of their search space. Then, we analyse and compare the planners based on a variety of properties organised in five segments, namely domain authoring, expressiveness, competence, performance and applicability. Furthermore, we select Web service composition as a real-world and current application, and classify and compare the approaches that employ HTN planning to solve the problem of service composition. Finally, we conclude with our findings and present directions for future work.},
  archivePrefix = {arXiv},
  eprint = {1403.7426},
  eprinttype = {arxiv},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Georgievski and Aiello - 2014 - An Overview of Hierarchical Task Network Planning.pdf},
  journal = {arXiv:1403.7426 [cs]},
  keywords = {Computer Science - Artificial Intelligence},
  language = {en},
  primaryClass = {cs}
}

@article{germain_reinforcement_nodate,
  title = {Reinforcement {{Learning}} for {{Real}}-{{Time Strategy}} Games},
  author = {Germain, Maximilien and Marfoq, Othmane},
  pages = {8},
  abstract = {This project aims to offer an overview of the machine learning methods developed for Real-Time Strategy games (RTS). More precisely, it focuses on Reinforcement Learning (RL) techniques and presents the challenges faced when one wishes to provide successful AI algorithms for this type of games. A comparison with board games is conducted to show why it is not possible to directly transpose in this new setting the methods designed to solve them. The current state of the art models for RTS games and their performances are presented.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Germain and Marfoq - Reinforcement Learning for Real-Time Strategy game.pdf},
  language = {en}
}

@incollection{glenn_retrograde_2007,
  title = {A {{Retrograde Approximation Algorithm}} for {{One}}-{{Player Can}}'t {{Stop}}},
  booktitle = {Computers and {{Games}}},
  author = {Glenn, James and Fang, Haw-ren and Kruskal, Clyde P.},
  year = {2007},
  volume = {4630},
  pages = {148--159},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-75538-8_13},
  abstract = {A one-player, finite, probabilistic game with perfect information can be presented as a bipartite graph. For one-player Can't Stop, the graph is cyclic and the challenge is to determine the game-theoretical values of the positions in the cycles. In this contribution we prove the existence and uniqueness of the solution to one-player Can't Stop, and give an efficient approximation algorithm to solve it by incorporating Newton's method with retrograde analysis. We give results of applying this method to small versions of one-player Can't Stop.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2006\\18 A Retrograde Approximation Algorithm for One-Player Can’t Stop.pdf},
  isbn = {978-3-540-75537-1 978-3-540-75538-8},
  language = {en}
}

@incollection{glenn_retrograde_2008,
  title = {A {{Retrograde Approximation Algorithm}} for {{Multi}}-Player {{Can}}'t {{Stop}}},
  booktitle = {Computers and {{Games}}},
  author = {Glenn, James and Fang, Haw-ren and Kruskal, Clyde P.},
  year = {2008},
  volume = {5131},
  pages = {252--263},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-87608-3_23},
  abstract = {An n-player, finite, probabilistic game with perfect information can be presented as a 2n-partite graph. For Can't Stop, the graph is cyclic and the challenge is to determine the game-theoretical values of the positions in the cycles. We have presented our success on tackling one-player Can't Stop and two-player Can't Stop. In this article we study the computational solution of multi-player Can't Stop (more than two players), and present a retrograde approximation algorithm to solve it by incorporating the multi-dimensional Newton's method with retrograde analysis. Results of experiments on small versions of three- and four-player Can't Stop are presented.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2008\\28 A Retrograde Approximation Algorithm for Multi-player Can’t Stop.pdf},
  isbn = {978-3-540-87607-6 978-3-540-87608-3},
  language = {en}
}

@incollection{goetsch_experiments_1990,
  title = {Experiments with the {{Null}}-{{Move Heuristic}}},
  booktitle = {Computers, {{Chess}}, and {{Cognition}}},
  author = {Goetsch, G. and Campbell, M. S.},
  year = {1990},
  pages = {159--168},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  doi = {10.1007/978-1-4613-9080-0_9},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Goetsch and Campbell - 1990 - Experiments with the Null-Move Heuristic.pdf},
  isbn = {978-1-4613-9082-4 978-1-4613-9080-0},
  language = {en}
}

@inproceedings{golpayegani_collaborative_2015,
  title = {Collaborative, Parallel {{Monte Carlo Tree Search}} for Autonomous Electricity Demand Management},
  booktitle = {2015 {{Sustainable Internet}} and {{ICT}} for {{Sustainability}} ({{SustainIT}})},
  author = {Golpayegani, Fatemeh and Dusparic, Ivana and Clarke, Siobhan},
  year = {2015},
  month = apr,
  pages = {1--8},
  publisher = {{IEEE}},
  address = {{Madrid, Spain}},
  doi = {10.1109/SustainIT.2015.7101360},
  abstract = {Balancing electricity supply and consumption is critical for the stable performance of an electricity Grid. Demand Side Management (DSM) refers to shifting consumers' energy usage to off-peaks as much as possible to avoid more electricity demand than available supply during peak times. Artificial intelligent planning algorithms have been applied to enabling electric devices to reschedule their operation to off-peaks. One such algorithm is Monte Carlo Tree Search (MCTS), which takes advantage of tree search and random sampling on decision space in order to find an optimal domain decision.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Golpayegani et al. - 2015 - Collaborative, parallel Monte Carlo Tree Search fo.pdf},
  isbn = {978-3-901882-70-8},
  language = {en}
}

@book{gonzalez_calero_artificial_2011,
  title = {Artificial {{Intelligence}} for {{Computer Games}}},
  editor = {Gonz{\'a}lez Calero, Pedro A. and {G{\'o}mez-Mart{\'i}n}, Marco Antonio},
  year = {2011},
  publisher = {{Springer}},
  address = {{New York}},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\YMIEEX2K\\González Calero and Gómez-Martín - 2011 - Artificial Intelligence for Computer Games.pdf},
  isbn = {978-1-4419-8187-5},
  keywords = {Computer games,Programming},
  language = {en},
  lccn = {MLCM 2015/42506 (Q)}
}

@incollection{graf_semeai_2014,
  title = {On {{Semeai Detection}} in {{Monte}}-{{Carlo Go}}},
  booktitle = {Computers and {{Games}}},
  author = {Graf, Tobias and Schaefers, Lars and Platzner, Marco},
  year = {2014},
  volume = {8427},
  pages = {14--25},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-09165-5_2},
  abstract = {A frequently mentioned limitation of Monte-Carlo Tree Search (MCTS) based Go programs is their inability to recognize and adequately handle capturing races, also known as semeai, especially when many of them appear simultaneously. The inability essentially stems from the fact that certain group status evaluations require deep lines of correct tactical play which is directly related to the exploratory nature of MCTS. In this paper we provide a technique for heuristically detecting and analyzing semeai during the search process of a state-of-the-art MCTS implementation. We evaluate the strength of our approach on game positions that are known to be difficult to handle even by the strongest Go programs to date. Our results show a clear identification of semeai and thereby advocate our approach as a promising heuristic for the design of future MCTS simulation policies.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2013\\6 On Semeai Detection in Monte-Carlo Go.pdf},
  isbn = {978-3-319-09164-8 978-3-319-09165-5},
  language = {en}
}

@incollection{graf_using_2016,
  title = {Using {{Deep Convolutional Neural Networks}} in {{Monte Carlo Tree Search}}},
  booktitle = {Computers and {{Games}}},
  author = {Graf, Tobias and Platzner, Marco},
  year = {2016},
  volume = {10068},
  pages = {11--21},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-50935-8_2},
  abstract = {Deep Convolutional Neural Networks have revolutionized Computer Go. Large networks have emerged as state-of-the-art models for move prediction and are used not only as stand-alone players but also inside Monte Carlo Tree Search to select and bias moves. Using neural networks inside the tree search is a challenge due to their slow execution time even if accelerated on a GPU. In this paper we evaluate several strategies to limit the number of nodes in the search tree in which neural networks are used. All strategies are assessed using the freely available cuDNN library. We compare our strategies against an optimal upper bound which can be estimated by removing timing constraints. We show that the best strategies are only 50 ELO points worse than this upper bound.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2016\\6 Using Deep Convolutional Neural Networks in Monte Carlo Tree Search.pdf},
  isbn = {978-3-319-50934-1 978-3-319-50935-8},
  language = {en}
}

@phdthesis{granberg_monto_2019,
  title = {Monto {{Carlo Tree Search}} in {{Real Time Strategy Games}} with {{Applications}} to {{Starcraft}} 2},
  author = {Granberg, Aron and {\"O}gren, Petter},
  year = {2019},
  month = may,
  abstract = {This thesis presents an architecture for an agent that can play the real-time strategy game Starcraft 2 (SC2) by applying Monte Carlo Tree Search (MCTS) together with genetic algorithms and machine learning methods. Together with the MCTS search, a light-weight and accurate combat simulator for SC2 as well as a build order optimizer are presented as independent modules. While MCTS has been well studied for turn-based games such as Go and Chess, its performance has so far been less explored in the context of real-time games. Using machine learning and planning methods in real-time strategy games without requiring long training times has proven to be a challenge. This thesis explores how a model based approach, based on the rules of the game, can be used to achieve a well performing agent.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Granberg and Ögren - 2019 - Monto Carlo Tree Search in Real Time Strategy Game.pdf;C\:\\Users\\aesou\\Zotero\\storage\\CCPTEHDL\\Granberg and Ögren - 2019 - Monto Carlo Tree Search in Real Time Strategy Game.pdf},
  language = {en}
}

@article{greer_tree_2013,
  title = {Tree {{Pruning}} for {{New Search Techniques}} in {{Computer Games}}},
  author = {Greer, Kieran},
  year = {2013},
  volume = {2013},
  pages = {e357068},
  publisher = {{Hindawi}},
  issn = {1687-7470},
  doi = {https://doi.org/10.1155/2013/357068},
  abstract = {This paper proposes a new mechanism for pruning a search game tree in computer chess. The algorithm stores and then reuses chains or sequences of moves, built up from previous searches. These move sequences have a built-in forward-pruning mechanism that can radically reduce the search space. A typical search process might retrieve a move from a Transposition Table, where the decision of what move to retrieve would be based on the position itself. This algorithm stores move sequences based on what previous sequences were better, or caused cutoffs. The sequence is then returned based on the current move only. This is therefore position independent and could also be useful in games with imperfect information or uncertainty, where the whole situation is not known at any one time. Over a small set of tests, the algorithm was shown to clearly out perform Transposition Tables, both in terms of search reduction and game-play results. Finally, a completely new search process will be suggested for computer chess or games in general.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\QGY6FMGG\\Greer - 2013 - Tree Pruning for New Search Techniques in Computer.pdf;C\:\\Users\\aesou\\Zotero\\storage\\TIHPEYKF\\357068.html},
  journal = {Advances in Artificial Intelligence},
  language = {en}
}

@incollection{grimbergen_cognitive_2008,
  title = {Cognitive {{Modeling}} of {{Knowledge}}-{{Guided Information Acquisition}} in {{Games}}},
  booktitle = {Computers and {{Games}}},
  author = {Grimbergen, Reijer},
  year = {2008},
  volume = {5131},
  pages = {169--179},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-87608-3_16},
  abstract = {Since Chase and Simon presented their influential paper on perception in chess in 1973, the use of chunks has become the subject of a number of studies into the cognitive behavior of human game players. However, the nature of chunks has remained elusive, and the reason for this lies in the lack of using a general cognitive theory to explain the nature of chunks. In this paper it will be argued that Marvin Minsky's Society of Mind theory is a good candidate for a cognitive theory to define chunks and to explain the relation between chunks and problemsolving tasks. To use Minsky's Society of Mind theory to model human cognitive behavior in games, we first need to understand more about the primitive agents dealing with the relation between perception and knowledge in memory. To investigate this relation, a reproduction experiment is performed in shogi showing that perception is guided by knowledge in long-term memory. From the results we may conclude that the primitive agents in a cognitive model for game-playing should represent abstract concepts such as board, piece, and king rather than the perceptual features of board and pieces.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2008\\21 Cognitive Modeling of Knowledge-Guided Information Acquisition in Games.pdf},
  isbn = {978-3-540-87607-6 978-3-540-87608-3},
  language = {en}
}

@incollection{guid_computer_2007,
  title = {Computer {{Analysis}} of {{Chess Champions}}},
  booktitle = {Computers and {{Games}}},
  author = {Guid, Matej and Bratko, Ivan},
  year = {2007},
  volume = {4630},
  pages = {1--12},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-75538-8_1},
  abstract = {Who is the best chess player of all time? Chess players are often interested in this question that has never been answered authoritatively, because it requires a comparison between chess players of different eras who never met across the board. In this contribution, we attempt to make such a comparison. It is based on the evaluation of the games played by the World Chess Champions in their championship matches. The evaluation is performed by the chess-playing program Crafty. For this purpose we slightly adapted Crafty. Our analysis takes into account the differences in players' styles to compensate the fact that calm positional players in their typical games have less chance to commit gross tactical errors than aggressive tactical players. Therefore, we designed a method to assess the difficulty of positions. Some of the results of this computer analysis might be quite surprising. Overall, the results can be nicely interpreted by a chess expert.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2006\\6 Computer Analysis of Chess Champions.pdf},
  isbn = {978-3-540-75537-1 978-3-540-75538-8},
  language = {en}
}

@incollection{guid_learning_2008,
  title = {Learning {{Positional Features}} for {{Annotating Chess Games}}: {{A Case Study}}},
  shorttitle = {Learning {{Positional Features}} for {{Annotating Chess Games}}},
  booktitle = {Computers and {{Games}}},
  author = {Guid, Matej and Mo{\v z}ina, Martin and Krivec, Jana and Sadikov, Aleksander and Bratko, Ivan},
  year = {2008},
  volume = {5131},
  pages = {192--204},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-87608-3_18},
  abstract = {By developing an intelligent computer system that will provide commentary of chess moves in a comprehensible, user-friendly, and instructive way, we are trying to use the power demonstrated by the current chess engines for tutoring chess and for annotating chess games. In this paper, we point out certain differences between the computer programs which are specialized for playing chess and our program which is aimed at providing quality commentary. Through a case study, we present an application of argument-based machine learning, which combines the techniques of machine learning and expert knowledge, to the construction of more complex positional features, in order to provide our annotating system with an ability to comment on various positional intricacies of positions in the game of chess.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2008\\23 Learning Positional Features for Annotating Chess Games A Case Study.pdf},
  isbn = {978-3-540-87607-6 978-3-540-87608-3},
  language = {en}
}

@article{ha_world_2018,
  title = {World {{Models}}},
  author = {Ha, David and Schmidhuber, J{\"u}rgen},
  year = {2018},
  month = mar,
  doi = {10.5281/zenodo.1207631},
  abstract = {We explore building generative neural network models of popular reinforcement learning environments. Our world model can be trained quickly in an unsupervised manner to learn a compressed spatial and temporal representation of the environment. By using features extracted from the world model as inputs to an agent, we can train a very compact and simple policy that can solve the required task. We can even train our agent entirely inside of its own hallucinated dream generated by its world model, and transfer this policy back into the actual environment.},
  archivePrefix = {arXiv},
  eprint = {1803.10122},
  eprinttype = {arxiv},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\A3UY43YS\\Ha and Schmidhuber - 2018 - World Models.pdf},
  journal = {arXiv:1803.10122 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{hamrick_combining_2020,
  title = {Combining {{Q}}-{{Learning}} and {{Search}} with {{Amortized Value Estimates}}},
  author = {Hamrick, Jessica B. and Bapst, Victor and {Sanchez-Gonzalez}, Alvaro and Pfaff, Tobias and Weber, Theophane and Buesing, Lars and Battaglia, Peter W.},
  year = {2020},
  month = jan,
  abstract = {We introduce ``Search with Amortized Value Estimates'' (SAVE), an approach for combining model-free Q-learning with model-based Monte-Carlo Tree Search (MCTS). In SAVE, a learned prior over state-action values is used to guide MCTS, which estimates an improved set of state-action values. The new Q-estimates are then used in combination with real experience to update the prior. This effectively amortizes the value computation performed by MCTS, resulting in a cooperative relationship between model-free learning and model-based search. SAVE can be implemented on top of any Q-learning agent with access to a model, which we demonstrate by incorporating it into agents that perform challenging physical reasoning tasks and Atari. SAVE consistently achieves higher rewards with fewer training steps, and\textemdash{}in contrast to typical model-based search approaches\textemdash{}yields strong performance with very small search budgets. By combining real experience with information computed during search, SAVE demonstrates that it is possible to improve on both the performance of model-free learning and the computational cost of planning.},
  archivePrefix = {arXiv},
  eprint = {1912.02807},
  eprinttype = {arxiv},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Hamrick et al. - 2020 - Combining Q-Learning and Search with Amortized Val.pdf},
  journal = {arXiv:1912.02807 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@incollection{hartisch_quantified_2016,
  title = {Quantified {{Integer Programs}} with {{Polyhedral Uncertainty Set}}},
  booktitle = {Computers and {{Games}}},
  author = {Hartisch, Michael and Ederer, Thorsten and Lorenz, Ulf and Wolf, Jan},
  year = {2016},
  volume = {10068},
  pages = {156--166},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-50935-8_15},
  abstract = {Quantified Integer Programs (QIPs) are integer programs with variables being either existentially or universally quantified. They can be interpreted as a two-person zero-sum game with an existential and a universal player where the existential player tries to meet all constraints and the universal player intends to force at least one constraint to be not satisfied.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2016\\19 Quantified Integer Programs with Polyhedral Uncertainty Set.pdf},
  isbn = {978-3-319-50934-1 978-3-319-50935-8},
  language = {en}
}

@article{hayes_unsupervised_2018,
  title = {Unsupervised {{Hierarchical Clustering}} of {{Build Orders}} in a {{Real}}-{{Time Strategy Game}}},
  author = {Hayes, Roy and Beling, Peter},
  year = {2018},
  month = mar,
  volume = {7},
  pages = {5--26},
  issn = {2052-773X},
  doi = {10.1007/s40869-018-0051-1},
  abstract = {Currently, no artificial intelligence (AI) agent can beat a professional real-time strategy game player. Lack of effective opponent modeling limits an AI agent's ability to adapt to new opponents or strategies. Opponent models provide an understanding of the opponent's strategy and potential future actions. To date, opponent models have relied on handcrafted features and expert-defined strategies, which restricts AI agent opponent models to previously known and easily understood strategies. In this paper, we propose size-first hierarchic clustering to cluster players that employ similar strategies in a real-time strategy (RTS) game. We employ an unsupervised hierarchal clustering algorithm to cluster game build orders into strategy groups. To eliminate small outlying clusters, the hierarchal clustering algorithm was modified to first group the smallest cluster with its closest neighbor, i.e., size-first hierarchal clustering. In our analysis, we employ a previously developed dataset based on StarCraft: Brood War game replays. In our proposed approach, principal component analysis (PCA) is used to visualize player clusters, and the obtained PCA graphs show that the clusters are qualitatively distinct. We also demonstrate that a game's outcome is marginally affected by both players' clusters. In addition, we show that the opponent's faction can be determined based on a player's transition between clusters overtime. The novelty of our analysis is the lack of expert-defined features and an automated stopping condition to determine the appropriate number of clusters. Thus, the proposed approach is bias-free and applicable to any StarCraft-like RTS game.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\SVU2ZRJW\\Hayes and Beling - 2018 - Unsupervised Hierarchical Clustering of Build Orde.pdf},
  journal = {The Computer Games Journal},
  language = {en},
  number = {1}
}

@incollection{hayward_automatic_2007,
  title = {Automatic {{Strategy Verification}} for {{Hex}}},
  booktitle = {Computers and {{Games}}},
  author = {Hayward, Ryan B. and Arneson, Broderick and Henderson, Philip},
  year = {2007},
  volume = {4630},
  pages = {112--121},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-75538-8_10},
  abstract = {We present a concise and/or-tree notation for describing Hex strategies together with an easily implemented algorithm for verifying strategy correctness. To illustrate our algorithm, we use it to verify Jing Yang's 7\texttimes{}7 centre-opening strategy.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2006\\15 Automatic Strategy Verification for Hex.pdf},
  isbn = {978-3-540-75537-1 978-3-540-75538-8},
  language = {en}
}

@article{he_automl_2020,
  title = {{{AutoML}}: {{A Survey}} of the {{State}}-of-the-{{Art}}},
  shorttitle = {{{AutoML}}},
  author = {He, Xin and Zhao, Kaiyong and Chu, Xiaowen},
  year = {2020},
  month = feb,
  abstract = {Deep-learning techniques have penetrated all aspects of our lives and brought us great convenience. However, the process of building a high-quality deep-learning system for a specific task is time-consuming, requires extensive resources and relies on human expertise, hindering the further development of deep learning applications in both industry and academia. To alleviate this problem, a growing number of research projects focus on automated machine learning (AutoML). In this paper, we provide a comprehensive and up-to-date study on the state-of-the-art (SOTA) in AutoML. First, we introduce the AutoML techniques in detail, in relation to the machine-learning pipeline. We then summarize existing research on neural architecture search (NAS), as this is one of the most popular topics in the field of AutoML. We also compare the performance of models generated by NAS algorithms with that of human-designed models. Finally, we present several open problems for future research.},
  archivePrefix = {arXiv},
  eprint = {1908.00709},
  eprinttype = {arxiv},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\QNPY4P2J\\He et al. - 2020 - AutoML A Survey of the State-of-the-Art.pdf;C\:\\Users\\aesou\\Zotero\\storage\\Y6WQ54KQ\\1908.html},
  journal = {arXiv:1908.00709 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@inproceedings{he_game_2008-1,
  title = {Game {{Player Strategy Pattern Recognition}} and {{How UCT Algorithms Apply Pre}}-Knowledge of {{Player}}'s {{Strategy}} to {{Improve Opponent AI}}},
  booktitle = {2008 {{International Conference}} on {{Computational Intelligence}} for {{Modelling Control}} \& {{Automation}}},
  author = {He, Suoju and Wang, Yi and Xie, Fan and Meng, Jin and Chen, Hongtao and Luo, Sai and Liu, Zhiqing and Zhu, Qiliang},
  year = {2008},
  pages = {1177--1181},
  publisher = {{IEEE}},
  address = {{Vienna, Austria}},
  doi = {10.1109/CIMCA.2008.82},
  abstract = {Player Strategy Pattern Recognition (PSPR) is to apply pattern recognition and its approach to identification of player's strategy during the gameplay. Correctly identified player's strategy, which is called knowledge, could be used to improve game opponent AI which can be implemented by KB-UCT (knowledgebased Upper Confidence bound for Trees). KB-UCT improves adaptability of game AI, the challenge level of the gameplay, and the performance of the opponent AI; as a result the entertainment of game is promoted. In this paper, the prey and predator game genre of Dead End game is used as a test-bed. During the PSPR, classification algorithm of KNN (k-nearest neighbor) is chosen to analyze off-line data from the simulated gamers who are choosing different strategies. Based on the information from PSPR, the game AI is promoted through application of KB-UCT, in this case, domain knowledge is used for UCT tree pruning; as a result the performance of the opponent AI is enhanced.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\He et al. - 2008 - Game Player Strategy Pattern Recognition and How U.pdf},
  isbn = {978-0-7695-3514-2},
  language = {en}
}

@article{heinrich_fictitious_2015,
  title = {Fictitious {{Self}}-{{Play}} in {{Extensive}}-{{Form Games}}},
  author = {Heinrich, Johannes and Lanctot, Marc},
  year = {2015},
  pages = {9},
  abstract = {Fictitious play is a popular game-theoretic model of learning in games. However, it has received little attention in practical applications to large problems. This paper introduces two variants of fictitious play that are implemented in behavioural strategies of an extensive-form game. The first variant is a full-width process that is realization equivalent to its normal-form counterpart and therefore inherits its convergence guarantees. However, its computational requirements are linear in time and space rather than exponential. The second variant, Fictitious Self-Play, is a machine learning framework that implements fictitious play in a sample-based fashion. Experiments in imperfect-information poker games compare our approaches and demonstrate their convergence to approximate Nash equilibria.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\MZPNR8SJ\\Heinrich and Lanctot - 2015 - Fictitious Self-Play in Extensive-Form Games.pdf},
  language = {en}
}

@article{heinz_adaptive_1999,
  title = {Adaptive {{Null}}-{{Move Pruning}}},
  author = {Heinz, E. A.},
  year = {1999},
  month = jan,
  volume = {22},
  pages = {123--132},
  issn = {1389-6911},
  doi = {10.3233/ICG-1999-22302},
  abstract = {General wisdom deems strong computer-chess programs to be ``brute-force searchers'' that explore game trees as exhaustively as possible within the given time limits. We review the results of the latest World Computer-Chess Championships and show how gr},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Heinz - 1999 - Adaptive Null-Move Pruning.pdf;C\:\\Users\\aesou\\Zotero\\storage\\DL5XSDGJ\\icg22302.html},
  journal = {ICGA Journal},
  language = {en},
  number = {3}
}

@article{heinz_extended_1998,
  title = {Extended {{Futility Pruning}}},
  author = {Heinz, E. A.},
  year = {1998},
  month = jan,
  volume = {21},
  pages = {75--83},
  publisher = {{IOS Press}},
  issn = {1389-6911},
  doi = {10.3233/ICG-1998-21202},
  abstract = {This article presents a new selective pruning technique for alpha-beta based game-tree search in computer chess, called extended futility pruning . It builds on ideas of both razoring and normal futility pruning at frontier nodes (depth = 1), and it},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Heinz - 1998 - Extended Futility Pruning.pdf;C\:\\Users\\aesou\\Zotero\\storage\\JVEQSET3\\icg21202.html},
  journal = {ICGA Journal},
  language = {en},
  number = {2}
}

@book{heinz_scalable_2000,
  title = {Scalable {{Search}} in {{Computer Chess}}},
  author = {Heinz, Ernst A.},
  year = {2000},
  publisher = {{Vieweg+Teubner Verlag}},
  address = {{Wiesbaden}},
  doi = {10.1007/978-3-322-90178-1},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Heinz - 2000 - Scalable Search in Computer Chess.pdf},
  isbn = {978-3-528-05732-9 978-3-322-90178-1},
  language = {en}
}

@incollection{henderson_probing_2008,
  title = {Probing the 4-3-2 {{Edge Template}} in {{Hex}}},
  booktitle = {Computers and {{Games}}},
  author = {Henderson, Philip and Hayward, Ryan B.},
  year = {2008},
  volume = {5131},
  pages = {229--240},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-87608-3_21},
  abstract = {For the game of Hex, we find conditions under which moves into a 4-3-2 edge template are provably inferior.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2008\\26 Probing the 4-3-2 Edge Template in Hex.pdf},
  isbn = {978-3-540-87607-6 978-3-540-87608-3},
  language = {en}
}

@article{hernandez-leal_survey_2019,
  title = {A {{Survey}} and {{Critique}} of {{Multiagent Deep Reinforcement Learning}}},
  author = {{Hernandez-Leal}, Pablo and Kartal, Bilal and Taylor, Matthew E.},
  year = {2019},
  month = nov,
  volume = {33},
  pages = {750--797},
  issn = {1387-2532, 1573-7454},
  doi = {10.1007/s10458-019-09421-1},
  abstract = {Deep reinforcement learning (RL) has achieved outstanding results in recent years. This has led to a dramatic increase in the number of applications and methods. Recent works have explored learning beyond single-agent scenarios and have considered multiagent learning (MAL) scenarios. Initial results report successes in complex multiagent domains, although there are several challenges to be addressed. The primary goal of this article is to provide a clear overview of current multiagent deep reinforcement learning (MDRL) literature. Additionally, we complement the overview with a broader analysis: (i) we revisit previous key components, originally presented in MAL and RL, and highlight how they have been adapted to multiagent deep reinforcement learning settings. (ii) We provide general guidelines to new practitioners in the area: describing lessons learned from MDRL works, pointing to recent benchmarks, and outlining open avenues of research. (iii) We take a more critical tone raising practical challenges of MDRL (e.g., implementation and computational demands). We expect this article will help unify and motivate future research to take advantage of the abundant literature that exists (e.g., RL and MAL) in a joint effort to promote fruitful research in the multiagent community.},
  archivePrefix = {arXiv},
  eprint = {1810.05587},
  eprinttype = {arxiv},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Hernandez-Leal et al. - 2019 - A Survey and Critique of Multiagent Deep Reinforce.pdf},
  journal = {Autonomous Agents and Multi-Agent Systems},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Multiagent Systems},
  language = {en},
  number = {6}
}

@article{hernandez-leal_survey_2019-1,
  title = {A Survey and Critique of Multiagent Deep Reinforcement Learning},
  author = {{Hernandez-Leal}, Pablo and Kartal, Bilal and Taylor, Matthew E.},
  year = {2019},
  month = nov,
  volume = {33},
  pages = {750--797},
  issn = {1573-7454},
  doi = {10.1007/s10458-019-09421-1},
  abstract = {Deep reinforcement learning (RL) has achieved outstanding results in recent years. This has led to a dramatic increase in the number of applications and methods. Recent works have explored learning beyond single-agent scenarios and have considered multiagent learning (MAL) scenarios. Initial results report successes in complex multiagent domains, although there are several challenges to be addressed. The primary goal of this article is to provide a clear overview of current multiagent deep reinforcement learning (MDRL) literature. Additionally, we complement the overview with a broader analysis: (i) we revisit previous key components, originally presented in MAL and RL, and highlight how they have been adapted to multiagent deep reinforcement learning settings. (ii) We provide general guidelines to new practitioners in the area: describing lessons learned from MDRL works, pointing to recent benchmarks, and outlining open avenues of research. (iii) We take a more critical tone raising practical challenges of MDRL (e.g., implementation and computational demands). We expect this article will help unify and motivate future research to take advantage of the abundant literature that exists (e.g., RL and MAL) in a joint effort to promote fruitful research in the multiagent community.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Hernandez-Leal et al. - 2019 - A survey and critique of multiagent deep reinforce 2.pdf},
  journal = {Autonomous Agents and Multi-Agent Systems},
  language = {en},
  number = {6}
}

@incollection{hirata_werewolf_2016,
  title = {Werewolf {{Game Modeling Using Action Probabilities Based}} on {{Play Log Analysis}}},
  booktitle = {Computers and {{Games}}},
  author = {Hirata, Yuya and Inaba, Michimasa and Takahashi, Kenichi and Toriumi, Fujio and Osawa, Hirotaka and Katagami, Daisuke and Shinoda, Kousuke},
  year = {2016},
  volume = {10068},
  pages = {103--114},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-50935-8_10},
  abstract = {In this study, we construct a non-human agent that can play the werewolf game (i.e., AI wolf) with aims of creating more advanced intelligence and acquire more advanced communication skills for AIbased systems. We therefore constructed a behavioral model using information regarding human players and the decisions made by such players; all such information was obtained from play logs of the werewolf game. To confirm our model, we conducted simulation experiments of the werewolf game using an agent based on our proposed behavioral model, as well as a random agent for comparison. Consequently, we obtained an 81.55\% coincidence ratio of agent behavior versus human behavior.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2016\\14 Werewolf Game Modeling Using Action Probabilities Based on Play Log Analysis.pdf},
  isbn = {978-3-319-50934-1 978-3-319-50935-8},
  language = {en}
}

@book{hjorth_games_2011,
  title = {Games and Gaming an Introduction to New Media},
  author = {Hjorth, Larissa},
  year = {2011},
  publisher = {{Berg}},
  address = {{Oxford; New York}},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\WYG47TKX\\Hjorth - 2011 - Games and gaming an introduction to new media.pdf},
  isbn = {978-1-84788-839-6},
  language = {English}
}

@article{hoki_efficiency_2012,
  title = {Efficiency of Three Forward-Pruning Techniques in Shogi: {{Futility}} Pruning, Null-Move Pruning, and {{Late Move Reduction}} ({{LMR}})},
  shorttitle = {Efficiency of Three Forward-Pruning Techniques in Shogi},
  author = {Hoki, Kunihito and Muramatsu, Masakazu},
  year = {2012},
  month = aug,
  volume = {3},
  pages = {51--57},
  issn = {1875-9521},
  doi = {10.1016/j.entcom.2011.11.003},
  abstract = {The efficiency of three forward-pruning techniques, i.e., futility pruning, null-move pruning, and LMR, is analyzed in shogi, a Japanese chess variant. It is shown that the techniques with the {$\alpha$}\textendash{$\beta$} pruning reduce the effective branching factor of shogi endgames to 2.8 without sacrificing much accuracy of the search results. Because the average number of the raw branching factor in shogi is around 80, the pruning techniques reduce the search space more effectively than in chess.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Hoki and Muramatsu - 2012 - Efficiency of three forward-pruning techniques in .pdf;C\:\\Users\\aesou\\Zotero\\storage\\YQX9GI98\\S1875952111000450.html},
  journal = {Entertainment Computing},
  keywords = {Branching factor,Brute-force search,Chess,Forward pruning,Shogi},
  language = {en},
  number = {3},
  series = {Games and {{AI}}}
}

@inproceedings{holldobler_lessons_2017,
  title = {Lessons {{Learned}} from {{AlphaGo}}},
  booktitle = {{{YSIP}} 2017},
  author = {H{\"o}lldobler, Steffen and M{\"o}hle, Sibylle and Tigunova, Anna},
  year = {2017},
  pages = {92--101},
  abstract = {The game of Go is known to be one of the most complicated board games. Competing in Go against a professional human player has been a long-standing challenge for AI. In this paper we shed light on the AlphaGo program that could beat a Go world champion, which was previously considered non-achievable for the state of the art AI.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Hölldobler et al. - 2017 - Lessons Learned from AlphaGo.pdf},
  language = {en}
}

@inproceedings{holler_htn_2018,
  title = {{{HTN Plan Repair Using Unmodified Planning Systems}}},
  booktitle = {Proceedings of the 28th {{International Conference}} on {{Automated Planning}} and {{Scheduling}}. {{Workshop}} on {{Hierarchical Planning}}},
  author = {Holler, Daniel and Bercher, Pascal and Behnke, Gregor and Biundo, Susanne},
  year = {2018},
  pages = {26--30},
  abstract = {To make planning feasible, planning models abstract from many details of the modeled system. When executing plans in the actual system, the model might be inaccurate in a critical point, and plan execution may fail. There are two options to handle this case: the previous solution can be modified to address the failure (plan repair), or the planning process can be re-started from the new situation (re-planning). In HTN planning, discarding the plan and generating a new one from the novel situation is not easily possible, because the HTN solution criteria make it necessary to take already executed actions into account. Therefore all approaches to repair plans in the literature are based on specialized algorithms. In this paper, we discuss the problem in detail and introduce a novel approach that makes it possible to use unchanged, offthe-shelf HTN planning systems to repair broken HTN plans. That way, no specialized solvers are needed.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Holler et al. - 2018 - HTN Plan Repair Using Unmodiﬁed Planning Systems.pdf},
  language = {en}
}

@inproceedings{holler_plan_2018,
  title = {Plan and {{Goal Recognition}} as {{HTN Planning}}},
  booktitle = {2018 {{IEEE}} 30th {{International Conference}} on {{Tools}} with {{Artificial Intelligence}} ({{ICTAI}})},
  author = {Holler, Daniel and Behnke, Gregor and Bercher, Pascal and Biundo, Susanne},
  year = {2018},
  month = nov,
  pages = {466--473},
  publisher = {{IEEE}},
  address = {{Volos, Greece}},
  doi = {10.1109/ICTAI.2018.00078},
  abstract = {Plan- and Goal Recognition (PGR) is the task of inferring the goals and plans of an agent based on its actions. A few years ago, an approach has been introduced that successfully exploits the performance of planning systems to solve it. That way, no specialized solvers are needed and PGR benefits from present and future research in planning. The approach uses classical planning systems and needs to plan (at least) once for every possible goal. However, models in PGR are often structured in a hierarchical way, similar to Hierarchical Task Networks (HTNs). These models are strictly more expressive than those in classical planning and can describe partially ordered sets of tasks or multiple goals with interleaving plans. We present the approach PGR as HTN Planning that enables the recognition of complex agent behavior by using unmodified, off-the-shelf HTN planners. Planning is thereby needed only once, regardless of how many possible goals there are. Our evaluation shows that current planning systems are able to handle large models with thousands of possible goals and that the approach results in high recognition rates.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Holler et al. - 2018 - Plan and Goal Recognition as HTN Planning.pdf},
  isbn = {978-1-5386-7449-9},
  language = {en}
}

@inproceedings{holte_move_2013,
  title = {Move {{Pruning}} and {{Duplicate Detection}}},
  booktitle = {Advances in {{Artificial Intelligence}}},
  author = {Holte, Robert C.},
  editor = {Za{\"i}ane, Osmar R. and Zilles, Sandra},
  year = {2013},
  pages = {40--51},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-38457-8_4},
  abstract = {This paper begins by showing that Burch and Holte's move pruning method is, in general, not safe to use in conjunction with the kind of duplicate detection done by standard heuristic search algorithms such as A*. It then investigates the interactions between move pruning and duplicate detection with the aim of elucidating conditions under which it is safe to use both techniques together. Conditions are derived under which simple interactions cannot possibly occur and it is shown that these conditions hold in many of the state spaces commonly used as research testbeds. Unfortunately, these conditions do not preclude more complex interactions from occurring. The paper then proves two conditions that must hold whenever move pruning is not safe to use with duplicate detection and discusses circumstances in which each of these conditions might not hold, i.e. circumstances in which it would be safe to use move pruning in conjunction with duplicate detection.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Holte - 2013 - Move Pruning and Duplicate Detection.pdf;C\:\\Users\\aesou\\Zotero\\storage\\8WCZAFRV\\Holte - 2013 - Move Pruning and Duplicate Detection.pdf},
  isbn = {978-3-642-38457-8},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@book{hornby_oxford_2015,
  title = {Oxford Advanced Learner's Dictionary of Current {{English}}},
  author = {Hornby, Albert Sydney and Deuter, Margaret},
  year = {2015},
  publisher = {{Oxford Univ. Press}},
  address = {{Oxford}},
  isbn = {978-0-19-479878-5 978-0-19-479879-2 978-0-19-479885-3},
  keywords = {\#nosource},
  language = {English}
}

@inproceedings{hsieh_building_2008,
  title = {Building a Player Strategy Model by Analyzing Replays of Real-Time Strategy Games},
  booktitle = {2008 {{IEEE International Joint Conference}} on {{Neural Networks}} ({{IEEE World Congress}} on {{Computational Intelligence}})},
  author = {Hsieh, Ji-Lung and Sun, Chuen-Tsai},
  year = {2008},
  month = jun,
  pages = {3106--3111},
  publisher = {{IEEE}},
  address = {{Hong Kong, China}},
  doi = {10.1109/IJCNN.2008.4634237},
  abstract = {Developing computer-controlled groups to engage in combat, control the use of limited resources, and create units and buildings in Real-Time Strategy(RTS) Games is a novel application in game AI. However, tightly controlled online commercial game pose challenges to researchers interested in observing player activities, constructing player strategy models, and developing practical AI technology in them. Instead of setting up new programming environments or building a large amount of agent's decision rules by player's experience for conducting real-time AI research, the authors use replays of the commercial RTS game StarCraft to evaluate human player behaviors and to construct an intelligent system to learn human-like decisions and behaviors. A case-based reasoning approach was applied for the purpose of training our system to learn and predict player strategies. Our analysis indicates that the proposed system is capable of learning and predicting individual player strategies, and that players provide evidence of their personal characteristics through their building construction order.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\JBJ8UPMP\\Hsieh and Sun - 2008 - Building a player strategy model by analyzing repl.pdf},
  isbn = {978-1-4244-1820-6},
  language = {en}
}

@article{hua_hyper-meta_2020,
  title = {Hyper-{{Meta Reinforcement Learning}} with {{Sparse Reward}}},
  author = {Hua, Yun and Wang, Xiangfeng and Jin, Bo and Li, Wenhao and Yan, Junchi and He, Xiaofeng and Zha, Hongyuan},
  year = {2020},
  month = feb,
  abstract = {Despite their success, existing meta reinforcement learning methods still have difficulty in learning a meta policy effectively for RL problems with sparse reward. To this end, we develop a novel meta reinforcement learning framework, Hyper-Meta RL (HMRL), for sparse reward RL problems. It consists of meta state embedding, meta reward shaping and meta policy learning modules: The cross-environment meta state embedding module constructs a common meta state space to adapt to different environments; The meta state based environment-specific meta reward shaping effectively extends the original sparse reward trajectory by cross-environmental knowledge complementarity; As a consequence, the meta policy then achieves better generalization and efficiency with the shaped meta reward. Experiments with sparse reward show the superiority of HMRL on both transferability and policy learning efficiency.},
  archivePrefix = {arXiv},
  eprint = {2002.04238},
  eprinttype = {arxiv},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Hua et al. - 2020 - Hyper-Meta Reinforcement Learning with Sparse Rewa.pdf},
  journal = {arXiv:2002.04238 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@inproceedings{huang_comparing_2019,
  title = {Comparing {{Observation}} and {{Action Representations}} for {{Deep Reinforcement Learning}} in {{\textmu{}RTS}}},
  booktitle = {{{AIIDE Workshop}} on {{Artificial Intelligence}} for {{Strategy Games}}},
  author = {Huang, Shengyi and Onta{\~n}{\'o}n, Santiago},
  year = {2019},
  month = oct,
  abstract = {This paper presents a preliminary study comparing different observation and action space representations for Deep Reinforcement Learning (DRL) in the context of Real-time Strategy (RTS) games. Specifically, we compare two representations: (1) a global representation where the observation represents the whole game state, and the RL agent needs to choose which unit to issue actions to, and which actions to execute; and (2) a local representation where the observation is represented from the point of view of an individual unit, and the RL agent picks actions for each unit independently. We evaluate these representations in \textmu{}RTS showing that the local representation seems to outperform the global representation when training agents with the task of harvesting resources.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Huang and Ontañón - 2019 - Comparing Observation and Action Representations f 2.pdf;C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Huang and Ontañón - 2019 - Comparing Observation and Action Representations f.pdf},
  language = {en}
}

@incollection{huang_investigating_2014,
  title = {Investigating the {{Limits}} of {{Monte}}-{{Carlo Tree Search Methods}} in {{Computer Go}}},
  booktitle = {Computers and {{Games}}},
  author = {Huang, Shih-Chieh and M{\"u}ller, Martin},
  year = {2014},
  volume = {8427},
  pages = {39--48},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-09165-5_4},
  abstract = {Monte-Carlo Tree Search methods have led to huge progress in computer Go. Still, program performance is uneven - most current Go programs are much stronger in some aspects of the game, such as local fighting and positional evaluation, than in other aspects. Well known weaknesses of many programs include (1) the handling of several simultaneous fights, including the two safe groups problem, and (2) dealing with coexistence in seki.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2013\\8 Investigating the Limits of Monte-Carlo Tree Search Methods in Computer Go.pdf},
  isbn = {978-3-319-09164-8 978-3-319-09165-5},
  language = {en}
}

@incollection{huang_mohex_2014,
  title = {{{MoHex}} 2.0: {{A Pattern}}-{{Based MCTS Hex Player}}},
  shorttitle = {{{MoHex}} 2.0},
  booktitle = {Computers and {{Games}}},
  author = {Huang, Shih-Chieh and Arneson, Broderick and Hayward, Ryan B. and M{\"u}ller, Martin and Pawlewicz, Jakub},
  year = {2014},
  volume = {8427},
  pages = {60--71},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-09165-5_6},
  abstract = {In recent years the Monte Carlo tree search revolution has spread from computer Go to many areas, including computer Hex. MCTSbased Hex players now outperform traditional knowledge-based alphabeta search players, and the reigning Computer Olympiad Hex gold medallist is the MCTS player MoHex. In this paper we show how to strengthen MoHex, and observe that\textemdash{}as in computer Go\textemdash{}using learned patterns in priors and replacing a hand-crafted simulation policy by a softmax policy that uses learned patterns significantly increases playing strength. The result is MoHex 2.0, about 250 Elo points stronger than MoHex on the 11\texttimes{}11 board, and 300 Elo points stronger on the 13\texttimes{}13 board.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2013\\10 MoHex 2.0 A Pattern-Based MCTS Hex Player.pdf},
  isbn = {978-3-319-09164-8 978-3-319-09165-5},
  language = {en}
}

@incollection{huang_monte-carlo_2011,
  title = {Monte-{{Carlo Simulation Balancing}} in {{Practice}}},
  booktitle = {Computers and {{Games}}},
  author = {Huang, Shih-Chieh and Coulom, R{\'e}mi and Lin, Shun-Shii},
  year = {2011},
  volume = {6515},
  pages = {81--92},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-17928-0_8},
  abstract = {Simulation balancing is a new technique to tune parameters of a playout policy for a Monte-Carlo game-playing program. So far, this algorithm had only been tested in a very artificial setting: it was limited to 5 \texttimes{} 5 and 6 \texttimes{} 6 Go, and required a stronger external program that served as a supervisor. In this paper, the effectiveness of simulation balancing is demonstrated in a more realistic setting. A state-of-the-art program, Erica, learned an improved playout policy on the 9 \texttimes{} 9 board, without requiring any external expert to provide position evaluations. The evaluations were collected by letting the program analyze positions by itself. The previous version of Erica learned pattern weights with the minorization-maximization algorithm. Thanks to simulation balancing, its playing strength was improved from a winning rate of 69\% to 78\% against Fuego 0.4.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2010\\13 Monte-Carlo Simulation Balancing in Practice.pdf},
  isbn = {978-3-642-17927-3 978-3-642-17928-0},
  language = {en}
}

@inproceedings{huang_multi-size_2018,
  title = {A Multi-Size Convolution Neural Network for {{RTS}} Games Winner Prediction},
  booktitle = {{{MATEC Web}} of {{Conferences}}},
  author = {Huang, Jie and Yang, Weilong},
  editor = {Wang, Yansong},
  year = {2018},
  month = nov,
  volume = {232},
  pages = {01054},
  doi = {10.1051/matecconf/201823201054},
  abstract = {Researches of AI planning in Real-Time Strategy (RTS) games have been widely applied to human behavior modeling and combat simulation. Winner prediction is an important research area for AI planning, which ensures the decision accuracy. In this paper, we introduce an effective architecture -- multi-size convolution neural network (MSCNN)-- into winner prediction. It can capture more feature for game states, because of the various sizes of filters in MSCNN. Experiments show that the modified evaluating algorithm can effectively improve the accuracy of winner prediction for RTS games.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\I8NV4U9Z\\Huang and Yang - 2018 - A multi-size convolution neural network for RTS ga.pdf},
  language = {en}
}

@inproceedings{huang_pruning_2010-1,
  title = {Pruning in {{UCT Algorithm}}},
  booktitle = {2010 {{International Conference}} on {{Technologies}} and {{Applications}} of {{Artificial Intelligence}}},
  author = {Huang, Jing and Liu, Zhiqing and Lu, Benjie and Xiao, Feng},
  year = {2010},
  month = nov,
  pages = {177--181},
  publisher = {{IEEE}},
  address = {{Hsinchu City, TBD, Taiwan}},
  doi = {10.1109/TAAI.2010.38},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Huang et al. - 2010 - Pruning in UCT Algorithm.pdf},
  isbn = {978-1-4244-8668-7},
  language = {en}
}

@incollection{ikeda_efficiency_2014,
  title = {Efficiency of {{Static Knowledge Bias}} in {{Monte}}-{{Carlo Tree Search}}},
  booktitle = {Computers and {{Games}}},
  author = {Ikeda, Kokolo and Viennot, Simon},
  year = {2014},
  volume = {8427},
  pages = {26--38},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-09165-5_3},
  abstract = {Monte-Carlo methods are currently the best known algorithms for the game of Go. It is already shown that Monte-Carlo simulations based on a probability model containing static knowledge of the game are more efficient than random simulations. Some programs also use such probability models in the tree search policy to limit the search to a subset of the legal moves or to bias the search. However, this aspect is not so well documented. In this paper, we describe more precisely how static knowledge can be used to improve the tree search policy. We show experimentally the efficiency of the proposed method by a large number of games played against open source Go programs.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2013\\7 Efficiency of Static Knowledge Bias in Monte-Carlo Tree Search.pdf},
  isbn = {978-3-319-09164-8 978-3-319-09165-5},
  language = {en}
}

@incollection{imagawa_monte_2016,
  title = {Monte {{Carlo Tree Search}} with {{Robust Exploration}}},
  booktitle = {Computers and {{Games}}},
  author = {Imagawa, Takahisa and Kaneko, Tomoyuki},
  year = {2016},
  volume = {10068},
  pages = {34--46},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-50935-8_4},
  abstract = {This paper presents a new Monte-Carlo tree search method that focuses on identifying the best move. UCT which minimizes the cumulative regret, has achieved remarkable success in Go and other games. However, recent studies on simple regret reveal that there are better exploration strategies. To further improve the performance, a leaf to be explored is determined not only by the mean but also by the whole reward distribution. We adopted a hybrid approach to obtain reliable distributions. A negamax-style backup of reward distributions is used in the shallower half of a search tree, and UCT is adopted in the rest of the tree. Experiments on synthetic trees show that this presented method outperformed UCT and similar methods, except for trees having uniform width and depth.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2016\\8 Monte Carlo Tree Search with Robust Exploration.pdf},
  isbn = {978-3-319-50934-1 978-3-319-50935-8},
  language = {en}
}

@incollection{isaac_using_2016,
  title = {Using {{Partial Tablebases}} in {{Breakthrough}}},
  booktitle = {Computers and {{Games}}},
  author = {Isaac, Andrew and Lorentz, Richard},
  year = {2016},
  volume = {10068},
  pages = {1--10},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-50935-8_1},
  abstract = {In the game of Breakthrough the endgame is reached when there are still many pieces on the board. This means there are too many possible positions to be able to construct a reasonable endgame tablebase on the standard 8 \texttimes{} 8 board, or even on a 6 \texttimes{} 6 board. The fact that Breakthrough pieces only move forward allows us to create partial tablebases on the last n rows of each side of the board. We show how doing this enables us to create a much stronger MCTS based 6 \texttimes{} 6 player and allows us to solve positions that would otherwise be out of reach.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2016\\5 Using Partial Tablebases in Breakthrough.pdf},
  isbn = {978-3-319-50934-1 978-3-319-50935-8},
  language = {en}
}

@incollection{isleifsdottir_gtq_2008,
  title = {{{GTQ}}: {{A Language}} and {{Tool}} for {{Game}}-{{Tree Analysis}}},
  shorttitle = {{{GTQ}}},
  booktitle = {Computers and {{Games}}},
  author = {{\'I}sleifsd{\'o}ttir, J{\'o}nhei{\dh}ur and Bj{\"o}rnsson, Yngvi},
  year = {2008},
  volume = {5131},
  pages = {217--228},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-87608-3_20},
  abstract = {The search engines of high-performance game-playing programs are becoming increasingly complex as more and more enhancements are added. To maintain and enhance such complex engines is a challenging task, and the risk of introducing bugs or other unwanted behavior during modifications is substantial. In this paper we introduce the Game Tree Query Language (GTQL), a query language specifically designed for analyzing game trees. The language can express queries about complex game-tree structures, including hierarchical relationships and aggregated attributes over subtree data. We also discuss the design and implementation of the Game Tree Query Tool (GTQT), a software tool that allows efficient execution of GTQL queries on game-tree log files. The tool helps program developers to gain added insight into the search process of their engines, as well as making regression testing easier. Furthermore, we use the tool to analyze and find interesting anomalies in search trees generated by a competitive chess program.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2008\\25 GTQ A Language and Tool for Game-Tree Analysis.pdf},
  isbn = {978-3-540-87607-6 978-3-540-87608-3},
  language = {en}
}

@inproceedings{james_analysis_2017,
  title = {An {{Analysis}} of {{Monte Carlo Tree Search}}},
  booktitle = {Thirty-{{First AAAI Conference}} on {{Artificial Intelligence}}},
  author = {James, Steven and Konidaris, George and Rosman, Benjamin},
  year = {2017},
  month = feb,
  abstract = {Monte Carlo Tree Search (MCTS) is a family of directed search algorithms that has gained widespread attention in recent years. Despite the vast amount of research into MCTS, the effect of modifications on the algorithm, as well as the manner in which it performs in various domains, is still not yet fully known. In particular, the effect of using knowledge-heavy rollouts in MCTS still remains poorly understood, with surprising results demonstrating that better-informed rollouts often result in worse-performing agents. We present experimental evidence suggesting that, under certain smoothness conditions, uniformly random simulation policies preserve the ordering over action preferences. This explains the success of MCTS despite its common use of these rollouts to evaluate states. We further analyse non-uniformly random rollout policies and describe conditions under which they offer improved performance.},
  copyright = {Authors who publish a paper in this conference agree to the following terms:   Author(s) agree to transfer their copyrights in their article/paper to the Association for the Advancement of Artificial Intelligence (AAAI), in order to deal with future requests for reprints, translations, anthologies, reproductions, excerpts, and other publications. This grant will include, without limitation, the entire copyright in the article/paper in all countries of the world, including all renewals, extensions, and reversions thereof, whether such rights current exist or hereafter come into effect, and also the exclusive right to create electronic versions of the article/paper, to the extent that such right is not subsumed under copyright.  The author(s) warrants that they are the sole author and owner of the copyright in the above article/paper, except for those portions shown to be in quotations; that the article/paper is original throughout; and that the undersigned right to make the grants set forth above is complete and unencumbered.  The author(s) agree that if anyone brings any claim or action alleging facts that, if true, constitute a breach of any of the foregoing warranties, the author(s) will hold harmless and indemnify AAAI, their grantees, their licensees, and their distributors against any liability, whether under judgment, decree, or compromise, and any legal fees and expenses arising out of that claim or actions, and the undersigned will cooperate fully in any defense AAAI may make to such claim or action. Moreover, the undersigned agrees to cooperate in any claim or other action seeking to protect or enforce any right the undersigned has granted to AAAI in the article/paper. If any such claim or action fails because of facts that constitute a breach of any of the foregoing warranties, the undersigned agrees to reimburse whomever brings such claim or action for expenses and attorneys' fees incurred therein.  Author(s) retain all proprietary rights other than copyright (such as patent rights).  Author(s) may make personal reuse of all or portions of the above article/paper in other works of their own authorship.  Author(s) may reproduce, or have reproduced, their article/paper for the author's personal use, or for company use provided that AAAI copyright and the source are indicated, and that the copies are not used in a way that implies AAAI endorsement of a product or service of an employer, and that the copies per se are not offered for sale. The foregoing right shall not permit the posting of the article/paper in electronic or digital form on any computer network, except by the author or the author's employer, and then only on the author's or the employer's own web page or ftp site. Such web page or ftp site, in addition to the aforementioned requirements of this Paragraph, must provide an electronic reference or link back to the AAAI electronic server, and shall not post other AAAI copyrighted materials not of the author's or the employer's creation (including tables of contents with links to other papers) without AAAI's written permission.  Author(s) may make limited distribution of all or portions of their article/paper prior to publication.  In the case of work performed under U.S. Government contract, AAAI grants the U.S. Government royalty-free permission to reproduce all or portions of the above article/paper, and to authorize others to do so, for U.S. Government purposes.  In the event the above article/paper is not accepted and published by AAAI, or is withdrawn by the author(s) before acceptance by AAAI, this agreement becomes null and void.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\James et al. - 2017 - An Analysis of Monte Carlo Tree Search.pdf;C\:\\Users\\aesou\\Zotero\\storage\\QYD2UCSI\\James et al. - 2017 - An Analysis of Monte Carlo Tree Search.pdf;C\:\\Users\\aesou\\Zotero\\storage\\VXIZPYCN\\14886.html},
  language = {en}
}

@article{james_analysis_nodate,
  title = {An {{Analysis}} of {{Monte Carlo Tree Search}}},
  author = {James, Steven and Konidaris, George and Rosman, Benjamin},
  pages = {8},
  abstract = {Monte Carlo Tree Search (MCTS) is a family of directed search algorithms that has gained widespread attention in recent years. Despite the vast amount of research into MCTS, the effect of modifications on the algorithm, as well as the manner in which it performs in various domains, is still not yet fully known. In particular, the effect of using knowledgeheavy rollouts in MCTS still remains poorly understood, with surprising results demonstrating that better-informed rollouts often result in worse-performing agents. We present experimental evidence suggesting that, under certain smoothness conditions, uniformly random simulation policies preserve the ordering over action preferences. This explains the success of MCTS despite its common use of these rollouts to evaluate states. We further analyse non-uniformly random rollout policies and describe conditions under which they offer improved performance.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\James et al. - An Analysis of Monte Carlo Tree Search.pdf},
  language = {en}
}

@inproceedings{johanson_evaluating_2013,
  title = {Evaluating {{State}}-{{Space Abstractions}} in {{Extensive}}-{{Form Games}}},
  booktitle = {12th {{International Conference}} on {{Autonomous Agents}} and {{Multiagent Systems}} 2013, {{AAMAS}} 2013},
  author = {Johanson, Michael and Burch, Neil and Valenzano, Richard and Bowling, Michael},
  year = {2013},
  month = may,
  volume = {1},
  abstract = {Efficient algorithms exist for finding optimal policies in extensive-form games. However, human-scale problems are typically so large that this computation remains infeasible with modern computing resources. State-space abstraction techniques allow for the derivation of a smaller and strategi-cally similar abstract domain, in which an optimal strategy can be computed and then used as a suboptimal strategy in the real domain. In this paper, we consider the task of evalu-ating the quality of an abstraction, independent of a specific abstract strategy. In particular, we use a recent metric for abstraction quality and examine imperfect recall abstrac-tions, in which agents " forget " previously observed informa-tion to focus the abstraction effort on more recent and rele-vant state information. We present experimental results in the domain of Texas hold'em poker that validate the use of distribution-aware abstractions over expectation-based ap-proaches, demonstrate that the new metric better predicts tournament performance, and show that abstractions built using imperfect recall outperform those built using perfect recall in terms of both exploitability and one-on-one play.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\FV46TQ9P\\Johanson et al. - 2013 - Evaluating State-Space Abstractions in Extensive-F.pdf}
}

@inproceedings{justesen_blood_2019,
  title = {Blood {{Bowl}}: {{A New Board Game Challenge}} and {{Competition}} for {{AI}}},
  shorttitle = {Blood {{Bowl}}},
  booktitle = {2019 {{IEEE Conference}} on {{Games}} ({{CoG}})},
  author = {Justesen, Niels and Uth, Lasse M{\o}ller and Jakobsen, Christopher and Moore, Peter David and Togelius, Julian and Risi, Sebastian},
  year = {2019},
  month = aug,
  pages = {1--8},
  issn = {2325-4270},
  doi = {10.1109/CIG.2019.8848063},
  abstract = {We propose the popular board game Blood Bowl as a new challenge for Artificial Intelligence (AI). Blood Bowl is a fully-observable, stochastic, turn-based, modern-style board game with a grid-based game board. At first sight, the game ought to be approachable by numerous game-playing algorithms. However, as all pieces on the board belonging to a player can be moved several times each turn, the turn-wise branching factor becomes overwhelming for traditional algorithms. Additionally, scoring points in the game is rare and difficult, which makes it hard to design heuristics for search algorithms or apply reinforcement learning. We present the Fantasy Football AI (FFAI) framework that implements the core rules of Blood Bowl and includes a forward model, several OpenAI Gym environments for reinforcement learning, competition functionalities, and a web application that allows for human play. We also present Bot Bowl I, the first AI competition that will use FFAI along with baseline agents and preliminary reinforcement learning results. Additionally, we present a wealth of opportunities for future AI competitions based on FFAI.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Justesen et al. - 2019 - Blood Bowl A New Board Game Challenge and Competi.pdf;C\:\\Users\\aesou\\Zotero\\storage\\IAN3IGIT\\8848063.html},
  keywords = {AI competition,artificial intelligence,Blood,Blood Bowl,board game challenge,Bot Bowl,computer games,Conferences,Fantasy Football AI framework,game-playing algorithms,Games,grid-based game board,Injuries,Internet,learning (artificial intelligence),modern-style board game,OpenAI Gym environments,reinforcement learning,Reinforcement learning,search algorithms,Sports,turn-wise branching factor,Web application}
}

@inproceedings{justesen_continual_2017,
  title = {Continual Online Evolutionary Planning for In-Game Build Order Adaptation in {{StarCraft}}},
  booktitle = {Proceedings of the {{Genetic}} and {{Evolutionary Computation Conference}} on   - {{GECCO}} '17},
  author = {Justesen, Niels and Risi, Sebastian},
  year = {2017},
  pages = {187--194},
  publisher = {{ACM Press}},
  address = {{Berlin, Germany}},
  doi = {10.1145/3071178.3071210},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\XHB8WZJ6\\Justesen and Risi - 2017 - Continual online evolutionary planning for in-game.pdf},
  isbn = {978-1-4503-4920-8},
  language = {en}
}

@article{justesen_learning_2017,
  title = {Learning {{Macromanagement}} in {{StarCraft}} from {{Replays}} Using {{Deep Learning}}},
  author = {Justesen, Niels and Risi, Sebastian},
  year = {2017},
  month = jul,
  abstract = {The real-time strategy game StarCraft has proven to be a challenging environment for artificial intelligence techniques, and as a result, current state-of-the-art solutions consist of numerous hand-crafted modules. In this paper, we show how macromanagement decisions in StarCraft can be learned directly from game replays using deep learning. Neural networks are trained on 789,571 state-action pairs extracted from 2,005 replays of highly skilled players, achieving top-1 and top-3 error rates of 54.6\% and 22.9\% in predicting the next build action. By integrating the trained network into UAlbertaBot, an open source StarCraft bot, the system can significantly outperform the game's built-in Terran bot, and play competitively against UAlbertaBot with a fixed rush strategy. To our knowledge, this is the first time macromanagement tasks are learned directly from replays in StarCraft. While the best hand-crafted strategies are still the state-of-the-art, the deep network approach is able to express a wide range of different strategies and thus improving the network's performance further with deep reinforcement learning is an immediately promising avenue for future research. Ultimately this approach could lead to strong StarCraft bots that are less reliant on hard-coded strategies.},
  archivePrefix = {arXiv},
  eprint = {1707.03743},
  eprinttype = {arxiv},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\3Y2MYRJ4\\Justesen and Risi - 2017 - Learning Macromanagement in StarCraft from Replays.pdf},
  journal = {arXiv:1707.03743 [cs]},
  keywords = {Computer Science - Artificial Intelligence},
  language = {en},
  primaryClass = {cs}
}

@inproceedings{justesen_online_2016,
  title = {Online {{Evolution}} for {{Multi}}-Action {{Adversarial Games}}},
  booktitle = {Applications of {{Evolutionary Computation}}},
  author = {Justesen, Niels and Mahlmann, Tobias and Togelius, Julian},
  editor = {Squillero, Giovanni and Burelli, Paolo},
  year = {2016},
  pages = {590--603},
  publisher = {{Springer International Publishing}},
  abstract = {We present Online Evolution, a novel method for playing turn-based multi-action adversarial games. Such games, which include most strategy games, have extremely high branching factors due to each turn having multiple actions. In Online Evolution, an evolutionary algorithm is used to evolve the combination of atomic actions that make up a single move, with a state evaluation function used for fitness. We implement Online Evolution for the turn-based multi-action game Hero Academy and compare it with a standard Monte Carlo Tree Search implementation as well as two types of greedy algorithms. Online Evolution is shown to outperform these methods by a large margin. This shows that evolutionary planning on the level of a single move can be very effective for this sort of problems.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\3ESD7BDA\\Justesen et al. - 2016 - Online Evolution for Multi-action Adversarial Game.pdf},
  isbn = {978-3-319-31204-0},
  keywords = {Action Sequence,Game State,Spell Action,Strategy Game,Time Budget},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{justesen_playing_2018-1,
  title = {Playing {{Multiaction Adversarial Games}}: {{Online Evolutionary Planning Versus Tree Search}}},
  shorttitle = {Playing {{Multiaction Adversarial Games}}},
  author = {Justesen, Niels and Mahlmann, Tobias and Risi, Sebastian and Togelius, Julian},
  year = {2018},
  month = sep,
  volume = {10},
  pages = {281--291},
  issn = {2475-1502, 2475-1510},
  doi = {10.1109/TCIAIG.2017.2738156},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Justesen et al. - 2018 - Playing Multiaction Adversarial Games Online Evol 2.pdf},
  journal = {IEEE Transactions on Games},
  language = {en},
  number = {3}
}

@inproceedings{justesen_script-_2014,
  title = {Script- and Cluster-Based {{UCT}} for {{StarCraft}}},
  booktitle = {2014 {{IEEE Conference}} on {{Computational Intelligence}} and {{Games}}},
  author = {Justesen, Niels and Tillman, Balint and Togelius, Julian and Risi, Sebastian},
  year = {2014},
  month = aug,
  publisher = {{IEEE}},
  address = {{Dortmund, Germany}},
  doi = {10.1109/CIG.2014.6932900},
  abstract = {Monte Carlo methods have recently shown promise in real-time strategy (RTS) games, which are challenging because of their fast pace with simultaneous moves and massive branching factors. This paper presents two extensions to the Monte Carlo method UCT Considering Durations (UCTCD) for finding optimal sequences of actions for units engaged in combat in the RTS game StarCraft. The first extension is a script-based approach inspired by Portfolio Greedy Search and searches for sequences of scripts instead of actions. The second extension is a clusterbased approach as it assigns scripts to clusters of units based on their type and position. The presented results demonstrate that both the script-based and cluster-based UCTCD extensions outperform the original UCTCD with a winning percentage of 100\% for battles with 32 units or more. Additionally, unit clustering is shown to give some improvement in large scenarios while it is less effective in small combats. The algorithms were tested in our StarCraft combat simulator called JarCraft, a complete Java translation of the original C++ package SparCraft, made in hopes of making this research area more accessible.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\2ZJZKL98\\Justesen et al. - 2014 - Script- and cluster-based UCT for StarCraft.pdf},
  isbn = {978-1-4799-3547-5},
  language = {en}
}

@inproceedings{justesen_when_2019,
  title = {When {{Are We Done}} with {{Games}}?},
  booktitle = {2019 {{IEEE Conference}} on {{Games}} ({{CoG}})},
  author = {Justesen, Niels and Debus, Michael S. and Risi, Sebastian},
  year = {2019},
  month = aug,
  pages = {1--8},
  issn = {2325-4270},
  doi = {10.1109/CIG.2019.8847963},
  abstract = {From an early point, games have been promoted as important challenges within the research field of Artificial Intelligence (AI). Recent developments in machine learning have allowed a few AI systems to win against top professionals in even the most challenging video games, including Dota 2 and StarCraft. It thus may seem that AI has now achieved all of the long-standing goals that were set forth by the research community. In this paper, we introduce a black box approach that provides a pragmatic way of evaluating the fairness of AI vs. human competitions, by only considering motoric and perceptual fairness on the competitors' side. Additionally, we introduce the notion of extrinsic and intrinsic factors of a game competition and apply these to discuss and compare the competitions in relation to human vs. human competitions. We conclude that Dota 2 and StarCraft II are not yet mastered by AI as they so far only have been able to win against top professionals in limited competition structures in restricted variants of the games.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Justesen et al. - 2019 - When Are We Done with Games.pdf;C\:\\Users\\aesou\\Zotero\\storage\\FW6S9RU5\\8847963.html},
  keywords = {AI systems,artificial intelligence,black box approach,competition structures,computer games,Computers,Dota 2,extrinsic factors,game competition,Games,human competitions,intrinsic factors,learning (artificial intelligence),machine learning,Machine learning,Pragmatics,StarCraft II,Task analysis,Training,video games}
}

@article{kahng_clear_2018,
  title = {Clear the {{Fog}}: {{Combat Value Assessment}} in {{Incomplete Information Games}} with {{Convolutional Encoder}}-{{Decoders}}},
  author = {Kahng, Hyungu and Jeong, Yonghyun and Cho, Yoon Sang and Ahn, Gonie and Park, Young Joon and Jo, Uk and Lee, Hankyu and Do, Hyungrok and Lee, Junseung and Choi, Hyunjin and Yoon, Iljoo and Lee, Hyunjae and Jun, Daehun and Bae, Changhyeon and Kim, Seoung Bum},
  year = {2018},
  pages = {7},
  abstract = {StarCraft, one of the most popular real-time strategy games, is a compelling environment for artificial intelligence research for both micro-level unit control and macro-level strategic decision making. In this study, we address an eminent problem concerning macro-level strategic decision making, known as the ``fog-of-war'', which rises naturally from the fact that information regarding the opponent's state is always provided in incomplete form. For intelligent agents to play like human players, it is obvious that making accurate predictions of the opponent's status under incomplete information will increase its chance of winning. To reflect this fact, we propose a convolutional encoder-decoder architecture that predicts potential counts and locations of the opponent's units based on only partially visible, noisy information. To evaluate the performance of our proposed method, we train an additional classifier on the encoder-decoder output to predict the game outcome (win or lose). Finally, we designed an agent incorporating the proposed method and conducted simulation games against rule-based agents to demonstrate both effectiveness and practicality. All experiments were conducted on actual game replay data acquired from professional players.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\DHGIA7MS\\Kahng et al. - 2018 - Clear the Fog Combat Value Assessment in Incomple.pdf},
  language = {en}
}

@inproceedings{kahng_opponent_2020,
  title = {Opponent {{Modeling Under Partial Observability}} in {{StarCraft}} with {{Deep Convolutional Encoder}}-{{Decoders}}},
  booktitle = {Intelligent {{Systems}} and {{Applications}}},
  author = {Kahng, Hyungu and Kim, Seoung Bum},
  editor = {Bi, Yaxin and Bhatia, Rahul and Kapoor, Supriya},
  year = {2020},
  pages = {751--759},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-29516-5_56},
  abstract = {StarCraft, one of the most popular real-time strategy games, is a compelling environment for artificial intelligence research involving various tasks of both micro-level unit control and macro-level strategic decision making. In this study, we address an eminent problem of concern in macro-level decision making known as the ``fog-of-war'', which rises from the partial observable nature of the game. Recovering information hidden under the fog can help capture advantageous high-level game dynamics, such as build orders, tactics and strategies of the opponent. Casted as a supervised learning problem, we propose a convolutional encoder-decoder architecture to predict potential counts and locations of the opponent's units based on only partially visible and noisy state information. We visualize the model predictions on simplified grids to primarily evaluate the performance of our proposed method. Furthermore, we train an additional convolutional neural network classifier on the encoder-decoder outputs to predict the final winner of the game, as a means of demonstrating both effectiveness and applicability.},
  isbn = {978-3-030-29516-5},
  keywords = {Convolutional neural networks,Fog-of-war,StarCraft},
  language = {en},
  series = {Advances in {{Intelligent Systems}} and {{Computing}}}
}

@inproceedings{kantharaju_ccg_2018,
  title = {{{\textmu{}CCG}}, a {{CCG}}-Based {{Game}}-{{Playing Agent}} for {{\textmu{}RTS}}},
  booktitle = {{{IEEE Conference}} on {{Computational Intelligence}} and {{Games}} ({{CIG}})},
  author = {Kantharaju, Pavan and Ontanon, Santiago and Geib, Christopher W},
  year = {2018},
  abstract = {This paper presents a Combinatory Categorial Grammar-based game playing agent called \textmu{}CCG for the RealTime Strategy testbed \textmu{}RTS. The key problem that \textmu{}CCG tries to address is that of adversarial planning in the very large search space of RTS games. In order to address this problem, we present a new hierarchical adversarial planning algorithm based on Combinatory Categorial Grammars (CCGs). The grammar used by our planner is automatically learned from sequences of actions taken from game replay data. We provide an empirical analysis of our agent against agents from the CIG 2017 \textmu{}RTS competition using competition rules. \textmu{}CCG represents the first complete agent to use a learned formal grammar representation of plans to adversarially plan in RTS games.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\JUS9HMDA\\Kantharaju et al. - 2018 - µCCG, a CCG-based Game-Playing Agent for µRTS.pdf},
  language = {en}
}

@inproceedings{kantharaju_extracting_2019,
  title = {Extracting {{CCGs}} for {{Plan Recognition}} in {{RTS Games}}},
  booktitle = {Proceedings of the {{Workshop}} on {{Knowledge Extraction}} in {{Games}} 2019},
  author = {Kantharaju, Pavan and Onta{\~n}{\'o}n, Santiago and Geib, Christopher W},
  year = {2019},
  abstract = {Domain-configurable planning and plan recognition approaches such as Hierarchical Task Network and Combinatory Categorial Grammar-based (CCG) planning and plan recognition require a domain expert to handcraft a domain definition for each new domain in which we want to plan or recognize. This paper describes an approach to automatically extracting these definitions from plan traces acquired from Real-Time Strategy (RTS) game replays. Specifically, we present a greedy approach to learning CCGs from sets of plan trace, goal pairs that extends prior work on learning CCGs for plan recognition. We provide an empirical evaluation of our learning algorithm in the challenging domain of RTS games. Our results show that we are able to learn a CCG that represents larger sequences of actions, and use them for plan recognition. Our results also demonstrate how scaling the size of the plan traces affects the size of the learned representation, which paves the road for interesting future work.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Kantharaju et al. - 2019 - Extracting CCGs for Plan Recognition in RTS Games.pdf},
  language = {en}
}

@inproceedings{kantharaju_scaling_2019,
  title = {Scaling up {{CCG}}-{{Based Plan Recognition}} via {{Monte}}-{{Carlo Tree Search}}},
  booktitle = {{{IEEE Conference}} on {{Games}} 2019},
  author = {Kantharaju, Pavan and Onta{\~n}{\'o}n, Santiago and Geib, Christopher W},
  year = {2019},
  month = aug,
  abstract = {This paper focuses on the problem of scaling Combinatory Categorial Grammar (CCG)-based plan recognition to large CCG representations in the context of Real-Time Strategy (RTS) games. Specifically, we present a technique to scale plan recognition to large domain representations using Monte-Carlo Tree Search (MCTS). CCG-based planning and plan recognition (like other domain-configurable planning frameworks) require domain definitions to be either manually authored or learned from data. Prior work has demonstrated successful learning of these CCG domain definitions from data, but these representations can be very large for complex application domains. We propose a MCTS-based approach to search for explanations and predict the goal of a given sequence of observed actions. We evaluate our approach on the RTS game AI testbed microRTS. Our experimental results show our method scales better to these large, learned CCGs than previous CCG-based approaches.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Kantharaju et al. - 2019 - Scaling up CCG-Based Plan Recognition via Monte-Ca.pdf},
  language = {en}
}

@phdthesis{karlsson_exploring_2018,
  title = {Exploring a Video Game {{AI}} Bot That Scans and Reacts to Its Surroundings in Real-Time.},
  author = {Karlsson, Rasmus},
  year = {2018},
  abstract = {The buzz surrounding artificial intelligences continues to grow. They are currently used in a wide variety of systems and appliances, such as video games, virtual personal assistants, and self-driving cars. This paper explores
the possibility of a self-learning AI that can play the classic arcade game Q*BERT, using only screenshots as input. It is tested to work on several different screens sizes, and the results are collected and compared to that of a
human player, as well as results from previous research. The results are fairly positive. While the AI had a hard time of matching the human player on average score, it did get close to the highest score.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\FCTXHUAQ\\Karlsson - 2018 - Exploring a video game AI bot that scans and react.pdf;C\:\\Users\\aesou\\Zotero\\storage\\6WGYI6IU\\record.html},
  language = {eng}
}

@inproceedings{karnin_almost_2013,
  title = {Almost {{Optimal Exploration}} in {{Multi}}-{{Armed Bandits}}},
  booktitle = {Proceedings of the 30 Th {{International Conference}} on {{Machine Learning}}, {{Atlanta}}, {{Georgia}}, {{USA}}.},
  author = {Karnin, Zohar and Koren, Tomer and Somekh, Oren},
  year = {2013},
  pages = {9},
  abstract = {We study the problem of exploration in stochastic Multi-Armed Bandits. Even in the simplest setting of identifying the best arm, there remains a logarithmic multiplicative gap between the known lower and upper bounds for the number of arm pulls required for the task. This extra logarithmic factor is quite meaningful in nowadays large-scale applications. We present two novel, parameterfree algorithms for identifying the best arm, in two different settings: given a target confidence and given a target budget of arm pulls, for which we prove upper bounds whose gap from the lower bound is only doublylogarithmic in the problem parameters. We corroborate our theoretical results with experiments demonstrating that our algorithm outperforms the state-of-the-art and scales better as the size of the problem increases.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\TZJXLTQL\\Karnin et al. - 2013 - Almost Optimal Exploration in Multi-Armed Bandits.pdf},
  language = {en}
}

@incollection{kato_cheat-proof_2007,
  title = {Cheat-{{Proof Serverless Network Games}}},
  booktitle = {Computers and {{Games}}},
  author = {Kato, Shunsaku and Miyazaki, Shuichi and Nishimura, Yusuke and Okabe, Yasuo},
  year = {2007},
  volume = {4630},
  pages = {234--243},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-75538-8_21},
  abstract = {We consider playing online games on peer-to-peer networks, without assuming servers that control the execution of a game. In such an environment, players may cheat the opponent by, for example, illegally replacing the cards in their hands. The aim of this paper is to examine a possibility of excluding such cheatings. We show that by employing cryptographic techniques, we can exclude some types of cheating at some level. Finally, based on our discussion, we implement the cheat-proof network ``Gunjin-Shogi'', which is a variant of Japanese Chess.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2006\\26 Cheat-Proof Serverless Network Games.pdf},
  isbn = {978-3-540-75537-1 978-3-540-75538-8},
  language = {en}
}

@inproceedings{kato_parallel_2010,
  title = {Parallel {{Monte}}-{{Carlo Tree Search}} with {{Simulation Servers}}},
  booktitle = {2010 {{International Conference}} on {{Technologies}} and {{Applications}} of {{Artificial Intelligence}}},
  author = {Kato, Hideki and Takeuchi, Ikuo},
  year = {2010},
  month = nov,
  pages = {491--498},
  publisher = {{IEEE}},
  address = {{Hsinchu City, TBD, Taiwan}},
  doi = {10.1109/TAAI.2010.83},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Kato and Takeuchi - 2010 - Parallel Monte-Carlo Tree Search with Simulation S.pdf},
  isbn = {978-1-4244-8668-7},
  language = {en}
}

@inproceedings{khan_competitive_2018,
  title = {A {{Competitive Combat Strategy}} and {{Tactics}} in {{RTS Games AI}} and {{StarCraft}}},
  booktitle = {Advances in {{Multimedia Information Processing}} \textendash{} {{PCM}} 2017},
  author = {Khan, Adil and Yang, Kai and Fu, Yunsheng and Lou, Fang and Jifara, Worku and Jiang, Feng and Shaohui, Liu},
  editor = {Zeng, Bing and Huang, Qingming and El Saddik, Abdulmotaleb and Li, Hongliang and Jiang, Shuqiang and Fan, Xiaopeng},
  year = {2018},
  volume = {10736},
  pages = {3--12},
  publisher = {{Springer International Publishing}},
  address = {{Harbin, China}},
  doi = {10.1007/978-3-319-77383-4_1},
  abstract = {This paper presents a competitive combat strategy and tactics in RTS Games AI. To put it simply, if a player is building up base, he is losing out on creating an army and If he is building up his army, he is losing out on having a strong base. The key to winning, in StarCraft or any other RTS game is to balance strategy, tactics, macro and micro. To improve the game, one has to be able to keep track of everything that's going on over the entire map. And one must be able to give orders quickly and efficiently so in this paper we propose a competitive battle strategy with the help of a plot and decision tree. We simulate the strategy in MicroRTS developed in java EE by conducting a game-play between human player and MicroRTS AI (Game AI), though our proposed strategy outperforms the Game AI rarely as we did not account game playing-speed that makes a huge difference in victory but at least we succeeded in introducing a strategy that could well compete the Game AI and may defeat it but rarely.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\VPBGR8JQ\\Khan et al. - 2018 - A Competitive Combat Strategy and Tactics in RTS G.pdf},
  isbn = {978-3-319-77382-7 978-3-319-77383-4},
  language = {en}
}

@inproceedings{kiarostami_multi-agent_2019,
  title = {Multi-{{Agent}} Non-{{Overlapping Pathfinding}} with {{Monte}}-{{Carlo Tree Search}}},
  booktitle = {2019 {{IEEE Conference}} on {{Games}} ({{CoG}})},
  author = {Kiarostami, Mohammad Sina and Reza Daneshvaramoli, Mohammad and Monfared, Saleh Khalaj and Rahmati, Dara and Gorgin, Saeid},
  year = {2019},
  month = aug,
  pages = {1--4},
  issn = {2325-4289},
  doi = {10.1109/CIG.2019.8848043},
  abstract = {In this work, we propose a novel implementation of Monte-Carlo Tree Search (MCTS) algorithm to solve a multiagent pathfinding (MAPF) problem. We employ an optimization of MCTS with low time-complexity and acceptable reliability to approach the MAPF problems with no time constraint. To examine the efficiency and performance of the proposed approach, the NumberLink problem as a MAPF is investigated. We show that the addressed problem could be characterized as multi-agent pathfinding problem with no overlapping paths for the agents. Furthermore, we define this problem to be a simplified and special case of Multi-commodity flow problem (MCFP). Our MCTS solution utilizes a modified search-tree structure to efficiently solve the problem based on a 2-dimensional search space which performs in quadratic time complexity (O(m4) where input size is m2) and linear memory complexity (O(m2)). To evaluate our algorithm, we investigate the efficiency of the proposed solution for the well-known Flow Free puzzle. Our implementation solves a large 40 \texttimes{} 40 Numberlink puzzle in 21 minutes. To the best of our knowledge, there is no other efficient solution for this puzzle where the size of the problem is considerably large.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Kiarostami et al. - 2019 - Multi-Agent non-Overlapping Pathfinding with Monte.pdf;C\:\\Users\\aesou\\Zotero\\storage\\FJID2JYZ\\8848043.html},
  keywords = {computational complexity,Flow Free,graph theory,linear memory complexity,MAPF problems,MCTS solution,modified search-tree structure,Monte Carlo,Monte Carlo methods,Monte-Carlo tree search algorithm,Multi-agent,multiagent pathfinding problem,multicommodity flow problem,Numberlink,numberlink problem,numberlink puzzle,Optimization,Pathfinding,quadratic time complexity,Reliability,search problems,Search problems,time 21.0 min,Time complexity,time constraint,Time factors,tree searching,trees (mathematics)}
}

@article{kim_game_2013,
  title = {Game {{AI Competitions}}: {{An Open Platform}} for {{Computational Intelligence Education}} [{{Educational Forum}}]},
  shorttitle = {Game {{AI Competitions}}},
  author = {Kim, K. and Cho, S.},
  year = {2013},
  month = aug,
  volume = {8},
  pages = {64--68},
  issn = {1556-603X},
  doi = {10.1109/MCI.2013.2264568},
  abstract = {Teaching computational intelligence (CI) to undergraduate and/or graduate students is challenging because the theories are difficult and they feel that the topics are not closely related to their lives. It is desirable to use interesting projects to attract students' attention into the computational intelligence. CI educators have been used different types of tools to define course projects for students. There are no standard course materials on the projects, hence each instructor develops his/her own projects by oneself, making it difficult to share them. Furthermore, if the projects are dependent on special hardware or software, it often becomes a bottleneck on the dissemination of the materials.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\89LUN98P\\Kim and Cho - 2013 - Game AI Competitions An Open Platform for Computa.pdf;C\:\\Users\\aesou\\Zotero\\storage\\JDB9226E\\6557046.html},
  journal = {IEEE Computational Intelligence Magazine},
  keywords = {artificial intelligence,CI,Computational intelligence,computational intelligence education,computer aided instruction,computer games,course projects,Education courses,educational courses,educational forum,Engineering education,Evolutionary computing,game AI competitions,graduate students,open platform,standard course materials,Standards,teaching computational intelligence},
  number = {3}
}

@article{kim_performance_2018,
  title = {Performance {{Evaluation Gaps}} in a {{Real}}-{{Time Strategy Game Between Human}} and {{Artificial Intelligence Players}}},
  author = {Kim, Man-Je and Kim, Kyung-Joong and Kim, Seungjun and Dey, Anind K.},
  year = {2018},
  volume = {6},
  pages = {13575--13586},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2018.2800016},
  abstract = {Since 2010, annual StarCraft artificial intelligence (AI) competitions have promoted the development of successful AI players for complex real-time strategy games. In these competitions, AI players are ranked based on their win ratio over thousands of head-to-head matches. Although simple and easily implemented, this evaluation scheme may less adequately help develop more human-competitive AI players. In this paper, we recruited 45 human StarCraft players at different expertise levels (expert/medium/novice) and asked them to play against the 18 top AI players selected from the five years of competitions (2011\textendash{}2015). The results show that the human evaluations of AI players differ substantially from the current standard evaluation and ranking method. In fact, from a human standpoint, there has been little progress in the quality of StarCraft AI players over the years. It is even possible that AI-only tournaments can lead to AIs being created that are unacceptable competitors for humans. This paper is the first to systematically explore the human evaluation of AI players, the evolution of AI players, and the differences between human perception and tournament-based evaluations. The discoveries from this paper can support AI developers in game companies and AI tournament organizers to better incorporate the perspective of human users into their AI systems.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\JRF2WE9F\\Kim et al. - 2018 - Performance Evaluation Gaps in a Real-Time Strateg.pdf},
  journal = {IEEE Access},
  language = {en}
}

@book{kirby_introduction_2011,
  title = {Introduction to Game {{AI}}},
  author = {Kirby, Neil},
  year = {2011},
  publisher = {{Course Technology/Cengage Learning}},
  address = {{Boston}},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\ZWTJK7J4\\Kirby - 2011 - Introduction to game AI.pdf},
  isbn = {978-1-59863-998-8},
  keywords = {Artificial intelligence,Computer games,Design,Programming},
  language = {en},
  lccn = {QA76.76.C672 K57 2011}
}

@incollection{kishimoto_about_2008,
  title = {About the {{Completeness}} of {{Depth}}-{{First Proof}}-{{Number Search}}},
  booktitle = {Computers and {{Games}}},
  author = {Kishimoto, Akihiro and M{\"u}ller, Martin},
  year = {2008},
  volume = {5131},
  pages = {146--156},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-87608-3_14},
  abstract = {Depth-first proof-number (df-pn) search is a powerful member of the family of algorithms based on proof and disproof numbers. While df-pn has succeeded in practice, its theoretical properties remain poorly understood. This paper resolves the question of completeness of df-pn: its ability to solve any finite boolean-valued game tree search problem in principle, given unlimited amounts of time and memory. The main results are that df-pn is complete on finite directed acyclic graphs (DAG) but incomplete on finite directed cyclic graphs (DCG).},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2008\\19 About the Completeness of Depth-First Proof-Number Search.pdf},
  isbn = {978-3-540-87607-6 978-3-540-87608-3},
  language = {en}
}

@incollection{kloetzer_monte-carlo_2011,
  title = {Monte-{{Carlo Opening Books}} for {{Amazons}}},
  booktitle = {Computers and {{Games}}},
  author = {Kloetzer, Julien},
  year = {2011},
  volume = {6515},
  pages = {124--135},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-17928-0_12},
  abstract = {Automatically creating opening books is a natural step towards the building of strong game-playing programs, especially when there is little available knowledge about the game. However, while recent popular Monte-Carlo Tree-Search programs showed strong results for various games, we show here that programs based on such methods cannot efficiently use opening books created using algorithms based on minimax. To overcome this issue, we propose to use an MCTS-based technique, Meta-MCTS, to create such opening books. This method, while requiring some tuning to arrive at the best opening book possible, shows promising results to create an opening book for the game of the Amazons, even if this is at the cost of removing its Monte-Carlo part.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2010\\17 Monte-Carlo Opening Books for Amazons.pdf},
  isbn = {978-3-642-17927-3 978-3-642-17928-0},
  language = {en}
}

@article{knuth_analysis_1975,
  title = {An Analysis of Alpha-Beta Pruning},
  author = {Knuth, Donald E. and Moore, Ronald W.},
  year = {1975},
  month = dec,
  volume = {6},
  pages = {293--326},
  issn = {0004-3702},
  doi = {10.1016/0004-3702(75)90019-3},
  abstract = {The alpha-beta technique for searching game trees is analyzed, in an attempt to provide some insight into its behavior. The first portion of this paper is an expository presentation of the method together with a proof of its correctness and a historical discussion. The alpha-beta procedure is shown to be optimal in a certain sense, and bounds are obtained for its running time with various kinds of random data.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Knuth and Moore - 1975 - An analysis of alpha-beta pruning.pdf;C\:\\Users\\aesou\\Zotero\\storage\\C552XZQS\\0004370275900193.html},
  journal = {Artificial Intelligence},
  number = {4}
}

@inproceedings{kocsis_bandit_2006,
  title = {Bandit {{Based Monte}}-{{Carlo Planning}}},
  booktitle = {Machine {{Learning}}: {{ECML}} 2006},
  author = {Kocsis, Levente and Szepesv{\'a}ri, Csaba},
  editor = {F{\"u}rnkranz, Johannes and Scheffer, Tobias and Spiliopoulou, Myra},
  year = {2006},
  pages = {282--293},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {For large state-space Markovian Decision Problems Monte-Carlo planning is one of the few viable approaches to find near-optimal solutions. In this paper we introduce a new algorithm, UCT, that applies bandit ideas to guide Monte-Carlo planning. In finite-horizon or discounted MDPs the algorithm is shown to be consistent and finite sample bounds are derived on the estimation error due to sampling. Experimental results show that in several domains, UCT is significantly more efficient than its alternatives.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Kocsis and Szepesvári - 2006 - Bandit Based Monte-Carlo Planning.pdf;C\:\\Users\\aesou\\Zotero\\storage\\LPDL4HBE\\Kocsis and Szepesvári - 2006 - Bandit Based Monte-Carlo Planning.pdf},
  isbn = {978-3-540-46056-5},
  keywords = {Bandit Problem,Drift Condition,Failure Probability,Multiarmed Bandit,Multiarmed Bandit Problem},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{kocsis_two_2003,
  title = {{{TWO LEARNING ALGORITHMS FOR FORWARD PRUNING}}},
  author = {Kocsis, Levente and {van den Herik}, H. Jaap and Uiterwijk, Jos W. H. M.},
  year = {2003},
  month = jan,
  volume = {26},
  pages = {165--181},
  publisher = {{IOS Press}},
  issn = {1389-6911},
  doi = {10.3233/ICG-2003-26303},
  abstract = {The article investigates two learning algorithms for forward pruning. The TS-FPV algorithm uses a tabu-search (TS) algorithm to explore the space of the forward-pruning vectors (FPVs). It focuses on critical FPVs. The RL-FPF algorithm is a reinforcem},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Kocsis et al. - 2003 - TWO LEARNING ALGORITHMS FOR FORWARD PRUNING.pdf;C\:\\Users\\aesou\\Zotero\\storage\\FLH4B34B\\icg26303.html},
  journal = {ICGA Journal},
  language = {en},
  number = {3}
}

@inproceedings{komenda_robustness_2014,
  title = {On {{Robustness}} of {{CMAB Algorithms}}: {{Experimental Approach}}},
  shorttitle = {On {{Robustness}} of {{CMAB Algorithms}}},
  booktitle = {Computer {{Games}}},
  author = {Komenda, Anton{\'i}n and Shleyfman, Alexander and Domshlak, Carmel},
  editor = {Cazenave, Tristan and Winands, Mark H. M. and Bj{\"o}rnsson, Yngvi},
  year = {2014},
  pages = {16--28},
  publisher = {{Springer International Publishing}},
  abstract = {In online planning with a team of cooperative agents, a straightforward model for decision making which actions the agents should execute can be represented as the problem of Combinatorial Multi-Armed Bandit. Similarly to the most prominent approaches for online planning with polynomial number of possible actions, state-of-the-art algorithms for online planning with exponential number of actions are based on Monte-Carlo sampling. However, without a proper selection of the appropriate subset of actions these techniques cannot be used. The most recent algorithms tackling this problem utilize an assumption of linearity with respect to the combinations of the actions.In this paper, we experimentally analyze robustness of two state-of-the-art algorithms NMC and LSI for online planning with combinatorial actions in various setups of Real-Time and Turn-Taking Strategy games.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\CU3JYGIM\\Komenda et al. - 2014 - On Robustness of CMAB Algorithms Experimental App.pdf},
  isbn = {978-3-319-14923-3},
  keywords = {Atomic Action,Combinatorial Action,Online Planning,Original Game,Tree Search Algorithm},
  language = {en},
  series = {Communications in {{Computer}} and {{Information Science}}}
}

@article{kostler_multi-objective_2013,
  title = {A {{Multi}}-Objective {{Genetic Algorithm}} for {{Build Order Optimization}} in {{StarCraft II}}},
  author = {K{\"o}stler, Harald and Gmeiner, Bj{\"o}rn},
  year = {2013},
  month = aug,
  volume = {27},
  pages = {221--233},
  issn = {0933-1875, 1610-1987},
  doi = {10.1007/s13218-013-0263-2},
  abstract = {This article presents a modified version of the multi-objective genetic algorithm NSGA II in order to find optimal opening strategies in the real-time strategy game StarCraft II. Based on an event-driven simulator capable of performing an accurate estimate of in-game build-times the quality of different build lists can be judged. These build lists are used as chromosomes within the genetic algorithm. Procedural constraints e.g. given by the Tech-Tree or other game mechanisms, are implicitly encoded into them. Typical goals are to find the build list producing most units of one or more certain types up to a certain time (Rush) or to produce one unit as early as possible (Tech-Push). Here, the number of entries in a build list varies and the objective values have in contrast to the search space a very small diversity. We introduce our game simulator including its graphical user interface, the modifications necessary to fit the genetic algorithm to our problem, test our algorithm on different Tech-Pushes and Rushes for all three races, and validate it with empirical data of expert StarCraft II players.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Köstler and Gmeiner - 2013 - A Multi-objective Genetic Algorithm for Build Orde.pdf},
  journal = {KI - K\"unstliche Intelligenz},
  language = {en},
  number = {3}
}

@inproceedings{kovarsky_heuristic_2005,
  title = {Heuristic {{Search Applied}} to {{Abstract Combat Games}}},
  booktitle = {Advances in {{Artificial Intelligence}}},
  author = {Kovarsky, Alexander and Buro, Michael},
  editor = {K{\'e}gl, Bal{\'a}zs and Lapalme, Guy},
  year = {2005},
  pages = {66--78},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {Creating strong AI forces in military war simulations or RTS video games poses many challenges including partially observable states, a possibly large number of agents and actions, and simultaneous concurrent move execution. In this paper we consider a tactical sub\textendash{}problem that needs to be addressed on the way to strong computer generated forces: abstract combat games in which a small number of inhomogeneous units battle with each other in simultaneous move rounds until all members of one group are eliminated. We present and test several adversarial heuristic search algorithms that are able to compute reasonable actions in those scenarios using short time controls. Tournament results indicate that a new algorithm for simultaneous move games which we call ``randomized alpha\textendash{}beta search'' (RAB) can be used effectively in the abstract combat application we consider. In this application it outperforms the other algorithms we implemented. We also show that RAB's performance is correlated with the degree of simultaneous move interdependence present in the game.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\YE729TQB\\Kovarsky and Buro - 2005 - Heuristic Search Applied to Abstract Combat Games.pdf},
  isbn = {978-3-540-31952-8},
  keywords = {Defensive Action,Monte Carlo,Node Limit,Search Depth,Simultaneous Move},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{kowalczuk_jamesbot_2019,
  title = {{{JamesBot}} - an Intelligent Agent Playing {{StarCraft II}}},
  booktitle = {2019 24th {{International Conference}} on {{Methods}} and {{Models}} in {{Automation}} and {{Robotics}} ({{MMAR}})},
  author = {Kowalczuk, Zdzis{\l}aw and Cybulski, Jan and Czubenko, Micha{\l}},
  year = {2019},
  month = aug,
  pages = {105--110},
  issn = {null},
  doi = {10.1109/MMAR.2019.8864611},
  abstract = {The most popular method for optimizing a certain strategy based on a reward is Reinforcement Learning (RL). Lately, a big challenge for this technique are computer games such as StarCraft II which is a real-time strategy game, created by Blizzard. The main idea of this game is to fight between agents and control objects on the battlefield in order to defeat the enemy. This work concerns creating an autonomous bot using reinforced learning, in particular, the Q-Learning algorithm for playing StarCraft. JamesBot consists of three parts. State Manager processes relevant information from the environment. Decision Manager consists of a table implementation of the Q-Learning algorithm, which assigns actions to states, and the epsilon-greedy strategy, which determines the behavior of the bot. In turn, Action Manager is responsible for executing commands. Testing bots involves fighting the default (simple) agent built into the game. Although JamesBot played better than the default (random) agent, it failed to gain the ability to defeat the opponent. The obtained results, however, are quite promising in terms of the possibilities of further development.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Kowalczuk et al. - 2019 - JamesBot - an intelligent agent playing StarCraft .pdf},
  keywords = {Action Manager,autonomous bot,Buildings,computer games,Decision Manager,epsilon-greedy strategy,Games,Informatics,intelligent agent,Intelligent agents,JamesBot,learning (artificial intelligence),machine learning,multi-agent systems,Q-Learning,Q-Learning algorithm,real-time strategy game,Real-time systems,reinforced learning,reinforcement learning,Reinforcement learning,RL,StarCraft II,State Manager,Telecommunications}
}

@article{kowalczyk_real-time_2018,
  title = {Real-Time Strategy Video Game Experience and Structural Connectivity - {{A}} Diffusion Tensor Imaging Study},
  author = {Kowalczyk, Natalia and Shi, Feng and Magnuski, Mikolaj and Skorko, Maciek and Dobrowolski, Pawel and Kossowski, Bartosz and Marchewka, Artur and Bielecki, Maksymilian and Kossut, Malgorzata and Brzezicka, Aneta},
  year = {2018},
  month = sep,
  volume = {39},
  pages = {3742--3758},
  issn = {10659471},
  doi = {10.1002/hbm.24208},
  abstract = {Experienced video game players exhibit superior performance in visuospatial cognition when compared to non-players. However, very little is known about the relation between video game experience and structural brain plasticity. To address this issue, a direct comparison of the white matter brain structure in RTS (real time strategy) video game players (VGPs) and non-players (NVGPs) was performed. We hypothesized that RTS experience can enhance connectivity within and between occipital and parietal regions, as these regions are likely to be involved in the spatial and visual abilities that are trained while playing RTS games. The possible influence of long-term RTS game play experience on brain structural connections was investigated using diffusion tensor imaging (DTI) and a region of interest (ROI) approach in order to describe the experience-related plasticity of white matter. Our results revealed significantly more total white matter connections between occipital and parietal areas and within occipital areas in RTS players compared to NVGPs. Additionally, the RTS group had an altered topological organization of their structural network, expressed in local efficiency within the occipito-parietal subnetwork. Furthermore, the positive association between network metrics and time spent playing RTS games suggests a close relationship between extensive, long-term RTS game play and neuroplastic changes. These results indicate that long-term and extensive RTS game experience induces alterations along axons that link structures of the occipito-parietal loop involved in spatial and visual processing.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\2YXGLZQC\\Kowalczyk et al. - 2018 - Real-time strategy video game experience and struc.pdf},
  journal = {Human Brain Mapping},
  language = {en},
  number = {9}
}

@inproceedings{kowalski_game_2015,
  title = {Game {{Description Language}} for {{Real}}-Time {{Games}}},
  booktitle = {Proceedings of the {{IJCAI}}-15 {{Workshop}} on {{General Game Playing}} ({{GIGA}}'15)},
  author = {Kowalski, Jakub and Kisielewicz, Andrzej},
  year = {2015},
  pages = {8},
  abstract = {We present a simple extension of the Game Description Language GDL that makes it possible to describe a large variety of games involving a realtime factor. Apart from the formal syntax and semantics of our extension (acronymed rtGDL), we provide also a series of illustrative examples. In addition, we show that by combining rtGDL with the GDL-II extension one may obtain a simple and elegant universal game description language that enables the formalization of the rules of arbitrary nplayer games allowing randomness, imperfect information and time-dependent events.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\DN6HEA9S\\Kowalski and Kisielewicz - 2015 - Game Description Language for Real-time Games.pdf},
  language = {en}
}

@inproceedings{kuchem_multi-objective_2013,
  title = {Multi-Objective Assessment of Pre-Optimized Build Orders Exemplified for {{StarCraft}} 2},
  booktitle = {2013 {{IEEE Conference}} on {{Computational Inteligence}} in {{Games}} ({{CIG}})},
  author = {Kuchem, Matthias and Preuss, Mike and Rudolph, Gunter},
  year = {2013},
  month = aug,
  pages = {1--8},
  publisher = {{IEEE}},
  address = {{Niagara Falls, ON, Canada}},
  doi = {10.1109/CIG.2013.6633626},
  abstract = {Modern realtime strategy (RTS) games as StarCraft 2 educe so-called metagames in which the players compete for the best strategies. The metagames of complex RTS games thrive in the absence of apparent dominant strategies, and developers will intervene to adjust the game when such strategies arise in public. However, there are still strategies considered as strong and ones thought of as weak. For the Zerg faction in StarCraft 2, we show how strong strategies can be identified by taking combat strength and economic power into account. The multi-objective perspective enables us to clearly rule out the unfavourable ones of the single optimal build orders and thus selects interesting openings to be tested by real players. By this means, we are e.g. able to explain the success of the recently proposed 7-roach opening. While we demonstrate our approach for StarCraft 2 only, it is of course applicable to other RTS games, given build-order optimization tools exist.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\CT56C3EN\\Kuchem et al. - 2013 - Multi-objective assessment of pre-optimized build .pdf},
  isbn = {978-1-4673-5311-3 978-1-4673-5308-3},
  language = {en}
}

@incollection{kupferschmid_skat_2007,
  title = {A {{Skat Player Based}} on {{Monte}}-{{Carlo Simulation}}},
  booktitle = {Computers and {{Games}}},
  author = {Kupferschmid, Sebastian and Helmert, Malte},
  year = {2007},
  volume = {4630},
  pages = {135--147},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-75538-8_12},
  abstract = {We apply Monte-Carlo simulation and alpha-beta search to the card game of Skat, which is similar to Bridge, but sufficiently different to require some new algorithmic ideas besides the techniques developed for Bridge. Our Skat-playing program, called DDS (Double Dummy Solver), integrates well-known techniques such as move ordering with two new search enhancements. Quasi-symmetry reduction generalizes symmetry reductions, disseminated by Ginsberg's Partition Search algorithm, to search states which are ``almost equivalent''. Adversarial heuristics generalize ideas from single-agent search algorithms like A{${_\ast}$} to two-player games, leading to guaranteed lower and upper bounds for the score of a game position. Combining these techniques with state-of-theart tree-search algorithms, our program determines the game-theoretical value of a typical Skat hand (with perfect information) in 10 milliseconds.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2006\\17 A Skat Player Based on Monte-Carlo Simulation.pdf},
  isbn = {978-3-540-75537-1 978-3-540-75538-8},
  language = {en}
}

@article{kurach_google_2019,
  title = {Google {{Research Football}}: {{A Novel Reinforcement Learning Environment}}},
  shorttitle = {Google {{Research Football}}},
  author = {Kurach, Karol and Raichuk, Anton and Sta{\'n}czyk, Piotr and Zaj{\k{a}}c, Micha{\l} and Bachem, Olivier and Espeholt, Lasse and Riquelme, Carlos and Vincent, Damien and Michalski, Marcin and Bousquet, Olivier and Gelly, Sylvain},
  year = {2019},
  month = jul,
  abstract = {Recent progress in the field of reinforcement learning has been accelerated by virtual learning environments such as video games, where novel algorithms and ideas can be quickly tested in a safe and reproducible manner. We introduce the Google Research Football Environment, a new reinforcement learning environment where agents are trained to play football in an advanced, physics-based 3D simulator. The resulting environment is challenging, easy to use and customize, and it is available under a permissive open-source license. In addition, it provides support for multiplayer and multi-agent experiments. We propose three full-game scenarios of varying difficulty with the Football Benchmarks and report baseline results for three commonly used reinforcement algorithms (IMPALA, PPO, and Ape-X DQN). We also provide a diverse set of simpler scenarios with the Football Academy and showcase several promising research directions.},
  archivePrefix = {arXiv},
  eprint = {1907.11180},
  eprinttype = {arxiv},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Kurach et al. - 2019 - Google Research Football A Novel Reinforcement Le.pdf},
  journal = {arXiv:1907.11180 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{laird_human-level_2001,
  title = {Human-{{Level AI}}'s {{Killer Application}}: {{Interactive Computer Games}}},
  shorttitle = {Human-{{Level AI}}'s {{Killer Application}}},
  author = {Laird, John and VanLent, Michael},
  year = {2001},
  month = jun,
  volume = {22},
  pages = {15--15},
  issn = {2371-9621},
  doi = {10.1609/aimag.v22i2.1558},
  copyright = {Copyright (c)},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\VL5SWZCS\\Laird and VanLent - 2001 - Human-Level AI's Killer Application Interactive C.pdf;C\:\\Users\\aesou\\Zotero\\storage\\I9TZAJI5\\1558.html},
  journal = {AI Magazine},
  language = {en},
  number = {2}
}

@inproceedings{lanctot_monte_2013,
  title = {Monte {{Carlo Tree Search}} for {{Simultaneous Move Games}}: {{A Case Study}} in the {{Game}} of {{Tron}}},
  shorttitle = {Monte {{Carlo Tree Search}} for {{Simultaneous Move Games}}},
  booktitle = {{{BNAIC}} 2013: {{Proceedings}} of the 25th {{Benelux Conference}} on {{Artificial Intelligence}}, {{Delft}}, {{The Netherlands}}, {{November}} 7-8, 2013},
  author = {Lanctot, M. and Wittlinger, C. and Den Teuling, N. G. P. and Winands, M. H. M.},
  year = {2013},
  month = nov,
  publisher = {{Delft University of Technology (TU Delft); under the auspices of the Benelux Association for Artificial Intelligence (BNVKI) and the Dutch Research School for Information and Knowledge Systems (SIKS)}},
  abstract = {MCTS has been successfully applied to many sequential games. This paper investigates Monte Carlo Tree Search (MCTS) for the simultaneous move game Tron. In this paper we describe two different ways to model the simultaneous move game, as a standard sequential game and as a stacked matrix game. Several variants are presented to adapt MCTS to simultaneous move games, such as Sequential UCT, Decoupled UCT, Exp3, and a novel stochastic method based on Regret Matching. Through the experiments in the game of Tron on four different boards, it is shown that Decoupled UCB1-Tuned perform best, winning 62.3\% of games overall. We also show that Regret Matching wins 53.1\% of games overall and search techniques that model the game sequentially win 51.4-54.3\% of games overall.},
  copyright = {(c) 2013 Lanctot, M.; Wittlinger, C.; Den Teuling, N.G.P.; Winands, M.H.M.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Lanctot et al. - 2013 - Monte Carlo Tree Search for Simultaneous Move Game.pdf;C\:\\Users\\aesou\\Zotero\\storage\\KCRRHM6P\\uuid814a4ba7-9508-410e-ba12-79de980cb67b.html},
  language = {en}
}

@article{lara-cabrera_procedural_2015,
  title = {Procedural {{Content Generation}} for {{Real}}-{{Time Strategy Games}}},
  author = {{Lara-Cabrera}, Ra{\'u}l and {Nogueira-Collazo}, Mariela and Cotta, Carlos and {Fern{\'a}ndez-Leiva}, Antonio},
  year = {2015},
  volume = {3},
  pages = {40},
  issn = {1989-1660},
  doi = {10.9781/ijimai.2015.325},
  abstract = {Videogames are one of the most important and profitable sectors in the industry of entertainment. Nowadays, the creation of a videogame is often a large-scale endeavor and bears many similarities with, e.g., movie production. On the central tasks in the development of a videogame is content generation, namely the definition of maps, terrains, non-player characters (NPCs) and other graphical, musical and AI-related components of the game. Such generation is costly due to its complexity, the great amount of work required and the need of specialized manpower. Hence the relevance of optimizing the process and alleviating costs. In this sense, procedural content generation (PCG) comes in handy as a means of reducing costs by using algorithmic techniques to automatically generate some game contents. PCG also provides advantages in terms of player experience since the contents generated are typically not fixed but can vary in different playing sessions, and can even adapt to the player herself. For this purpose, the underlying algorithmic technique used for PCG must be also flexible and adaptable. This is the case of computational intelligence in general and evolutionary algorithms in particular. In this work we shall provide an overview of the use of evolutionary intelligence for PCG, with special emphasis on its use within the context of realtime strategy games. We shall show how these techniques can address both playability and aesthetics, as well as improving the game AI.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\AM39DW6R\\Lara-Cabrera et al. - 2015 - Procedural Content Generation for Real-Time Strate.pdf},
  journal = {International Journal of Interactive Multimedia and Artificial Intelligence},
  language = {en},
  number = {2}
}

@inproceedings{lara-cabrera_review_2013,
  title = {A Review of Computational Intelligence in {{RTS}} Games},
  booktitle = {2013 {{IEEE Symposium}} on {{Foundations}} of {{Computational Intelligence}} ({{FOCI}})},
  author = {{Lara-Cabrera}, Raul and Cotta, Carlos and {Fernandez-Leiva}, Antonio J.},
  year = {2013},
  month = apr,
  pages = {114--121},
  publisher = {{IEEE}},
  address = {{Singapore, Singapore}},
  doi = {10.1109/FOCI.2013.6602463},
  abstract = {Real-time strategy games offer a wide variety of fundamental AI research challenges. Most of these challenges have applications outside the game domain. This paper provides a review on computational intelligence in real-time strategy games (RTS). It starts with challenges in real-time strategy games, then it reviews different tasks to overcome this challenges. Later, it describes the techniques used to solve this challenges and it makes a relationship between techniques and tasks. Finally, it presents a set of different frameworks used as test-beds for the techniques employed. This paper is intended to be a starting point for future researchers on this topic.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\BAWU3LC7\\Lara-Cabrera et al. - 2013 - A review of computational intelligence in RTS game.pdf},
  isbn = {978-1-4673-5901-6},
  language = {en}
}

@inproceedings{lara-cabrera_self-adaptive_2014,
  title = {A Self-Adaptive Evolutionary Approach to the Evolution of Aesthetic Maps for a {{RTS}} Game},
  booktitle = {2014 {{IEEE Congress}} on {{Evolutionary Computation}} ({{CEC}})},
  author = {{Lara-Cabrera}, Raul and Cotta, Carlos and {Fernandez-Leiva}, Antonio J.},
  year = {2014},
  month = jul,
  pages = {298--304},
  publisher = {{IEEE}},
  address = {{Beijing, China}},
  doi = {10.1109/CEC.2014.6900562},
  abstract = {Procedural content generation (PCG) is a research field on the rise, with numerous papers devoted to this topic. This paper presents a PCG method based on a self-adaptive evolution strategy for the automatic generation of maps for the real-time strategy (RTS) game Planet Wars. These maps are generated in order to fulfill the aesthetic preferences of the user, as implied by her assessment of a collection of maps used as training set. A topological approach is used for the characterization of the maps and their subsequent evaluation: the sphere-of-influence graph (SIG) of each map is built, several graph-theoretic measures are computed on it, and a feature selection method is utilized to determine adequate subsets of measures to capture the class of the map. A multiobjective evolutionary algorithm is subsequently employed to evolve maps, using these feature sets in order to measure distance to good (aesthetic) and bad (non-aesthetic) maps in the training set. The so-obtained results are visually analyzed and compared to the target maps using a Kohonen network.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\9KRZNVQS\\Lara-Cabrera et al. - 2014 - A self-adaptive evolutionary approach to the evolu.pdf},
  isbn = {978-1-4799-1488-3 978-1-4799-6626-4},
  language = {en}
}

@incollection{leckie_monte-carlo_2007,
  title = {Monte-{{Carlo Methods}} in {{Pool Strategy Game Trees}}},
  booktitle = {Computers and {{Games}}},
  author = {Leckie, Will and Greenspan, Michael},
  year = {2007},
  volume = {4630},
  pages = {244--255},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-75538-8_22},
  abstract = {An Eight Ball pool strategy algorithm with look-ahead is presented. The strategy uses a probabilistically evaluated game search tree to discover the best shot to attempt at each turn. Performance results of the strategy algorithm from a simulated tournament are presented. Players looking further ahead in the search tree performed better against their shallower-searching competitors, at the expense of larger execution time. The advantage of a deeper search tree was magnified for players with greater shooting precision.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2006\\27 Monte-Carlo Methods in Pool Strategy Game Trees.pdf},
  isbn = {978-3-540-75537-1 978-3-540-75538-8},
  language = {en}
}

@article{lecun_deep_2015,
  title = {Deep Learning},
  author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  year = {2015},
  month = may,
  volume = {521},
  pages = {436--444},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature14539},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\WT3JXSS4\\LeCun et al. - 2015 - Deep learning.pdf},
  journal = {Nature},
  language = {en},
  number = {7553}
}

@inproceedings{lee_exploring_2019,
  title = {{{EXPLORING TO LEARN WINNING STRATEGY}}},
  booktitle = {{{IADIS International Conference Interfaces}} and {{Human Computer Interaction}} 2019 (Part of {{MCCSIS}} 2019)},
  author = {Lee, Samuel},
  year = {2019},
  pages = {377--380},
  abstract = {This paper presents a novel algorithm UCB* (called UCB star) to make intelligent decisions, enhancing thewinning chances within games through the utilization of a well-known tree search algorithm, Monte Carlo Tree Search(MCTS). In},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\TXJZ87GA\\Lee - 2019 - EXPLORING TO LEARN WINNING STRATEGY.pdf;C\:\\Users\\aesou\\Zotero\\storage\\5Y3QQ6CF\\exploring-to-learn-winning-strategy.html},
  isbn = {978-989-8533-91-3}
}

@inproceedings{leece_opponent_2014,
  title = {Opponent State Modeling in {{RTS}} Games with Limited Information Using {{Markov}} Random Fields},
  booktitle = {2014 {{IEEE Conference}} on {{Computational Intelligence}} and {{Games}}},
  author = {Leece, Michael and Jhala, Arnav},
  year = {2014},
  month = aug,
  pages = {1--7},
  issn = {2325-4289},
  doi = {10.1109/CIG.2014.6932877},
  abstract = {One of the critical problems in adversarial and imperfect information domains is modeling an opponent's state from the information available to the acting agent. In the domain of real time strategy games, this information consists of the portion of the map and enemy units visible to the agent at any given point in the match. From this, we wish to infer the true values of the opponent's state, to inform both current actions and planning ahead. We present a graphical model for opponent modeling in StarCraft: Brood War that uses observed quantities to infer distributions for unseen features. We train and test this model using replays of professional play, and show that our results improve upon prior work. In addition, we present a new metric for measuring aggregate performance of a model within this domain. Finally, we consider possible use cases and extensions for this model.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\LQ4SGCUR\\Leece and Jhala - 2014 - Opponent state modeling in RTS games with limited .pdf;C\:\\Users\\aesou\\Zotero\\storage\\SHBCZURM\\6932877.html},
  keywords = {acting agent,adversarial information domains,aggregate performance,Buildings,Computational modeling,computer games,computer graphics,Economics,enemy units,Games,graphical model,imperfect information domains,Lead,Logic gates,map,Markov processes,Markov random fields,multi-agent systems,opponent state modeling,opponent state true values,professional play,random processes,real time strategy games,RTS games,StarCraft Brood War,Training}
}

@inproceedings{leece_sequential_2014,
  title = {Sequential {{Pattern Mining}} in {{StarCraft}}: {{Brood War}} for {{Short}} and {{Long}}-{{Term Goals}}},
  booktitle = {Artificial {{Intelligence}} in {{Adversarial Real}}-{{Time Games}}: {{Papers}} from the {{AIIDE Workshop}}},
  author = {Leece, Michael and Jhala, Arnav},
  year = {2014},
  pages = {6},
  abstract = {A wide variety of strategies have been used to create agents in the growing field of real-time strategy AI. However, a frequent problem is the necessity of handcrafting competencies, which becomes prohibitively difficult in a large space with many corner cases. A preferable approach would be to learn these competencies from the wealth of expert play available. We present a system that uses the Generalized Sequential Pattern (GSP) algorithm from data mining to find common patterns in StarCraft:Brood War replays at both the microand macro-level, and verify that these correspond to human understandings of expert play. In the future, we hope to use these patterns to learn tasks and goals in an unsupervised manner for an HTN planner.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\TNASAQZJ\\Leece and Jhala - 2014 - Sequential Pattern Mining in StarCraft Brood War .pdf},
  language = {en}
}

@inproceedings{lelis_stratified_2017,
  title = {Stratified {{Strategy Selection}} for {{Unit Control}} in {{Real}}-{{Time Strategy Games}}},
  booktitle = {Proceedings of the {{Twenty}}-{{Sixth International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Lelis, Levi H. S.},
  year = {2017},
  month = aug,
  pages = {3735--3741},
  publisher = {{International Joint Conferences on Artificial Intelligence Organization}},
  address = {{Melbourne, Australia}},
  doi = {10.24963/ijcai.2017/522},
  abstract = {In this paper we introduce Stratified Strategy Selection (SSS), a novel search algorithm for micromanaging units in real-time strategy (RTS) games. SSS uses a type system to partition the player's units into types and assumes that units of the same type must follow the same strategy. SSS searches in the state space induced by the type system to select, from a pool of options, a strategy for each unit. Empirical results on a simulator of an RTS game shows that SSS employing either fixed or adaptive type systems is able to substantially outperform stateof-the-art search-based algorithms in combat scenarios with up to 100 units.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\4IRH5AQP\\Lelis - 2017 - Stratified Strategy Selection for Unit Control in .pdf},
  isbn = {978-0-9992411-0-3},
  language = {en}
}

@inproceedings{lemaitre_towards_2015,
  title = {Towards a {{Resource}}-Based {{Model}} of {{Strategy}} to {{Help Designing Opponent AI}} in {{RTS Games}}:},
  shorttitle = {Towards a {{Resource}}-Based {{Model}} of {{Strategy}} to {{Help Designing Opponent AI}} in {{RTS Games}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Agents}} and {{Artificial Intelligence}}},
  author = {Lemaitre, Juliette and Lourdeaux, Domitile and Chopinaud, Caroline},
  year = {2015},
  pages = {210--215},
  publisher = {{SCITEPRESS - Science and and Technology Publications}},
  address = {{Lisbon, Portugal}},
  doi = {10.5220/0005254402100215},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\U5WDJ8KZ\\Lemaitre et al. - 2015 - Towards a Resource-based Model of Strategy to Help.pdf},
  isbn = {978-989-758-073-4 978-989-758-074-1},
  language = {en}
}

@inproceedings{leme_solving_2015,
  title = {Solving {{Sokoban Optimally}} with {{Domain}}-{{Dependent Move Pruning}}},
  booktitle = {2015 {{Brazilian Conference}} on {{Intelligent Systems}} ({{BRACIS}})},
  author = {Leme, Renato R. and Pereira, Andr{\'e} G. and Ritt, Marcus and Buriol, Luciana S.},
  year = {2015},
  month = nov,
  pages = {264--269},
  issn = {null},
  doi = {10.1109/BRACIS.2015.47},
  abstract = {Move pruning increases the efficiency of heuristic search techniques by not expanding parts of the state space. In this article, we propose an admissible domain-dependent move pruning (DDMP) technique for Sokoban. When exploring a node DDMP analyzes and selects a subset of successor nodes required to be generated to preserve all optimal solutions. DDMP has low space and time overhead. It reduces the number of successor nodes that need to be generated and thus the branching factor. Reducing the number of successor nodes is especially important for Sokoban due to the highly costly heuristic functions and deadlock detection techniques. In addition, the subset of selected successor nodes is, in general, the "worst successor nodes available" which increases the chance of deadlocks early detecting. We define DDMP formally and prove its admissibility. When applied to the standard set of instances DDMP reduces the branching factor, detects more deadlocks, and decreases the effort to solve instances in the number of explored nodes and total time. DDMP has a positive synergy with recent deadlock detection techniques. Combined they increase the number of optimally solved instances compared to previous methods.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Leme et al. - 2015 - Solving Sokoban Optimally with Domain-Dependent Mo.pdf;C\:\\Users\\aesou\\Zotero\\storage\\VR5EASJ2\\7424030.html},
  keywords = {A*,branching factor,computational complexity,Computers,Databases,DDMP technique,deadlock detection technique,Detectors,domain-dependent move pruning technique,graph theory,Heuristic algorithms,heuristic function,Heuristic search,move pruning,Move pruning,Pattern databases,Search problems,Single-agent search,Sokoban,Standards,state-space methods,System recovery}
}

@article{li_deep_2017,
  title = {Deep {{Reinforcement Learning}}: {{An Overview}}},
  shorttitle = {Deep {{Reinforcement Learning}}},
  author = {Li, Yuxi},
  year = {2017},
  month = jan,
  abstract = {We give an overview of recent exciting achievements of deep reinforcement learning (RL). We discuss six core elements, six important mechanisms, and twelve applications. We start with background of machine learning, deep learning and reinforcement learning. Next we discuss core RL elements, including value function, in particular, Deep Q-Network (DQN), policy, reward, model and planning, exploration, and knowledge. After that, we discuss important mechanisms for RL, including attention and memory, unsupervised learning, transfer learning, multiagent RL, hierarchical RL, and learning to learn. Then we discuss various applications of RL, including games, in particular, AlphaGo, robotics, natural language processing, including dialogue systems, machine translation, and text generation, computer vision, business management, finance, healthcare, education, Industry 4.0, smart grid, intelligent transportation systems, and computer systems. We mention topics not reviewed yet, and list a collection of RL resources. After presenting a brief summary, we close with discussions.},
  archivePrefix = {arXiv},
  eprint = {1701.07274},
  eprinttype = {arxiv},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\TA3E5Q9G\\Li - 2017 - Deep Reinforcement Learning An Overview.pdf},
  journal = {arXiv:1701.07274 [cs]},
  keywords = {Computer Science - Machine Learning},
  language = {en},
  primaryClass = {cs}
}

@article{li_deep_2018,
  title = {Deep {{Reinforcement Learning}}},
  author = {Li, Yuxi},
  year = {2018},
  month = oct,
  abstract = {We discuss deep reinforcement learning in an overview style. We draw a big picture, filled with details. We discuss six core elements, six important mechanisms, and twelve applications, focusing on contemporary work, and in historical contexts. We start with background of artificial intelligence, machine learning, deep learning, and reinforcement learning (RL), with resources. Next we discuss RL core elements, including value function, policy, reward, model, exploration vs. exploitation, and representation. Then we discuss important mechanisms for RL, including attention and memory, unsupervised learning, hierarchical RL, multi-agent RL, relational RL, and learning to learn. After that, we discuss RL applications, including games, robotics, natural language processing (NLP), computer vision, finance, business management, healthcare, education, energy, transportation, computer systems, and, science, engineering, and art. Finally we summarize briefly, discuss challenges and opportunities, and close with an epilogue.},
  archivePrefix = {arXiv},
  eprint = {1810.06339},
  eprinttype = {arxiv},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Li - 2018 - Deep Reinforcement Learning.pdf},
  journal = {arXiv:1810.06339 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@inproceedings{liebana_monte-carlo_2012,
  title = {Monte-{{Carlo Tree Search}} for the {{Physical Travelling Salesman Problem}}},
  booktitle = {{{EvoApplications}}},
  author = {Liebana, Diego Perez and Rohlfshagen, Philipp and Lucas, Simon M.},
  year = {2012},
  doi = {10.1007/978-3-642-29178-4_26},
  abstract = {The significant success of MCTS in recent years, particularly in the game Go, has led to the application of MCTS to numerous other domains. In an ongoing effort to better understand the performance of MCTS in open-ended real-time video games, we apply MCTS to the Physical Travelling Salesman Problem (PTSP). We discuss different approaches to tailor MCTS to this particular problem domain and subsequently identify and attempt to overcome some of the apparent shortcomings. Results show that suitable heuristics can boost the performance of MCTS significantly in this domain. However, visualisations of the search indicate that MCTS is currently seeking solutions in a rather greedy manner, and coercing it to balance short term and long term constraints for the PTSP remains an open problem.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\9JJ47X38\\Liebana et al. - 2012 - Monte-Carlo Tree Search for the Physical Travellin.pdf},
  keywords = {Const (computer programming),Greedy algorithm,Monte Carlo tree search,Nonlinear gameplay,Problem domain,Travelling salesman problem}
}

@article{liebana_solving_2014,
  title = {Solving the {{Physical Traveling Salesman Problem}}: {{Tree Search}} and {{Macro Actions}}},
  shorttitle = {Solving the {{Physical Traveling Salesman Problem}}},
  author = {Liebana, Diego Perez and Powley, Edward Jack and Whitehouse, Daniel and Rohlfshagen, Philipp and Samothrakis, Spyridon and Cowling, Peter I. and Lucas, Simon M.},
  year = {2014},
  volume = {6},
  pages = {31--45},
  doi = {10.1109/TCIAIG.2013.2263884},
  abstract = {This paper presents a number of approaches for solving a real-time game consisting of a ship that must visit a number of waypoints scattered around a 2-D maze full of obstacles. The game, the Physical Traveling Salesman Problem (PTSP), which featured in two IEEE conference competitions during 2012, provides a good balance between long-term planning (finding the optimal sequence of waypoints to visit), and short-term planning (driving the ship in the maze). This paper focuses on the algorithm that won both PTSP competitions: it takes advantage of the physics of the game to calculate the optimal order of waypoints, and it employs Monte Carlo tree search (MCTS) to drive the ship. The algorithm uses repetitions of actions (macro actions) to reduce the search space for navigation. Variations of this algorithm are presented and analyzed, in order to understand the strength of each one of its constituents and to comprehend what makes such an approach the best controller found so far for the PTSP.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\Z6P5NHB5\\Liebana et al. - 2014 - Solving the Physical Traveling Salesman Problem T.pdf},
  journal = {IEEE Transactions on Computational Intelligence and AI in Games},
  keywords = {Algorithm,Monte Carlo method,Monte Carlo tree search,Real-time transcription,Travelling salesman problem}
}

@inproceedings{lim_evolving_2010-1,
  title = {Evolving {{Behaviour Trees}} for the {{Commercial Game DEFCON}}},
  booktitle = {Applications of {{Evolutionary Computation}}},
  author = {Lim, Chong-U and Baumgarten, Robin and Colton, Simon},
  editor = {Di Chio, Cecilia and Cagnoni, Stefano and Cotta, Carlos and Ebner, Marc and Ek{\'a}rt, Anik{\'o} and {Esparcia-Alcazar}, Anna I. and Goh, Chi-Keong and Merelo, Juan J. and Neri, Ferrante and Preu{\ss}, Mike and Togelius, Julian and Yannakakis, Georgios N.},
  year = {2010},
  pages = {100--110},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {Behaviour trees provide the possibility of improving on existing Artificial Intelligence techniques in games by being simple to implement, scalable, able to handle the complexity of games, and modular to improve reusability. This ultimately improves the development process for designing automated game players. We cover here the use of behaviour trees to design and develop an AI-controlled player for the commercial real-time strategy game DEFCON. In particular, we evolved behaviour trees to develop a competitive player which was able to outperform the game's original AI-bot more than 50\% of the time. We aim to highlight the potential for evolving behaviour trees as a practical approach to developing AI-bots in games.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\NMRLWRKJ\\Lim et al. - 2010 - Evolving Behaviour Trees for the Commercial Game D.pdf},
  isbn = {978-3-642-12239-2},
  keywords = {Behaviour Tree,Finite State Machine,Game Developer,Genetic Programming,Human Player},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{lim_properties_2006,
  title = {Properties of Forward Pruning in Game-Tree Search},
  booktitle = {Proceedings of the 21st National Conference on {{Artificial}} Intelligence - {{Volume}} 2},
  author = {Lim, Yew Jin and Lee, Wee Sun},
  year = {2006},
  month = jul,
  pages = {1020--1025},
  publisher = {{AAAI Press}},
  address = {{Boston, Massachusetts}},
  abstract = {Forward pruning, or selectively searching a subset of moves, is now commonly used in game-playing programs to reduce the number of nodes searched with manageable risk. Forward pruning techniques should consider how pruning errors in a game-tree search propagate to the root to minimize the risk of making errors. In this paper, we explore forward pruning using theoretical analyses and Monte Carlo simulations and report on two findings. Firstly, we find that pruning errors propagate differently depending on the player to move, and show that pruning errors on the opponent's moves are potentially more serious than pruning errors on the player's own moves. This suggests that pruning on the player's own move can be performed more aggressively compared to pruning on the opponent's move. Secondly, we examine the ability of the minimax search to filter away pruning errors and give bounds on the rate of error propagation to the root. We find that if the rate of pruning error is kept constant, the growth of errors with the depth of the tree dominates the filtering effect, therefore suggesting that pruning should be done more aggressively near the root and less aggressively near the leaves.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Lim and Lee - 2006 - Properties of forward pruning in game-tree search.pdf},
  isbn = {978-1-57735-281-5},
  series = {{{AAAI}}'06}
}

@article{lin_stardata_2017,
  title = {{{STARDATA}}: {{A StarCraft AI Research Dataset}}},
  shorttitle = {{{STARDATA}}},
  author = {Lin, Zeming and Gehring, Jonas and Khalidov, Vasil and Synnaeve, Gabriel},
  year = {2017},
  month = aug,
  abstract = {We release a dataset of 65646 StarCraft replays that contains 1535 million frames and 496 million player actions. We provide full game state data along with the original replays that can be viewed in StarCraft. The game state data was recorded every 3 frames which ensures suitability for a wide variety of machine learning tasks such as strategy classification, inverse reinforcement learning, imitation learning, forward modeling, partial information extraction, and others. We use TorchCraft to extract and store the data, which standardizes the data format for both reading from replays and reading directly from the game. Furthermore, the data can be used on different operating systems and platforms. The dataset contains valid, non-corrupted replays only and its quality and diversity was ensured by a number of heuristics. We illustrate the diversity of the data with various statistics and provide examples of tasks that benefit from the dataset. We make the dataset available at https://github.com/TorchCraft/StarData . En Taro Adun!},
  archivePrefix = {arXiv},
  eprint = {1708.02139},
  eprinttype = {arxiv},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\Z58L9QAB\\Lin et al. - 2017 - STARDATA A StarCraft AI Research Dataset.pdf},
  journal = {arXiv:1708.02139 [cs]},
  keywords = {Computer Science - Artificial Intelligence},
  language = {en},
  primaryClass = {cs}
}

@article{liu_computer_2015,
  title = {Computer {{Science}} and {{Engineering}}},
  author = {Liu, Siming},
  year = {2015},
  pages = {106},
  abstract = {Real-Time Strategy games have become a new frontier of artificial intelligence research. Advances in real-time strategy game AI, like with chess and checkers before, will significantly advance the state of the art in AI research. This thesis aims to investigate using heuristic search algorithms to generate effective micro behaviors in combat scenarios for real-time strategy games. Macro and micro management are two key aspects of real-time strategy games. While good macro helps a player collect more resources and build more units, good micro helps a player win skirmishes against equal numbers of opponent units or win even when outnumbered. In this research, we use influence maps and potential fields as a basis representation to evolve micro behaviors. We first compare genetic algorithms against two types of hill climbers for generating competitive unit micro management. Second, we investigated the use of case-injected genetic algorithms to quickly and reliably generate high quality micro behaviors. Then we compactly encoded micro behaviors including influence maps, potential fields, and reactive control into fourteen parameters and used genetic algorithms to search for a complete micro bot, ECSLBot. We compare the performance of our ECSLBot with two state of the art bots, UAlbertaBot and Nova, on several skirmish scenarios in a popular real-time strategy game StarCraft. The results show that the ECSLBot tuned by genetic algorithms outperforms UAlbertaBot and Nova in kiting efficiency, target selection, and fleeing. In addition, the same approach works to create competitive micro behaviors in another game SeaCraft. Using parallelized genetic algorithms to evolve parameters in SeaCraft we are able to speed up the evolutionary process from twenty one hours to nine minutes. We believe this work provides evidence that genetic algorithms and our representation may be a viable approach to creating effective micro behaviors for winning skirmishes in real-time strategy games.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\QBNP4DHU\\Liu - 2015 - Computer Science and Engineering.pdf},
  language = {en}
}

@inproceedings{liu_evolving_2014,
  title = {Evolving Effective Micro Behaviors in {{RTS}} Game},
  booktitle = {2014 {{IEEE Conference}} on {{Computational Intelligence}} and {{Games}}},
  author = {Liu, Siming and Louis, Sushil J. and Ballinger, Christopher},
  year = {2014},
  month = aug,
  pages = {1--8},
  publisher = {{IEEE}},
  address = {{Dortmund, Germany}},
  doi = {10.1109/CIG.2014.6932904},
  abstract = {We investigate using genetic algorithms to generate high quality micro management in combat scenarios for realtime strategy games. Macro and micro management are two key aspects of real-time strategy games. While good macro helps a player collect more resources and build more units, good micro helps a player win skirmishes against equal numbers and types of opponent units or win even when outnumbered. In this paper, we use influence maps and potential fields to generate micro management positioning and movement tactics. Micro behaviors are compactly encoded into fourteen parameters and we use genetic algorithms to search for effective micro management tactics for the given units. We tested the performance of our ECSLBot (the evolved player), obtained in this way against the default StarCraft AI, and two other state of the art bots, UAlbertaBot and Nova on several skirmish scenarios. The results show that the ECSLBot tuned by genetic algorithms outperforms the UAlbertaBot and Nova in kiting efficiency, target selection, and knowing when to flee to survive. We believe our approach is easy to extend to other types of units and can be easily adopted by other AI bots.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\PF3DVIS8\\Liu et al. - 2014 - Evolving effective micro behaviors in RTS game.pdf},
  isbn = {978-1-4799-3547-5},
  language = {en}
}

@phdthesis{liu_evolving_2015,
  title = {Evolving {{Effective Micro Behaviors}} for {{Real}}-{{Time Strategy Games}}},
  author = {Liu, Siming},
  year = {2015},
  abstract = {Real-Time Strategy games have become a new frontier of artificial intelligence research. Advances in real-time strategy game AI, like with chess and checkers before, will significantly advance the state of the art in AI research. This thesis aims to investigate using heuristic search algorithms to generate effective micro behaviors in combat scenarios for real-time strategy games. Macro and micro management are two key aspects of real-time strategy games. While good macro helps a player collect more resources and build more units, good micro helps a player win skirmishes against equal numbers of opponent units or win even when outnumbered. In this research, we use influence maps and potential fields as a basis representation to evolve micro behaviors. We first compare genetic algorithms against two types of hill climbers for generating competitive unit micro management. Second, we investigated the use of case-injected genetic algorithms to quickly and reliably generate high quality micro behaviors. Then we compactly encoded micro behaviors including influence maps, potential fields, and reactive control into fourteen parameters and used genetic algorithms to search for a complete micro bot, ECSLBot. We compare the performance of our ECSLBot with two state of the art bots, UAlbertaBot and Nova, on several skirmish scenarios in a popular real-time strategy game StarCraft. The results show that the ECSLBot tuned by genetic algorithms outperforms UAlbertaBot and Nova in kiting efficiency, target selection, and fleeing. In addition, the same approach works to create competitive micro behaviors in another game SeaCraft. Using parallelized genetic algorithms to evolve parameters in SeaCraft we are able to speed up the evolutionary process from twenty one hours to nine minutes. We believe this work provides evidence that genetic algorithms and our representation may be a viable approach to creating effective micro behaviors for winning skirmishes in real-time strategy games.},
  copyright = {In Copyright(All Rights Reserved)},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\VL4DGT5S\\Liu - 2015 - Evolving Effective Micro Behaviors for Real-Time S.pdf},
  language = {en},
  school = {University of Nevada, Reno},
  type = {{{PhD}}}
}

@incollection{liu_fast_2016,
  title = {Fast {{Seed}}-{{Learning Algorithms}} for {{Games}}},
  booktitle = {Computers and {{Games}}},
  author = {Liu, Jialin and Teytaud, Olivier and Cazenave, Tristan},
  year = {2016},
  volume = {10068},
  pages = {58--70},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-50935-8_6},
  abstract = {Recently, a methodology has been proposed for boosting the computational intelligence of randomized game-playing programs. We propose faster variants of these algorithms, namely rectangular algorithms (fully parallel) and bandit algorithms (faster in a sequential setup). We check the performance on several board games and card games. In addition, in the case of Go, we check the methodology when the opponent is completely distinct to the one used in the training.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2016\\10 Fast Seed-Learning Algorithms for Games.pdf},
  isbn = {978-3-319-50934-1 978-3-319-50935-8},
  language = {en}
}

@incollection{liu_frequency_2008,
  title = {Frequency {{Distribution}} of {{Contextual Patterns}} in the {{Game}} of {{Go}}},
  booktitle = {Computers and {{Games}}},
  author = {Liu, Zhiqing and Dou, Qing and Lu, Benjie},
  year = {2008},
  volume = {5131},
  pages = {125--134},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-87608-3_12},
  abstract = {In this paper, we present two statistical experiments on the frequency distribution of patterns in the game of Go. In these experiments, we extract contextual patterns of Go as spatial combinations of moves. An analysis of a collection of 9447 professional game records of Go shows that the frequency distribution of contextual patterns in professional games displays a Mandelbrot fit to Zipf's law. Additionally, we show that the Zipfian frequency distribution of Go patterns in professional games is deliberate by rejecting the null hypothesis that the frequency distribution of patterns in random games exhibits a Zipfian frequency distribution.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2008\\17 Frequency Distribution of Contextual Patterns in the Game of Go.pdf},
  isbn = {978-3-540-87607-6 978-3-540-87608-3},
  language = {en}
}

@inproceedings{liu_hierarchical_2019,
  title = {A {{Hierarchical Model}} for {{StarCraft II Mini}}-{{Game}}},
  booktitle = {2019 18th {{IEEE International Conference On Machine Learning And Applications}} ({{ICMLA}})},
  author = {Liu, Tianlin and Wu, Xihong and Luo, Dingsheng},
  year = {2019},
  month = dec,
  pages = {222--227},
  issn = {null},
  doi = {10.1109/ICMLA.2019.00042},
  abstract = {StarCraft II is one of the most challenging real-time strategy games, due to huge action space, large observation space, imperfect information, etc. Therefore, it is hard to learn the full game of StarCraft II. To reduce the learning complexity, DeepMind and Blizzard released several mini-games, in which the BuildMarines mini-game is most challenging, due to long time horizons, partially-observed state, high-dimensional, continuous action space and observation space. In this paper, we propose a hierarchical modeling method to solve those challenges in BuildMarines mini-game. Our approach consists of two levels, combining learning-based (high-level) and rule-based (low-level) method. The learning-based method leverages DQN reinforcement learning algorithm, while the rule-based method leverages script to realize. Experimental results show that the proposed approach is effective for an agent to learn the long planning horizon game, BuildMarines.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Liu et al. - 2019 - A Hierarchical Model for StarCraft II Mini-Game.pdf;C\:\\Users\\aesou\\Zotero\\storage\\ICZFVAQ9\\8999313.html},
  keywords = {Buildings,Games,Learning (artificial intelligence),Machine learning,Minerals,Planning,StarCraft II; game; real time strategy game; hierarchical modeling; DQN; reinforcement learning,Task analysis}
}

@inproceedings{liu_using_2013,
  title = {Using {{CIGAR}} for Finding Effective Group Behaviors in {{RTS}} Game},
  booktitle = {2013 {{IEEE Conference}} on {{Computational Inteligence}} in {{Games}} ({{CIG}})},
  author = {Liu, Siming and Louis, Sushil J. and Nicolescu, Monica},
  year = {2013},
  month = aug,
  pages = {1--8},
  publisher = {{IEEE}},
  address = {{Niagara Falls, ON, Canada}},
  doi = {10.1109/CIG.2013.6633652},
  abstract = {We investigate using case-injected genetic algorithms to quickly generate high quality unit micro-management in real-time strategy game skirmishes. Good group positioning and movement, which are part of unit micro-management, can help win skirmishes against equal numbers and types of opponent units or win even when outnumbered. In this paper, we use influence maps to generate group positioning and potential fields to guide unit movement and compare the performance of caseinjected genetic algorithms, genetic algorithms, and two types of hill-climbing search in finding good unit behaviors for defeating the default Starcraft Brood Wars AI. Early results showed that our hill-climbers were quick but unreliable while the genetic algorithm was slow but reliably found quality solutions a hundred percent of the time. Case-injected genetic algorithms, on the other hand were designed to learn from experience to increase problem solving performance on similar problems. Preliminary results with case-injected genetic algorithms indicate that they find high quality results as reliable as genetic algorithms but up to twice as quickly on related maps.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Liu et al. - 2013 - Using CIGAR for finding effective group behaviors .pdf},
  isbn = {978-1-4673-5311-3 978-1-4673-5308-3},
  language = {en}
}

@inproceedings{lorentz_amazons_2008,
  title = {Amazons {{Discover Monte}}-{{Carlo}}},
  booktitle = {Computers and {{Games}}},
  author = {Lorentz, Richard J.},
  editor = {{van den Herik}, H. Jaap and Xu, Xinhe and Ma, Zongmin and Winands, Mark H. M.},
  year = {2008},
  pages = {13--24},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-87608-3_2},
  abstract = {Monte-Carlo algorithms and their UCT-like successors have recently shown remarkable promise for Go-playing programs. We apply some of these same algorithms to an Amazons-playing program. Our experiments suggest that a pure MC/UCT type program for playing Amazons has little promise, but by using strong evaluation functions we are able to create a hybrid MC/UCT program that is superior to both the basic MC/UCT program and the conventional minimax-based programs. The MC/UCT program is able to beat Invader, a strong minimax program, over 80\% of the time at tournament time controls.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2008\\7 Amazons Discover Monte−Carlo.pdf},
  isbn = {978-3-540-87608-3},
  keywords = {Child Node,Evaluation Function,Legal Move,Random Simulation,Similar Negative Effect},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{lorentz_improving_2011,
  title = {Improving {{Monte}}\textendash{{Carlo Tree Search}} in {{Havannah}}},
  booktitle = {Computers and {{Games}}},
  author = {Lorentz, Richard J.},
  editor = {{van den Herik}, H. Jaap and Iida, Hiroyuki and Plaat, Aske},
  year = {2011},
  pages = {105--115},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-17928-0_10},
  abstract = {Havannah is a game played on an hexagonal board of hexagons where the base of the board typically ranges from four to ten hexagons. The game is known to be difficult to program. We study an MCTS-based approach to programming Havannah using our program named Wanderer. We experiment with five techniques of the basic MCTS algorithms and demonstrate that at normal time controls of approximately 30 seconds per move Wanderer can make quite strong moves with bases of size four or five, and play a reasonable game with bases of size six or seven. At longer time controls (ten minutes per move) Wanderer (1) appears to play nearly perfectly with base four, (2) is difficult for most humans to beat at base five, and (3) gives a good game at bases six and seven. Future research focuses on larger board sizes.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2010\\15 Improving Monte−Carlo Tree Search in Havannah.pdf},
  isbn = {978-3-642-17928-0},
  keywords = {Base Size,Board Size,Good Move,Strong Player,Winning Percentage},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@incollection{lorentz_programming_2014,
  title = {Programming {{Breakthrough}}},
  booktitle = {Computers and {{Games}}},
  author = {Lorentz, Richard and Horey, Therese},
  year = {2014},
  volume = {8427},
  pages = {49--59},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-09165-5_5},
  abstract = {Breakthrough is an abstract strategy board game that requires considerable strategy in the early stages of the game but can suddenly and unexpectedly turn tactical. Further, the strategic elements can be extremely subtle and the tactics can be quite deep, involving sequences of 20 or more moves. We are developing an MCTS-based program to play Breakthrough and demonstrate that this approach, with proper adjustments, produces a program that can beat most human players.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2013\\9 Programming Breakthrough.pdf},
  isbn = {978-3-319-09164-8 978-3-319-09165-5},
  language = {en}
}

@incollection{lorenz_solution_2014,
  title = {Solution {{Techniques}} for {{Quantified Linear Programs}} and the {{Links}} to {{Gaming}}},
  booktitle = {Computers and {{Games}}},
  author = {Lorenz, Ulf and Opfer, Thomas and Wolf, Jan},
  year = {2014},
  volume = {8427},
  pages = {110--124},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-09165-5_10},
  abstract = {Quantified linear programs (QLPs) are linear programs (LPs) with variables being either existentially or universally quantified. QLPs are two-person zero-sum games between an existential and a universal player on the one side, and convex multistage decision problems on the other side. Solutions of feasible QLPs are so called winning strategies for the existential player that specify how to react on moves \textendash{} well-thought fixations of universally quantified variables \textendash{} of the universal player to be sure to win the game. To find a certain best strategy among different winning strategies, we propose the extension of the QLP decision problem by an objective function. To solve the resulting QLP optimization problem, we exploit the problem's hybrid nature and combine linear programming techniques with solution techniques from game-tree search. As a result, we present an extension of the Nested Benders Decomposition algorithm by the {$\alpha\beta$}-algorithm and its heuristical move-ordering as used in game-tree search to solve minimax trees. The applicability of our method to both QLPs and models of PSPACE-complete games such as Connect6 is examined in an experimental evaluation.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2013\\14 Solution Techniques for Quantified Linear Programs and the Links to Gaming.pdf},
  isbn = {978-3-319-09164-8 978-3-319-09165-5},
  language = {en}
}

@article{louis_multi-objective_2018,
  title = {Multi-Objective Evolution for {{3D RTS Micro}}},
  author = {Louis, Sushil J. and Liu, Siming},
  year = {2018},
  month = mar,
  abstract = {We attack the problem of controlling teams of autonomous units during skirmishes in real-time strategy games. Earlier work had shown promise in evolving control algorithm parameters that lead to high performance team behaviors similar to those favored by good human players in real-time strategy games like Starcraft. This algorithm specifically encoded parameterized kiting and fleeing behaviors and the genetic algorithm evolved these parameter values. In this paper we investigate using influence maps and potential fields alone to compactly represent and control real-time team behavior for entities that can maneuver in three dimensions. A two-objective fitness function that maximizes damage done and minimizes damage taken guides our multi-objective evolutionary algorithm. Preliminary results indicate that evolving friend and enemy unit potential field parameters for distance, weapon characteristics, and entity health suffice to produce complex, high performing, three-dimensional, team tactics.},
  archivePrefix = {arXiv},
  eprint = {1803.02943},
  eprinttype = {arxiv},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\V2LMSF9M\\Louis and Liu - 2018 - Multi-objective evolution for 3D RTS Micro.pdf},
  journal = {arXiv:1803.02943 [cs]},
  keywords = {Computer Science - Neural and Evolutionary Computing},
  language = {en},
  primaryClass = {cs}
}

@article{lowe_putting_2020,
  title = {Putting {{An End}} to {{End}}-to-{{End}}: {{Gradient}}-{{Isolated Learning}} of {{Representations}}},
  shorttitle = {Putting {{An End}} to {{End}}-to-{{End}}},
  author = {L{\"o}we, Sindy and O'Connor, Peter and Veeling, Bastiaan S.},
  year = {2020},
  month = jan,
  abstract = {We propose a novel deep learning method for local self-supervised representation learning that does not require labels nor end-to-end backpropagation but exploits the natural order in data instead. Inspired by the observation that biological neural networks appear to learn without backpropagating a global error signal, we split a deep neural network into a stack of gradient-isolated modules. Each module is trained to maximally preserve the information of its inputs using the InfoNCE bound from Oord et al. [2018]. Despite this greedy training, we demonstrate that each module improves upon the output of its predecessor, and that the representations created by the top module yield highly competitive results on downstream classification tasks in the audio and visual domain. The proposal enables optimizing modules asynchronously, allowing large-scale distributed training of very deep neural networks on unlabelled datasets.},
  archivePrefix = {arXiv},
  eprint = {1905.11786},
  eprinttype = {arxiv},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Löwe et al. - 2020 - Putting An End to End-to-End Gradient-Isolated Le.pdf},
  journal = {arXiv:1905.11786 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@inproceedings{lu_awt_2008,
  title = {{{AWT}}: {{Aspiration}} with {{Timer Search Algorithm}} in {{Siguo}}},
  shorttitle = {{{AWT}}},
  booktitle = {Computers and {{Games}}},
  author = {Lu, Hui and Xia, ZhengYou},
  editor = {{van den Herik}, H. Jaap and Xu, Xinhe and Ma, Zongmin and Winands, Mark H. M.},
  year = {2008},
  pages = {264--274},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-87608-3_24},
  abstract = {Game playing is one of the classic problems of artificial intelligence. The Siguo game is an emerging field of research in the area of game-playing programs. It provides a new test bed for artificial intelligence with imperfect information. To improve search efficiency for Siguo with more branches and the uncertain payoff in the game tree, this paper presents a modified Alpha-Beta Aspiration Search algorithm, which is called Alpha-Beta Aspiration with Timer Algorithm (AWT). The AWT can quickly find a suboptimal payoff (acceptable value) from the game tree by adjusting a window with a timer. The timer is controlled by two parameters (M, N) that vary with the chess-board status of Siguo. Experiments show that AWT achieves the goals of the improvability of time efficiency, although it costs a little more memory and does not lead to the best payoff, but to an acceptable payoff.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2008\\29 AWT Aspiration with Timer Search Algorithm in Siguo.pdf},
  isbn = {978-3-540-87608-3},
  keywords = {Game Playing,Game Tree,Imperfect Information,Leaf Node,Search Process},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{lu_hmcts-op_2020,
  title = {{{HMCTS}}-{{OP}}: {{Hierarchical MCTS Based Online Planning}} in the {{Asymmetric Adversarial Environment}}},
  shorttitle = {{{HMCTS}}-{{OP}}},
  author = {Lu, Lina and Zhang, Wanpeng and Gu, Xueqiang and Ji, Xiang and Chen, Jing},
  year = {2020},
  month = may,
  volume = {12},
  pages = {719},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  doi = {10.3390/sym12050719},
  abstract = {The Monte Carlo Tree Search (MCTS) has demonstrated excellent performance in solving many planning problems. However, the state space and the branching factors are huge, and the planning horizon is long in many practical applications, especially in the adversarial environment. It is computationally expensive to cover a sufficient number of rewarded states that are far away from the root in the flat non-hierarchical MCTS. Therefore, the flat non-hierarchical MCTS is inefficient for dealing with planning problems with a long planning horizon, huge state space, and branching factors. In this work, we propose a novel hierarchical MCTS-based online planning method named the HMCTS-OP to tackle this issue. The HMCTS-OP integrates the MAXQ-based task hierarchies and the hierarchical MCTS algorithms into the online planning framework. Specifically, the MAXQ-based task hierarchies reduce the search space and guide the search process. Therefore, the computational complexity is significantly reduced. Moreover, the reduction in the computational complexity enables the MCTS to perform a deeper search to find better action in a limited time. We evaluate the performance of the HMCTS-OP in the domain of online planning in the asymmetric adversarial environment. The experiment results show that the HMCTS-OP outperforms other online planning methods in this domain.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Lu et al. - 2020 - HMCTS-OP Hierarchical MCTS Based Online Planning .pdf;C\:\\Users\\aesou\\Zotero\\storage\\GNCCMFTM\\Lu et al. - 2020 - HMCTS-OP Hierarchical MCTS Based Online Planning .pdf;C\:\\Users\\aesou\\Zotero\\storage\\YZEHSEMR\\719.html},
  journal = {Symmetry},
  keywords = {asymmetric adversarial environment,HMCTS,MAXQ,online planning},
  language = {en},
  number = {5}
}

@article{lucas_artificial_2012,
  title = {Artificial and {{Computational Intelligence}} in {{Games}} ({{Dagstuhl Seminar}} 12191)},
  author = {Lucas, Simon M. and Mateas, Michael and Preuss, Mike and Spronck, Pieter and Togelius, Julian},
  year = {2012},
  doi = {10.4230/dagrep.2.5.43},
  abstract = {This report documents the program and the outcomes of Dagstuhl Seminar 12191 ``Artificial and Computational Intelligence in Games''. The aim for the seminar was to bring together creative experts in an intensive meeting with the common goals of gaining a deeper understanding of various aspects of artificial and computational intelligence in games, to help identify the main challenges in game AI research and the most promising venues to deal with them. This was accomplished mainly by means of workgroups on 14 different topics (ranging from search, learning, and modeling to architectures, narratives, and evaluation), and plenary discussions on the results of the workgroups. This report presents the conclusions that each of the workgroups reached. We also added short descriptions of the few talks that were unrelated to any of the workgroups.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\3ZYTJQFJ\\Lucas et al. - 2012 - Artificial and Computational Intelligence in Games.pdf},
  journal = {Schloss Dagstuhl - Leibniz-Zentrum fuer Informatik GmbH, Wadern/Saarbruecken, Germany},
  language = {en}
}

@article{lucas_efcient_2019,
  title = {Efficient {{Evolutionary Methods}} for {{Game Agent Optimisation}}: {{Model}}-{{Based}} Is {{Best}}},
  author = {Lucas, Simon M and Liu, Jialin and Bravi, Ivan and Gaina, Raluca D and Woodward, John and Volz, Vanessa and {Perez-Liebana}, Diego},
  year = {2019},
  pages = {8},
  abstract = {This paper introduces a simple and fast variant of Planet Wars as a test-bed for statistical planning based Game AI agents, and for noisy hyper-parameter optimisation. Planet Wars is a real-time strategy game with simple rules but complex gameplay. The variant introduced in this paper is designed for speed to enable efficient experimentation, and also for a fixed action space to enable practical inter-operability with General Video Game AI agents. If we treat the game as a win-loss game (which is standard), then this leads to challenging noisy optimisation problems both in tuning agents to play the game, and in tuning game parameters.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\2P2W9IIF\\Lucas et al. - 2019 - Efﬁcient Evolutionary Methods for Game Agent Optim.pdf},
  journal = {arXiv:1901.00723 [cs.AI]},
  language = {en}
}

@article{lucas_game_2018,
  title = {Game {{AI Research}} with {{Fast Planet Wars Variants}}},
  author = {Lucas, Simon M.},
  year = {2018},
  month = jun,
  abstract = {This paper describes a new implementation of Planet Wars, designed from the outset for Game AI research. The skill-depth of the game makes it a challenge for game-playing agents, and the speed of more than 1 million game ticks per second enables rapid experimentation and prototyping. The parameterised nature of the game together with an interchangeable actuator model make it well suited to automated game tuning. The game is designed to be fun to play for humans, and is directly playable by General Video Game AI agents.},
  archivePrefix = {arXiv},
  eprint = {1806.08544},
  eprinttype = {arxiv},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\WFFEUVKE\\Lucas - 2018 - Game AI Research with Fast Planet Wars Variants.pdf},
  journal = {arXiv:1806.08544 [cs]},
  keywords = {Computer Science - Artificial Intelligence},
  language = {en},
  primaryClass = {cs}
}

@article{lucas_n-tuple_2018,
  title = {The {{N}}-{{Tuple Bandit Evolutionary Algorithm}} for {{Game Agent Optimisation}}},
  author = {Lucas, Simon M. and Liu, Jialin and {Perez-Liebana}, Diego},
  year = {2018},
  month = feb,
  abstract = {This paper describes the N-Tuple Bandit Evolutionary Algorithm (NTBEA), an optimisation algorithm developed for noisy and expensive discrete (combinatorial) optimisation problems. The algorithm is applied to two game-based hyperparameter optimisation problems. The N-Tuple system directly models the statistics, approximating the fitness and number of evaluations of each modelled combination of parameters. The model is simple, efficient and informative. Results show that the NTBEA significantly outperforms grid search and an estimation of distribution algorithm.},
  archivePrefix = {arXiv},
  eprint = {1802.05991},
  eprinttype = {arxiv},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\3HKZSRDB\\Lucas et al. - 2018 - The N-Tuple Bandit Evolutionary Algorithm for Game.pdf},
  journal = {arXiv:1802.05991 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Neural and Evolutionary Computing},
  language = {en},
  primaryClass = {cs}
}

@article{magerko_13th_2018,
  title = {The 13th {{AAAI Conference}} on {{Artificial Intelligence}} and {{Interactive Digital Entertainment}}},
  author = {Magerko, Brian},
  year = {2018},
  volume = {39},
  pages = {75-},
  issn = {0738-4602},
  abstract = {The 13th AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment (AIIDE 2017) was held at the Snowbird Ski and Summer Resort in Little Cottonwood Canyon in the Wasatch Range of the Rocky Mountains near Salt Lake City, Utah. Along with the main conference presentations, the meeting included two tutorials, three workshops, and invited keynote talks. This report summarizes the main conference. It also includes contributions from the organizers of the three workshops.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\I3WGQKYF\\Magerko - 2018 - The 13th AAAI Conference on Artificial Intelligenc.PDF;C\:\\Users\\aesou\\Zotero\\storage\\WPISYL88\\i.html},
  journal = {AI Magazine},
  keywords = {Artificial intelligence,Human-computer interaction},
  language = {English},
  number = {2}
}

@phdthesis{maly_learning_2017,
  title = {Learning to Play Real-Time Strategy Games from Demonstration Using Decentralized {{MAS}}},
  author = {Mal{\'y}, Jan},
  year = {2017},
  abstract = {Despite the amount of effort put in Artificial Intelligence research, bots for real-time strategy games present no threat for professional human players.
There are still many challenges to overcome by researchers to develop AI able to beat experts. In this work, we deal with three challenges: adaptive planning, domain knowledge integration and integration of AI techniques to unified architecture. We introduce the usage of Inverse Reinforcement Learning as a new approach for decision making based on the observation human gameplay. To be able to integrate Inverse Reinforcement Learning with other techniques needed for the complete bot, we build our AI on new unified architecture in the form of a highly decentralized Multi-agent system. After using a small
set of replays, our bot was able to learn strategy which beats built-in AI in some scenarios. The bot also shows
the ability to adapt its behavior to the situation. The approach presents a novel way of developing challenging
bots with little to none domain expert knowledge.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\TH4ICN35\\Malý - 2017 - Learning to play real-time strategy games from dem.pdf},
  school = {Czech Technical University in Prague},
  type = {Master}
}

@article{mandai_linucb_2016,
  title = {{{LinUCB}} Applied to {{Monte Carlo}} Tree Search},
  author = {Mandai, Yusaku and Kaneko, Tomoyuki},
  year = {2016},
  month = sep,
  volume = {644},
  pages = {114--126},
  issn = {0304-3975},
  doi = {10.1016/j.tcs.2016.06.035},
  abstract = {UCT is a standard method of Monte Carlo tree search (MCTS) algorithms, which have been applied to various domains and have achieved remarkable success. This study proposes a family of LinUCT algorithms that incorporate LinUCB into MCTS algorithms. LinUCB is a recently developed method that generalizes past episodes by ridge regression with feature vectors and rewards. LinUCB outperforms UCB1 in contextual multi-armed bandit problems. We introduce a straightforward application of LinUCB, LinUCTPLAIN by substituting UCB1 with LinUCB in UCT. We show that it does not work well owing to the minimax structure of game trees. To better handle such tree structures, we present LinUCTRAVE and LinUCTFP by further incorporating two existing techniques, rapid action value estimation (RAVE) and feature propagation, which recursively propagates the feature vector of a node to that of its parent. Experiments were conducted with a synthetic model, which is an extension of the standard incremental random tree model in which each node has a feature vector that represents the characteristics of the corresponding position, and Finnsson's shock step game which is used to empirically analyze the performance of UCT with respect to the distribution of suboptimal moves. The experiments results indicate that LinUCTRAVE and LinUCTFP outperform UCT, especially when the branching factor is relatively large.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Mandai and Kaneko - 2016 - LinUCB applied to Monte Carlo tree search.pdf;C\:\\Users\\aesou\\Zotero\\storage\\YR695BMY\\S0304397516302808.html},
  journal = {Theoretical Computer Science},
  keywords = {Contextual bandit,LinUCB,MCTS,Multi-armed bandit problem},
  language = {en},
  series = {Recent {{Advances}} in {{Computer Games}}}
}

@incollection{mandziuk_creating_2013,
  title = {Creating a {{Personality System}} for {{RTS Bots}}},
  booktitle = {Believable {{Bots}}},
  author = {Ma{\'n}dziuk, Jacek and Sza{\l}aj, Przemys{\l}aw},
  editor = {Hingston, Philip},
  year = {2013},
  pages = {231--264},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-32323-2_10},
  abstract = {Bots in Real Time Strategy games often play according to a predefined scripts, which usually makes their behavior repetitive and predictable. In this chapter, we discuss a notion of personality for a RTS bot and how it can be used to control bot's behavior. We introduce a personality system that allows us to easily create different personalities and we discuss how different components of the system can be identified and defined. The process of personality creation is based on a several traits, which describe a general bot's charateristic. It allows us to create a wide variety of consistent personalities with the desired level of randomness, and, at the same time, to precisely control bot's behavior by enforcing or preventing certain strategies and techniques.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\PCCZ7WB3\\Mańdziuk and Szałaj - 2013 - Creating a Personality System for RTS Bots.pdf},
  isbn = {978-3-642-32322-5 978-3-642-32323-2},
  language = {en}
}

@incollection{mandziuk_mcts/uct_2018,
  title = {{{MCTS}}/{{UCT}} in {{Solving Real}}-{{Life Problems}}},
  booktitle = {Advances in {{Data Analysis}} with {{Computational Intelligence Methods}}: {{Dedicated}} to {{Professor Jacek \.Zurada}}},
  author = {Ma{\'n}dziuk, Jacek},
  editor = {Gaw{\k{e}}da, Adam E and Kacprzyk, Janusz and Rutkowski, Leszek and Yen, Gary G.},
  year = {2018},
  pages = {277--292},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-67946-4_11},
  abstract = {Monte Carlo Tree Search (MCTS) supported by the Upper Confidence Bounds Applied to Trees (UCT) method, i.e. MCTS/UCT, since its onset in 2006, has been one of the state-of-the-art techniques in game-playing domain. In particular, the recent breakthroughing success of this method (combined with deep neural networks trained with the reinforcement learning algorithm) in the game of Go, made its leading position even stronger than before. In this paper we summarize our studies in application of MCTS/UCT to domains other than games, with particular emphasis on hard real-life problems which possess a large degree of uncertainty due to existence of certain stochastic factors in their definition. The two example problems of this nature considered in this work are Capacitated Vehicle Routing Problem with Traffic Jams and Risk-Aware Project Scheduling Problem. Our results show that MCTS/UCT is a viable method in these two domains, efficiently dealing with uncertainty by means of on-line adaptation of the core MCTS simulations to the current situation (actual realization of the stochastic components).},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Mańdziuk - 2018 - MCTSUCT in Solving Real-Life Problems.pdf},
  isbn = {978-3-319-67946-4},
  keywords = {Dynamic Vehicle Routing Problem,Monte Carlo Tree Search,Project scheduling,Traffic jams,Upper Confidence Bounds Applied to Trees},
  language = {en},
  series = {Studies in {{Computational Intelligence}}}
}

@article{mannion_special_2018,
  title = {Special Issue on Adaptive and Learning Agents 2017},
  author = {Mannion, Patrick and Harutyunyan, Anna and Subramanian, Kaushik},
  year = {2018},
  month = oct,
  volume = {33},
  issn = {0269-8889, 1469-8005},
  doi = {10.1017/S026988891800022X},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\XMDB876W\\Mannion et al. - 2018 - Special issue on adaptive and learning agents 2017.pdf},
  journal = {The Knowledge Engineering Review},
  language = {en}
}

@article{marcus_deep_2018,
  title = {Deep {{Learning}}: {{A Critical Appraisal}}},
  shorttitle = {Deep {{Learning}}},
  author = {Marcus, Gary},
  year = {2018},
  month = jan,
  abstract = {Although deep learning has historical roots going back decades, neither the term "deep learning" nor the approach was popular just over five years ago, when the field was reignited by papers such as Krizhevsky, Sutskever and Hinton's now classic (2012) deep network model of Imagenet. What has the field discovered in the five subsequent years? Against a background of considerable progress in areas such as speech recognition, image recognition, and game playing, and considerable enthusiasm in the popular press, I present ten concerns for deep learning, and suggest that deep learning must be supplemented by other techniques if we are to reach artificial general intelligence.},
  archivePrefix = {arXiv},
  eprint = {1801.00631},
  eprinttype = {arxiv},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Marcus - 2018 - Deep Learning A Critical Appraisal.pdf;C\:\\Users\\aesou\\Zotero\\storage\\TJCIS5LY\\Marcus - 2018 - Deep Learning A Critical Appraisal.pdf;C\:\\Users\\aesou\\Zotero\\storage\\SCWDZF6N\\1801.html},
  journal = {arXiv:1801.00631 [cs, stat]},
  keywords = {97R40,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,I.2.0,I.2.6,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{marcus_innateness_2018,
  title = {Innateness, {{AlphaZero}}, and {{Artificial Intelligence}}},
  author = {Marcus, Gary},
  year = {2018},
  month = jan,
  abstract = {The concept of innateness is rarely discussed in the context of artificial intelligence. When it is discussed, or hinted at, it is often the context of trying to reduce the amount of innate machinery in a given system. In this paper, I consider as a test case a recent series of papers by Silver et al (Silver et al., 2017a) on AlphaGo and its successors that have been presented as an argument that a "even in the most challenging of domains: it is possible to train to superhuman level, without human examples or guidance", "starting tabula rasa." I argue that these claims are overstated, for multiple reasons. I close by arguing that artificial intelligence needs greater attention to innateness, and I point to some proposals about what that innateness might look like.},
  archivePrefix = {arXiv},
  eprint = {1801.05667},
  eprinttype = {arxiv},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\T8YF2M7K\\Marcus - 2018 - Innateness, AlphaZero, and Artificial Intelligence.pdf;C\:\\Users\\aesou\\Zotero\\storage\\XIBG2RPQ\\1801.html},
  journal = {arXiv:1801.05667 [cs]},
  keywords = {97R40,Computer Science - Artificial Intelligence,I.2.0,I.2.6},
  primaryClass = {cs}
}

@article{marcus_next_2020,
  title = {The {{Next Decade}} in {{AI}}: {{Four Steps Towards Robust Artificial Intelligence}}},
  shorttitle = {The {{Next Decade}} in {{AI}}},
  author = {Marcus, Gary},
  year = {2020},
  month = feb,
  abstract = {Recent research in artificial intelligence and machine learning has largely emphasized general-purpose learning and ever-larger training sets and more and more compute. In contrast, I propose a hybrid, knowledge-driven, reasoning-based approach, centered around cognitive models, that could provide the substrate for a richer, more robust AI than is currently possible.},
  archivePrefix = {arXiv},
  eprint = {2002.06177},
  eprinttype = {arxiv},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\QZJXGICQ\\Marcus - 2020 - The Next Decade in AI Four Steps Towards Robust A.pdf;C\:\\Users\\aesou\\Zotero\\storage\\QPJ9HQWY\\2002.html},
  journal = {arXiv:2002.06177 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,I.2,I.2.6},
  primaryClass = {cs}
}

@inproceedings{marino_evolving_2018,
  title = {Evolving {{Action Abstractions}} for {{Real}}-{{Time Planning}} in {{Extensive}}-{{Form Games}}},
  booktitle = {Proceedings of the {{Conference}} on {{Artificial Intelligence}} ({{AAAI}})},
  author = {Mari{\~n}o, Julian R. H. and Moraes, Rubens O. and Toledo, Claudio and Lelis, Levi H. S.},
  year = {2018},
  abstract = {A key challenge for planning systems in real-time
multi-agent domains is to search in large action spaces
to decide an agent's next action. Previous works showed that hand-crafted action abstractions allow planning systems to focus their search on a subset of promising actions. In this paper we show that the problem of generating action abstractions can be cast as a problem of selecting a subset of pure strategies from a pool of options.
We model the selection of a subset of pure strategies
as a two-player game in which the strategy set of
the players is the powerset of the pool of options\textemdash{}
we call this game the subset selection game. We then
present an evolutionary algorithm for solving such a
game. Empirical results on small matches of  RTS
show that our evolutionary approach is able to converge to a Nash equilibrium for the subset selection game. Also, results on larger matches show that search algorithms using action abstractions derived by our evolutionary approach are able to substantially outperform all  state-of-the-art planning systems tested.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Mariño et al. - 2018 - Evolving Action Abstractions for Real-Time Plannin.pdf;C\:\\Users\\aesou\\Zotero\\storage\\NGIE68SA\\Marino et al. - 2019 - Evolving Action Abstractions for Real-Time Plannin.pdf}
}

@inproceedings{marino_learning_2019,
  title = {Learning {{Strategies}} for {{Real}}-{{Time Strategy Games}} with {{Genetic Programming}}},
  booktitle = {Proceedings of the {{AAAI Conference}} on {{Artificial Intelligence}} and {{Interactive Digital Entertainment}}},
  author = {Mari{\~n}o, Julian R. H.},
  year = {2019},
  month = oct,
  volume = {15},
  pages = {219--220},
  abstract = {Planning in real-time strategy (RTS) games is challenging due to their very large state and action spaces. Action abstractions have shown to be a promising approach for dealing with this challenge. Previous approaches induce action abstractions from a small set of hand-crafted strategies, which are used by algorithms to search only on the actions returned by the strategies. Previous works use a set of expert-designed strategies for inducing action abstractions. The main drawback of this approach is that it limits the agent behaviour to the knowledge encoded in the strategies. In this research, we focus on learning novel and effective strategies for RTS games, to induce action abstractions. In addition to being effective, we are interested in learning strategies that can be easily interpreted by humans, allowing a better understanding of the workings of the resulting agent.},
  copyright = {Copyright (c) 2019 Association for the Advancement of Artificial Intelligence},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\526D2U38\\Mariño - 2019 - Learning Strategies for Real-Time Strategy Games w.pdf},
  language = {en}
}

@book{mark_behavioral_2009,
  title = {Behavioral Mathematics for Game {{AI}}},
  author = {Mark, Dave},
  year = {2009},
  publisher = {{Charles River Media, Course Technology, Cengage Learning}},
  address = {{Boston, MA}},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\BN6Z6STG\\Mark - 2009 - Behavioral mathematics for game AI.pdf},
  isbn = {978-1-58450-684-3},
  keywords = {Artificial intelligence,Computer games,Design,Human behavior,Mathematics,Programming},
  language = {en},
  lccn = {QA76.76.C672 M345 2009}
}

@book{mark_behavioral_2009-1,
  title = {Behavioral Mathematics for Game {{AI}}},
  author = {Mark, Dave},
  year = {2009},
  publisher = {{Charles River Media, Course Technology, Cengage Learning}},
  address = {{Boston, MA}},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\6Q7YWRIL\\Mark - 2009 - Behavioral mathematics for game AI.pdf},
  isbn = {978-1-58450-684-3},
  keywords = {Artificial intelligence,Computer games,Design,Human behavior,Mathematics,Programming},
  language = {en},
  lccn = {QA76.76.C672 M345 2009}
}

@article{marsland_review_1986,
  title = {A {{Review}} of {{Game}}-{{Tree Pruning}}},
  author = {Marsland, T. A.},
  year = {1986},
  month = jan,
  volume = {9},
  pages = {3--19},
  publisher = {{IOS Press}},
  issn = {1389-6911},
  doi = {10.3233/ICG-1986-9102},
  abstract = {Chess programs have three major components: move generation, search, and evaluation. All components are important, although evaluation with its quiescence analysis is the part which makes each program's play unique. The speed of a chess program is a},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Marsland - 1986 - A Review of Game-Tree Pruning.pdf;C\:\\Users\\aesou\\Zotero\\storage\\AVBFRPZN\\Marsland - 1986 - A Review of Game-Tree Pruning.pdf;C\:\\Users\\aesou\\Zotero\\storage\\8FFI9N4X\\icg9-1-02.html},
  journal = {ICGA Journal},
  language = {en},
  number = {1}
}

@article{mateas_behavior_2002,
  title = {A Behavior Language for Story-Based Believable Agents},
  author = {Mateas, M. and Stern, A.},
  year = {2002},
  month = jul,
  volume = {17},
  pages = {39--47},
  issn = {1541-1672},
  doi = {10.1109/MIS.2002.1024751},
  abstract = {ABL is a reactive planning language, based on the Oz Project language Hap, designed specifically for authoring believable agents - characters which express rich personality, and which, in our case, play roles in an interactive, dramatic story world. Here we give a brief overview of the language Hap and discuss the new features in ABL, focusing on ABL's support for multi-character coordination. We also describe the ABL idioms we are using to organize character behaviors in the context of an interactive drama.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\BTB5GJKB\\Mateas and Stern - 2002 - A behavior language for story-based believable age.pdf},
  journal = {IEEE Intelligent Systems},
  language = {en},
  number = {4}
}

@incollection{mateas_behavior_2004,
  title = {A {{Behavior Language}}: {{Joint Action}} and {{Behavioral Idioms}}},
  shorttitle = {A {{Behavior Language}}},
  booktitle = {Life-{{Like Characters}}},
  author = {Mateas, Michael and Stern, Andrew},
  editor = {Prendinger, Helmut and Ishizuka, Mitsuru and Gabbay, M. and Siekmann, J{\"o}rg},
  year = {2004},
  pages = {135--161},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-662-08373-4_7},
  abstract = {This chapter presents ABL (A Behavior Language, pronounced "able"), specifically designed to support the creation of life-like computer characters. Concurrent with our development of ABL, we are using the language to implement the believable agent layer of our interactive drama project, Fa\c{c}ade. With code examples and case-studies we describe the primary features of ABL, including sequential and parallel behaviors, joint goals and behaviors for multi-agent coordination, and reflection for meta-behaviors. Specific idioms are detailed for using ABL to author story-based believable agents that can maintain reactive, moment-by-moment believability while simultaneously performing in tightly coordinated long term dramatic discourse sequences, called dramatic beats.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\XNY5LHBB\\Mateas and Stern - 2004 - A Behavior Language Joint Action and Behavioral I.pdf},
  isbn = {978-3-642-05655-0 978-3-662-08373-4},
  language = {en}
}

@inproceedings{mccoy_integrated_2008,
  title = {An {{Integrated Agent}} for {{Playing Real}}-{{Time Strategy Games}}},
  booktitle = {{{AAAI}}},
  author = {McCoy, Joshua and Mateas, Michael},
  year = {2008},
  abstract = {We present a real-time strategy (RTS) game AI agent that integrates multiple specialist components to play a complete game. Based on an analysis of how skilled human players conceptualize RTS gameplay, we partition the problem space into domains of competence seen in expert human play. This partitioning helps us to manage and take advantage of the large amount of sophisticated domain knowledge developed by human players. We present results showing that incorporating expert high-level strategic knowledge allows our agent to consistently defeat established scripted AI players. In addition, this work lays the foundation to incorporate tactics and unit micromanagement techniques developed by both man and},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\J6P7BBGW\\McCoy and Mateas - 2008 - An Integrated Agent for Playing Real-Time Strategy.pdf},
  keywords = {High- and low-level,Problem domain,Real-time clock}
}

@inproceedings{mesentier_silva_evolving_2019,
  title = {Evolving the {{Hearthstone Meta}}},
  booktitle = {2019 {{IEEE Conference}} on {{Games}} ({{CoG}})},
  author = {de Mesentier Silva, Fernando and Canaan, Rodrigo and Lee, Scott and Fontaine, Matthew C. and Togelius, Julian and Hoover, Amy K.},
  year = {2019},
  month = aug,
  pages = {1--8},
  publisher = {{IEEE}},
  address = {{London, United Kingdom}},
  doi = {10.1109/CIG.2019.8847966},
  abstract = {Balancing an ever growing strategic game of high complexity, such as Hearthstone is a complex task. The target of making strategies diverse and customizable results in a delicate intricate system. Tuning over 2000 cards to generate the desired outcome without disrupting the existing environment becomes a laborious challenge. In this paper, we discuss the impacts that changes to existing cards can have on strategy in Hearthstone. By analyzing the win rate on match-ups across different decks, being played by different strategies, we propose to compare their performance before and after changes are made to improve or worsen different cards. Then, using an evolutionary algorithm, we search for a combination of changes to the card attributes that cause the decks to approach equal, 50\% win rates. We then expand our evolutionary algorithm to a multi-objective solution to search for this result, while making the minimum amount of changes, and as a consequence disruption, to the existing cards. Lastly, we propose and evaluate metrics to serve as heuristics with which to decide which cards to target with balance changes.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Mesentier Silva et al. - 2019 - Evolving the Hearthstone Meta.pdf},
  isbn = {978-1-72811-884-0},
  language = {en}
}

@article{metoyer_explaining_2010,
  title = {Explaining How to Play Real-Time Strategy Games},
  author = {Metoyer, Ronald and Stumpf, Simone and Neumann, Christoph and Dodge, Jonathan and Cao, Jill and Schnabel, Aaron},
  year = {2010},
  month = may,
  volume = {23},
  pages = {295--301},
  issn = {09507051},
  doi = {10.1016/j.knosys.2009.11.006},
  abstract = {Real-time strategy games share many aspects with real situations in domains such as battle planning, air traffic control, and emergency response team management which makes them appealing test-beds for Artificial Intelligence (AI) and machine learning. End user annotations could help to provide supplemental information for learning algorithms, especially when training data is sparse. This paper presents a formative study to uncover how experienced users explain game play in real-time strategy games. We report the results of our analysis of explanations and discuss their characteristics that could support the design of systems for use by experienced real-time strategy game users in specifying or annotating strategy-oriented behavior.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\ISJEP7BS\\Metoyer et al. - 2010 - Explaining how to play real-time strategy games.pdf},
  journal = {Knowledge-Based Systems},
  language = {en},
  number = {4}
}

@article{metoyer_explaining_2010-1,
  title = {Explaining How to Play Real-Time Strategy Games},
  author = {Metoyer, Ronald and Stumpf, Simone and Neumann, Christoph and Dodge, Jonathan and Cao, Jill and Schnabel, Aaron},
  year = {2010},
  month = may,
  volume = {23},
  pages = {295--301},
  issn = {09507051},
  doi = {10.1016/j.knosys.2009.11.006},
  abstract = {Real-time strategy games share many aspects with real situations in domains such as battle planning, air traffic control, and emergency response team management which makes them appealing test-beds for Artificial Intelligence (AI) and machine learning. End-user annotations could help to provide supplemental information for learning algorithms, especially when training data is sparse. This paper presents a formative study to uncover how experienced users explain game play in real-time strategy games. We report the results of our analysis of explanations and discuss their characteristics that could support the design of systems for use by experienced real-time strategy game users in specifying or annotating strategy-oriented behavior.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\HF9E52E8\\Metoyer et al. - 2010 - Explaining how to play real-time strategy games 2.pdf},
  journal = {Knowledge-Based Systems},
  language = {en},
  number = {4}
}

@book{millington_ai_2019,
  title = {{{AI}} for Games},
  author = {Millington, Ian},
  year = {2019},
  edition = {Third edition},
  publisher = {{Taylor \& Francis, a CRC title, part of the Taylor \& Francis imprint, a member of the Taylor \& Francis Group, the academic division of T\&F Informa, plc}},
  address = {{Boca Raton}},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Books\\Millington - 2019 - AI for games.pdf},
  isbn = {978-1-138-48397-2},
  keywords = {Artificial intelligence,Computer animation,Computer games,Programming},
  language = {en},
  lccn = {QA76.76.C672 M549 2019}
}

@book{millington_artificial_2009,
  title = {Artificial Intelligence for Games},
  author = {Millington, Ian and Funge, John David},
  year = {2009},
  edition = {2nd ed},
  publisher = {{Morgan Kaufmann/Elsevier}},
  address = {{Burlington, MA}},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\88X37285\\Millington and Funge - 2009 - Artificial intelligence for games.pdf},
  isbn = {978-0-12-374731-0},
  keywords = {Artificial intelligence,Computer animation,Computer games,Programming},
  language = {en},
  lccn = {QA76.76.C672 M549 2009}
}

@incollection{miltersen_computing_2007,
  title = {Computing {{Proper Equilibria}} of {{Zero}}-{{Sum Games}}},
  booktitle = {Computers and {{Games}}},
  author = {Miltersen, Peter Bro and S{\o}rensen, Troels Bjerre},
  year = {2007},
  volume = {4630},
  pages = {200--211},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-75538-8_18},
  abstract = {We show that a proper equilibrium of a matrix game can be found in polynomial time by solving a linear (in the number of pure strategies of the two players) number of linear programs of roughly the same dimensions as the standard linear programs describing the Nash equilibria of the game.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2006\\23 Computing Proper Equilibria of Zero-Sum Games.pdf},
  isbn = {978-3-540-75537-1 978-3-540-75538-8},
  language = {en}
}

@article{mirsoleimani_new_2016,
  title = {A {{New Method}} for {{Parallel Monte Carlo Tree Search}}},
  author = {Mirsoleimani, S. Ali and Plaat, Aske and van den Herik, Jaap and Vermaseren, Jos},
  year = {2016},
  month = may,
  abstract = {In recent years there has been much interest in the Monte Carlo tree search algorithm, a new, adaptive, randomized optimization algorithm. In fields as diverse as Artificial Intelligence, Operations Research, and High Energy Physics, research has established that Monte Carlo tree search can find good solutions without domain dependent heuristics. However, practice shows that reaching high performance on large parallel machines is not so successful as expected. This paper proposes a new method for parallel Monte Carlo tree search based on the pipeline computation pattern.},
  archivePrefix = {arXiv},
  eprint = {1605.04447},
  eprinttype = {arxiv},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Mirsoleimani et al. - 2016 - A New Method for Parallel Monte Carlo Tree Search.pdf},
  journal = {arXiv:1605.04447 [cs]},
  keywords = {Computer Science - Distributed; Parallel; and Cluster Computing},
  language = {en},
  primaryClass = {cs}
}

@inproceedings{mirsoleimani_parallel_2015,
  title = {Parallel {{Monte Carlo Tree Search}} from {{Multi}}-Core to {{Many}}-Core {{Processors}}},
  booktitle = {2015 {{IEEE Trustcom}}/{{BigDataSE}}/{{ISPA}}},
  author = {Mirsoleimani, S. Ali and Plaat, Aske and van den Herik, Jaap and Vermaseren, Jos},
  year = {2015},
  month = aug,
  pages = {77--83},
  publisher = {{IEEE}},
  address = {{Helsinki, Finland}},
  doi = {10.1109/Trustcom.2015.615},
  abstract = {In recent years there has been much interest in the MCTS algorithm, a new, adaptive, randomized optimization algorithm. In fields as diverse as Artificial Intelligence, Operations Research, and High Energy Physics, research has established that MCTS can find good solutions without domain dependent heuristics. However, practice shows that reaching high performance on large parallel machines is not so successful as expected. So far, the reasons are not well understood. This paper investigates the scalability of two popular parallelization approaches (tree parallelization and root parallelization) of the MCTS algorithm, using the Intel Xeon Phi highly multi-threaded shared-memory system. Moreover, we compare the results on a Xeon CPU and a Xeon Phi to understand the scalability of the parallel MCTS algorithms, and to understand their absolute performance. We find that tree parallelization can achieve near perfect speedup for up to 16 threads on the Xeon CPU and up to 64 threads on the Xeon Phi. For root parallelization we find that the effect of locks is small. Moreover, we establish the overall parallel speedup of the two parallelization methods of the MCTS algorithm is fundamentally limited on the Xeon Phi for games such as Hex or Go. The limiting factor is not, as might be expected, the parallel algorithm, or its implementation, but the high level of sequential calculations in each thread, for which no vectorization method is known.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Mirsoleimani et al. - 2015 - Parallel Monte Carlo Tree Search from Multi-core t.pdf},
  isbn = {978-1-4673-7952-6},
  language = {en}
}

@article{mnih_human-level_2015,
  title = {Human-Level Control through Deep Reinforcement Learning},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  year = {2015},
  month = feb,
  volume = {518},
  pages = {529--533},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature14236},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\8KXE8AHF\\Mnih et al. - 2015 - Human-level control through deep reinforcement lea.pdf},
  journal = {Nature},
  language = {en},
  number = {7540}
}

@book{moore_basics_2016,
  title = {Basics of {{Game Design}}},
  author = {Moore, Michael},
  year = {2016},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\PMMVSR3A\\Moore - 2016 - Basics of Game Design.pdf},
  isbn = {978-1-4398-6776-1},
  language = {English}
}

@inproceedings{moraes_action_2018,
  title = {Action {{Abstractions}} for {{Combinatorial Multi}}-{{Armed Bandit Tree Search}}},
  booktitle = {{{AAAI Publications}}, {{Fourteenth Artificial Intelligence}} and {{Interactive Digital Entertainment Conference}}},
  author = {Moraes, Rubens O and Mari{\~n}o, Julian R. H. and Lelis, Levi H. S. and Nascimento, Mario A.},
  year = {2018},
  abstract = {Search algorithms based on combinatorial multi-armed bandits (CMABs) are promising for dealing with state-space sequential decision problems. However, current CMAB-based algorithms do not scale to problem domains with very large actions spaces, such as real-time strategy games played in large maps. In this paper we introduce CMAB-based search algorithms that use action abstraction schemes to reduce the action space considered during search. One of the approaches we introduce use regular action abstractions (A1N), while the other two use asymmetric action abstractions (A2N and A3N). Empirical results on \textmu{}RTS show that A1N, A2N, and A3N are able to outperform an existing CMAB-based algorithm in matches played in large maps, and A3N is able to outperform all state-of-the-art search algorithms tested.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\6VVE949K\\Moraes - 2018 - Action Abstractions for Combinatorial Multi-Armed .pdf},
  language = {en}
}

@inproceedings{moraes_asymmetric_2018,
  title = {Asymmetric {{Action Abstractions}} for {{Multi}}-{{Unit Control}} in {{Adversarial Real}}-{{Time Games}}},
  booktitle = {The {{Thirty}}-{{Second AAAI Conference}} on {{Artificial Intelligence AAAI}}-18},
  author = {Moraes, Rubens O and Lelis, Levi H S},
  year = {2018},
  pages = {876--883},
  abstract = {Action abstractions restrict the number of legal actions available during search in multi-unit real-time adversarial games, thus allowing algorithms to focus their search on a set of promising actions. Optimal strategies derived from unabstracted spaces are guaranteed to be no worse than optimal strategies derived from action-abstracted spaces. In practice, however, due to real-time constraints and the state space size, one is only able to derive good strategies in un-abstracted spaces in small-scale games. In this paper we introduce search algorithms that use an action abstraction scheme we call asymmetric abstraction. Asymmetric abstractions retain the un-abstracted spaces' theoretical advantage over regularly abstracted spaces while still allowing the search algorithms to derive effective strategies, even in large-scale games. Empirical results on combat scenarios that arise in a real-time strategy game show that our search algorithms are able to substantially outperform state-of-the-art approaches.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Moraes and Lelis - 2018 - Asymmetric Action Abstractions for Multi-Unit Cont.pdf},
  language = {en}
}

@inproceedings{moraes_nested-greedy_2018,
  title = {Nested-{{Greedy Search}} for {{Adversarial Real}}-{{Time Games}}},
  booktitle = {{{AIIDE}}},
  author = {Moraes, Rubens O and Lelis, Levi H S},
  year = {2018},
  abstract = {Churchill and Buro (2013) launched a line of research through Portfolio Greedy Search (PGS), an algorithm for adversarial real-time planning that uses scripts to simplify the problem's action space. In this paper we present a problem in PGS's search scheme that has hitherto been overlooked. Namely, even under the strong assumption that PGS is able to evaluate all actions available to the player, PGS might fail to return the best action. We then describe an idealized algorithm that is guaranteed to return the best action and present an approximation of such algorithm, which we call NestedGreedy Search (NGS). Empirical results on \textmu{}RTS show that NGS is able to outperform PGS as well as state-of-the-art methods in matches played in small to medium-sized maps.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\6AXIZA8N\\Moraes and Lelis - 2018 - Nested-Greedy Search for Adversarial Real-Time Gam.pdf},
  language = {en}
}

@article{muszycka_empirical_1985,
  title = {An Empirical Comparison of Pruning Strategies in Game Trees},
  author = {Muszycka, Agata and Shinghal, Rajjan},
  year = {1985},
  month = may,
  volume = {SMC-15},
  pages = {389--399},
  issn = {2168-2909},
  doi = {10.1109/TSMC.1985.6313374},
  abstract = {Size pruning strategies on uniform and nonuniform game trees of 24 different sizes, each being assigned leaf-node static values under four different schemes, are compared. The performance of these strategies is compared on the basis of nodes created, node visits, and CPU time. These are believed to be the most exhaustive experimental results so far reported.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Muszycka and Shinghal - 1985 - An empirical comparison of pruning strategies in g.pdf;C\:\\Users\\aesou\\Zotero\\storage\\SPEYBFPP\\6313374.html},
  journal = {IEEE Transactions on Systems, Man, and Cybernetics},
  keywords = {computer games,CPU time,Cybernetics,game trees,Games,Humans,leaf-node static values,Military aircraft,node visits,Observers,pruning strategies,Reactive power,trees (mathematics),Visualization},
  number = {3}
}

@incollection{neller_monte_2016,
  title = {Monte {{Carlo Approaches}} to {{Parameterized Poker Squares}}},
  booktitle = {Computers and {{Games}}},
  author = {Neller, Todd W. and Yang, Zuozhi and Messinger, Colin M. and Anton, Calin and {Castro-Wunsch}, Karo and Maga, William and Bogaerts, Steven and Arrington, Robert and Langley, Clay},
  year = {2016},
  volume = {10068},
  pages = {22--33},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-50935-8_3},
  abstract = {Parameterized Poker Squares (PPS) is a generalization of Poker Squares where players must adapt to a point system supplied at play time and thus dynamically compute highly-varied strategies. Herein, we detail the top three performing AI players in a PPS research competition, all three of which make various use of Monte Carlo techniques.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2016\\7 Monte Carlo Approaches to Parameterized Poker Squares.pdf},
  isbn = {978-3-319-50934-1 978-3-319-50935-8},
  language = {en}
}

@incollection{neller_optimal_2014,
  title = {Optimal, {{Approximately Optimal}}, and {{Fair Play}} of the {{Fowl Play Card Game}}},
  booktitle = {Computers and {{Games}}},
  author = {Neller, Todd W. and Malec, Marcin and Presser, Clifton G. M. and Jacobs, Forrest},
  year = {2014},
  volume = {8427},
  pages = {233--243},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-09165-5_20},
  abstract = {After introducing the jeopardy card game Fowl Play, we present equations for optimal two-player play, describe their solution with a variant of value iteration, and visualize the optimal play policy. Next, we discuss the approximation of optimal play and note that neural network learning can achieve a win rate within 1 \% of optimal play yet with a 5-orders-of-magnitude reduction in memory requirements. Optimal komi (i.e., compensation points) are computed for the two-player games of Pig and Fowl Play. Finally, we make use of such komi computations in order to redesign Fowl Play for two-player fairness, creating the game Red Light.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2013\\24 Optimal, Approximately Optimal, and Fair Play of the Fowl Play Card Game.pdf},
  isbn = {978-3-319-09164-8 978-3-319-09165-5},
  language = {en}
}

@incollection{neller_rook_2011,
  title = {Rook {{Jumping Maze Design Considerations}}},
  booktitle = {Computers and {{Games}}},
  author = {Neller, Todd W. and Fisher, Adrian and Choga, Munyaradzi T. and Lalvani, Samir M. and McCarty, Kyle D.},
  year = {2011},
  volume = {6515},
  pages = {188--198},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-17928-0_18},
  abstract = {We define the Rook Jumping Maze, provide historical perspective, and describe a generation method for such mazes. When applying stochastic local search algorithms to maze design, most creative effort concerns the definition of an objective function that rates maze quality. We define and discuss several maze features to consider in such a function definition. Finally, we share our preferred design choices, make design process observations, and note the applicability of these techniques to variations of the Rook Jumping Maze.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2010\\23 Rook Jumping Maze Design Considerations.pdf},
  isbn = {978-3-642-17927-3 978-3-642-17928-0},
  language = {en}
}

@incollection{nesic_heuristic_2016,
  title = {Heuristic {{Function Evaluation Framework}}},
  booktitle = {Computers and {{Games}}},
  author = {Ne{\v s}i{\'c}, Nera and Schiffel, Stephan},
  year = {2016},
  volume = {10068},
  pages = {71--80},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-50935-8_7},
  abstract = {We present a heuristic function evaluation framework that allows to quickly compare a heuristic function's output to benchmark values that are precomputed for a subset of the state space of the game. Our framework reduces the time to evaluate a heuristic function drastically while also providing some insight into where the heuristic is performing well or below par. We analyze the feasibility of using Monte-Carlo Tree Search to compute benchmark values instead of relying on game theoretic values that are hard to obtain in many cases. We also propose several metrics for comparing heuristic evaluations to benchmark values and discuss the feasibility of using MCTS benchmarks with those metrics.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2016\\11 Heuristic Function Evaluation Framework.pdf},
  isbn = {978-3-319-50934-1 978-3-319-50935-8},
  language = {en}
}

@article{neufeld_building_2019,
  title = {Building a {{Planner}}: {{A Survey}} of {{Planning Systems Used}} in {{Commercial Video Games}}},
  shorttitle = {Building a {{Planner}}},
  author = {Neufeld, Xenija and Mostaghim, Sanaz and {Sancho-Pradel}, Dario L. and Brand, Sandy},
  year = {2019},
  month = jun,
  volume = {11},
  pages = {91--108},
  issn = {2475-1502, 2475-1510},
  doi = {10.1109/TG.2017.2782846},
  abstract = {In the last decade, many commercial video games have used planners instead of classical Behavior Trees or Finite State Machines to define agent behaviors. Planners allow looking ahead in time and can prevent some problems of purely reactive systems. Furthermore, some of them allow coordination of multiple agents. However, implementing a planner for highlydynamic environments like video games is a difficult task. This work aims to provide an overview of different elements of planners and the problems that developers might have when dealing with them. We identify the major areas of plan creation and execution, trying to guide developers through the process of implementing a planner and discuss possible solutions for problems that may arise in the following areas: environment, planning domain, goals, agents, actions, plan creation and plan execution processes. Giving insights into multiple commercial games, we show different possibilities of solving such problems and discuss which solutions are better suited under specific circumstances and why some academic approaches find a limited application in the context of commercial titles.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Neufeld et al. - 2019 - Building a Planner A Survey of Planning Systems U.pdf},
  journal = {IEEE Transactions on Games},
  language = {en},
  number = {2}
}

@inproceedings{neufeld_evolving_2019,
  title = {Evolving {{Game State Evaluation Functions}} for a {{Hybrid Planning Approach}}},
  booktitle = {{{IEEE Conference}} on {{Games}} 2019},
  author = {Neufeld, Xenija and Mostaghim, Sanaz and {Perez-Liebana}, Diego},
  year = {2019},
  month = aug,
  abstract = {Real-time games often require a combination of long-term and short-term planning as well as interleaved planning and execution. In our previous work, we introduced a hybrid planning and execution approach, in which high-level strategical planning is performed by a Hierarchical Task Network Planner and micro-management is done through Monte Carlo Tree Search. We use evaluation functions that represent weighted sums of selected game features as an interface between the two hierarchy levels.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Neufeld et al. - 2019 - Evolving Game State Evaluation Functions for a Hyb.pdf},
  language = {en}
}

@inproceedings{neufeld_hybrid_2018,
  title = {A {{Hybrid Approach}} to {{Planning}} and {{Execution}} in {{Dynamic Environments Through Hierarchical Task Networks}} and {{Behavior Trees}}},
  booktitle = {Fourteenth {{Artificial Intelligence}} and {{Interactive Digital Entertainment Conference}} ({{AIIDE}} 2018)},
  author = {Neufeld, Xenija and Mostaghim, Sanaz and Brand, Sandy},
  year = {2018},
  pages = {7},
  abstract = {Intelligent autonomous agents that are acting in dynamic environments in real-time are often required to follow long-term strategies while also remaining reactive and being able to act deliberately. In order to create intelligent behaviors for video game characters, there are two common approaches \textendash{} planners are used for long-term strategical planning, whereas Behavior Trees allow for reactive acting. Although both methodologies have their advantages, when used on their own, they fail to fully achieve both requirements described above. In this work, we propose a hybrid approach combining a Hierarchical Task Network planner for high-level planning while delegating low-level decision making and acting to Behavior Trees. Furthermore, we compare this approach with a pure planner in a multi-agent environment.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Neufeld et al. - 2018 - A Hybrid Approach to Planning and Execution in Dyn.pdf},
  language = {en}
}

@inproceedings{neufeld_hybrid_2019,
  title = {A {{Hybrid Planning}} and {{Execution Approach Through HTN}} and {{MCTS}}},
  booktitle = {The 3rd {{Workshop}} on {{Integrated Planning}}, {{Acting}}, and {{Execution}} - {{ICAPS}}'19},
  author = {Neufeld, Xenija and Mostaghim, Sanaz and {Perez-Liebana}, Diego},
  year = {2019},
  month = jul,
  pages = {37--45},
  abstract = {Many planning environments require from an agent to show a combination of long-term strategical behavior and reactive short-term tactical behavior. In order to combine planning on both hierarchy levels and to detect potential failures, they also require an interleaved planning and execution approach. In this work, we propose a hybrid planning approach with a Hierarchical Task Network planner being responsible for strategical planning and Monte Carlo Tree Search taking over the tactical decision-making. We describe a possible way to connect these layers and a monitoring system that is able to detect failures on higher hierarchy levels during execution. The proposed approach is tested in a Real Time Strategy game that offers a highly-dynamic and non-deterministic multi-unit environment.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Neufeld et al. - 2019 - A Hybrid Planning and Execution Approach Through H.pdf},
  language = {en}
}

@incollection{nijssen_enhancements_2011,
  title = {Enhancements for {{Multi}}-{{Player Monte}}-{{Carlo Tree Search}}},
  booktitle = {Computers and {{Games}}},
  author = {Nijssen, J. A. M. and Winands, Mark H. M.},
  year = {2011},
  volume = {6515},
  pages = {238--249},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-17928-0_22},
  abstract = {Monte-Carlo Tree Search (MCTS) is becoming increasingly popular for playing multi-player games. In this paper we propose two enhancements for MCTS in multi-player games: (1) Progressive History and (2) Multi-Player Monte-Carlo Tree Search Solver (MP-MCTS-Solver). We analyze the performance of these enhancements in two different multi-player games: Focus and Chinese Checkers. Based on the experimental results we conclude that Progressive History is a considerable improvement in both games and MP-MCTS-Solver, using the standard update rule, is a genuine improvement in Focus.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2010\\27 Enhancements for Multi-Player Monte-Carlo Tree Search.pdf},
  isbn = {978-3-642-17927-3 978-3-642-17928-0},
  language = {en}
}

@incollection{niu_improved_2008,
  title = {An {{Improved Safety Solver}} in {{Go Using Partial Regions}}},
  booktitle = {Computers and {{Games}}},
  author = {Niu, Xiaozhen and M{\"u}ller, Martin},
  year = {2008},
  volume = {5131},
  pages = {102--112},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-87608-3_10},
  abstract = {Previous safety-of-territory solvers for the game of Go have worked on whole regions surrounded by stones of one color. Their applicability is limited to small to medium-size regions. We describe a new technique that is able to prove that parts of large regions are safe. By using pairs of dividing points, even huge regions can be divided into smaller partial regions that can be proven much easier and faster. Our experimental results show that the new technique significantly improves the performance of our previous state of the art safety-of-territory solver. Especially in earlier game phases, the solver utilizing the new technique outperforms the previous solver by a large margin.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2008\\15 An Improved Safety Solver in Go Using Partial Regions.pdf},
  isbn = {978-3-540-87607-6 978-3-540-87608-3},
  language = {en}
}

@incollection{niu_open_2007,
  title = {An {{Open Boundary Safety}}-of-{{Territory Solver}} for the {{Game}} of {{Go}}},
  booktitle = {Computers and {{Games}}},
  author = {Niu, Xiaozhen and M{\"u}ller, Martin},
  year = {2007},
  volume = {4630},
  pages = {37--49},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-75538-8_4},
  abstract = {This paper presents Safety Solver 2.0, a safety-of-territory solver for the game of Go that can solve problems in areas with open boundaries. Previous work on assessing safety of territory has concentrated on regions that are completely surrounded by stones of one player. Safety Solver 2.0 can identify open boundary problems under real game conditions, and generate moves for invading or defending such areas. Several search enhancements improve the solver's performance. The experimental results demonstrate that the solver can find good moves in small to medium-size open boundary areas.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2006\\9 An Open Boundary Safety-of-Territory Solver for the Game of Go.pdf},
  isbn = {978-3-540-75537-1 978-3-540-75538-8},
  language = {en}
}

@misc{noauthor_alphastar_nodate,
  title = {{{AlphaStar}}: {{Mastering}} the {{Real}}-{{Time Strategy Game StarCraft II}}},
  shorttitle = {{{AlphaStar}}},
  abstract = {StarCraft, considered to be one of the most challenging Real-Time Strategy games and one of the longest-played esports of all time, has emerged by consensus as a ``grand challenge'' for AI research. Here, we introduce our StarCraft II program AlphaStar, the first Artificial Intelligence to defeat a top professional player.},
  howpublished = {https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/},
  journal = {DeepMind}
}

@misc{noauthor_alphastar_nodate-1,
  title = {{AlphaStar: Grandmaster level in StarCraft II using multi-agent reinforcement learning}},
  shorttitle = {{AlphaStar}},
  abstract = {We research and build safe AI systems that learn how to solve problems and advance scientific discovery for all. Explore our work: deepmind.com/research},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\ULGULHQE\\AlphaStar-Grandmaster-level-in-StarCraft-II-using-multi-agent-reinforcement-learning.html},
  howpublished = {/blog/article/AlphaStar-Grandmaster-level-in-StarCraft-II-using-multi-agent-reinforcement-learning},
  journal = {Deepmind},
  language = {ALL}
}

@misc{noauthor_are_2019,
  title = {Are {{Behavior Trees}} a {{Thing}} of the {{Past}}?},
  year = {2019},
  month = feb,
  abstract = {Introducing the Next Generation of AI: How Utility AIs Are Replacing Behaviour Trees.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\7WKMX37Z\\Are_Behavior_Trees_a_Thing_of_the_Past.html},
  language = {en}
}

@article{noauthor_battle_2018,
  title = {Battle Royale Game},
  year = {2018},
  month = nov,
  abstract = {A battle royale game, also spelled battle royal, is a video game genre that blends the survival, exploration and scavenging elements of a survival game with last-man-standing gameplay. Battle royale games challenge a large number of players, starting with minimal equipment, to search for weapons and armor and eliminate all other opponents while avoiding being trapped outside of a shrinking "safe area", with the winner being the last competitor in the game. The name for the genre is taken from the 2000 Japanese film Battle Royale, which presents a similar theme of a last-man-standing competition in a shrinking play zone.The genre's origins arose from mods for large-scale online survival games like Minecraft and ARMA 2, before becoming popularized with standalone games such as PlayerUnknown's Battlegrounds, which had sold over 40 million copies by mid-2018. The same year, the free-to-play Fortnite Battle Royale rapidly became a cultural phenomenon, overtaking Battlegrounds in terms of player numbers and revenue.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\9M9ZSFHR\\index.html},
  journal = {Wikipedia},
  language = {en}
}

@article{noauthor_dune_2018,
  title = {Dune {{II}}},
  year = {2018},
  month = dec,
  abstract = {Dune II: The Building of a Dynasty (titled Dune II: Battle for Arrakis in Europe and Dune: The Battle for Arrakis for the North American Mega Drive/Genesis port respectively) is a real-time strategy Dune video game developed by Westwood Studios and released by Virgin Games in December 1992. It is based upon David Lynch's 1984 movie Dune, an adaptation of Frank Herbert's science fiction novel of the same name.
While not necessarily the first real-time strategy (RTS) video game, Dune II established the format that would be followed for years to come. As such, Dune II is the archetypal "real-time strategy" game. Striking a balance between complexity and innovation, it was a huge success and laid the foundation for Command \& Conquer, Warcraft, StarCraft, and many other RTS games that followed.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\S8EWAK7F\\index.html},
  journal = {Wikipedia},
  language = {en}
}

@misc{noauthor_feature_2004,
  title = {Feature},
  year = {2004},
  month = sep,
  file = {C\:\\Users\\aesou\\Zotero\\storage\\RDUTD52H\\feature.html},
  howpublished = {https://web.archive.org/web/20040913063641/http://www.1up.com/do/feature?cId=3134179}
}

@misc{noauthor_fortnite_2018,
  title = {Fortnite | {{Battle Royale}}},
  year = {2018},
  abstract = {Fortnite Battle Royale is the FREE 100-player PvP mode in Fortnite. One giant map. A battle bus. Fortnite building skills and destructible environments combined with intense PvP combat. The last one standing wins. Available on PC, PlayStation 4, Xbox One, Nintendo Switch, Android, iOS \& Mac.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\Q7EZ8S4Z\\battle-royale.html},
  howpublished = {https://www.epicgames.com/fortnite/en-US/buy-now/battle-royale},
  journal = {Epic Games' Fortnite},
  language = {en-US}
}

@misc{noauthor_global_2018,
  title = {Global {{Games Market Revenues}} 2018 | {{Per Region}} \& {{Segment}} | {{Newzoo}}},
  year = {2018},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\TLC35HE9\\global-games-market-reaches-137-9-billion-in-2018-mobile-games-take-half.html},
  howpublished = {https://newzoo.com/insights/articles/global-games-market-reaches-137-9-billion-in-2018-mobile-games-take-half/}
}

@misc{noauthor_global_2018-1,
  title = {Global {{Games Market Revenues}} 2018 | {{Per Region}} \& {{Segment}}},
  year = {2018},
  abstract = {The global games market will reach \$137.9Bn in 2018 with mobile game revenues at \$70.3 billion. PC games and console games will generate \$32.9Bn and \$34.6Bn, respectively. In total, digital game revenues will reach \$125.4Bn, or 91\% of the total market. Learn more.},
  journal = {Newzoo},
  keywords = {\#nosource}
}

@article{noauthor_real-time_2018,
  title = {Real-Time Strategy},
  year = {2018},
  month = dec,
  abstract = {Real-time strategy (RTS) is a subgenre of strategy video games where the game does not progress incrementally in turns.In an RTS, the participants position and maneuver units and structures under their control to secure areas of the map and/or destroy their opponents' assets. In a typical RTS, it is possible to create additional units and structures during the course of a game. This is generally limited by a requirement to expend accumulated resources. These resources are in turn garnered by controlling special points on the map and/or possessing certain types of units and structures devoted to this purpose. More specifically, the typical game of the RTS genre features resource gathering, base building, in-game technological development and indirect control of units. The term "real-time strategy" was coined by Brett Sperry to market Dune II in the early 1990s.The tasks a player must perform to succeed at an RTS can be very demanding, and complex user interfaces have evolved to cope with the challenge. Some features have been borrowed from desktop environments; for example, the technique of "clicking and dragging" to select all units under a given area. Though some game genres share conceptual and gameplay similarities with the RTS template, recognized genres are generally not subsumed as RTS games. For instance, city-building games, construction and management simulations, and games of the real-time tactics variety are generally not considered to be "real-time strategy".},
  copyright = {Creative Commons Attribution-ShareAlike License},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\SILGRFX8\\index.html},
  journal = {Wikipedia},
  language = {en}
}

@article{noauthor_video_2018,
  title = {Video Game},
  year = {2018},
  month = dec,
  abstract = {A video game is an electronic game that involves interaction with a user interface to generate visual feedback on a two- or three-dimensional video display device such as a TV screen or computer monitor. Since the 1980s, video games have become an increasingly important part of the entertainment industry, and whether they are also a form of art is a matter of dispute.
The electronic systems used to play video games are called platforms. Video games are developed and released for one or several platforms and may not be available on others. Specialized platforms such as arcade games, which present the game in a large, typically coin-operated chassis, were common in the 1980s in video arcades, but declined in popularity as other, more affordable platforms became available. These include dedicated devices such as video game consoles, as well as general-purpose computers like a laptop, desktop or handheld computing devices. 
The input device used for games, the game controller, varies across platforms. Common controllers include gamepads, joysticks, mouse devices, keyboards, the touchscreens of mobile devices, or even a person's body, using a Kinect sensor. Players view the game on a display device such as a television or computer monitor or sometimes on virtual reality head-mounted display goggles. There are often game sound effects, music and voice actor lines which come from loudspeakers or headphones. Some games in the 2000s include haptic, vibration-creating effects, force feedback peripherals and virtual reality headsets.
In the 2010s, the commercial importance of the video game industry is increasing. The emerging Asian markets and mobile games on smartphones in particular are driving the growth of the industry. As of 2015, video games generated sales of US\$74 billion annually worldwide, and were the third-largest segment in the U.S. entertainment market, behind broadcast and cable TV.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\G4TVJLH6\\index.html},
  journal = {Wikipedia},
  language = {en}
}

@article{nogueira_collazo_virtual_2014,
  title = {Virtual Player Design Using Self-Learning via Competitive Coevolutionary Algorithms},
  author = {Nogueira Collazo, Mariela and Cotta, Carlos and {Fern{\'a}ndez-Leiva}, Antonio J.},
  year = {2014},
  month = jun,
  volume = {13},
  pages = {131--144},
  issn = {1567-7818, 1572-9796},
  doi = {10.1007/s11047-014-9411-3},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\SK8XCTM3\\Nogueira Collazo et al. - 2014 - Virtual player design using self-learning via comp.pdf},
  journal = {Natural Computing},
  language = {en},
  number = {2}
}

@inproceedings{norling_creating_2004,
  title = {Creating {{Interactive Characters}} with {{BDI Agents}}},
  booktitle = {Proceedings of the {{Australian Workshop}} on {{Interactive Entertainment}} ({{IE}}'04)},
  author = {Norling, Emma and Sonenberg, Liz},
  year = {2004},
  pages = {8},
  abstract = {This paper discusses the use of BDI agents for the development of human-like synthetic characters. The folk psychological roots of the paradigm map closely to the way people typically explain both their behaviour and that of others, and this greatly facilitates knowledge elicitation and representation. This is illustrated through some examples from a project in which models of expert players of Quake 2 were developed. The knowledge elicitation methodology that was used is explained, and samples of the code are presented, demonstrating the way in which a BDI-based agent programming language can clearly and succinctly capture individual differences. The example presented is of modelling expert players in an existing game, but the paper argues that the same techniques can be used to build a completely original character, using a role-player as the basis. Finally, some of the limitations of the BDI paradigm are examined, with a brief discussion of how they can be addressed, using the existing framework as a basis.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Norling and Sonenberg - 2004 - Creating Interactive Characters with BDI Agents.pdf},
  language = {en}
}

@article{novak_classification_2018,
  title = {Classification of Modern Real-Time Strategy Game Worlds},
  author = {Novak, Damijan and {\v C}ep, Ale{\v s} and Verber, Domen},
  year = {2018},
  pages = {6},
  abstract = {The research area of real-time strategy (RTS) games is fragmented and vast. We addressed this issue by establishing a classification of modern RTS game worlds. First, we describe the four aspects (environmental, hierarchy abstraction, strength and time) that comprise the rich worlds of RTS games. Second, based on these aspects, three main classes of RTS game worlds are created, and the classification interval is defined. The interval is necessary for the further creation of new (sub)classes if the need arises. Third, classification is made of modern RTS game worlds (StarCraft, microRTS, SparCraft, and others).},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\D2KRW3Y3\\Novak et al. - 2018 - Classification of modern real-time strategy game w.pdf},
  language = {en}
}

@article{novak_comparison_2020,
  title = {A {{Comparison}} of {{Evolutionary}} and {{Tree}}-{{Based Approaches}} for {{Game Feature Validation}} in {{Real}}-{{Time Strategy Games}} with a {{Novel Metric}}},
  author = {Novak, Damijan and Verber, Domen and Dugonik, Jani and Fister, Iztok},
  year = {2020},
  month = may,
  volume = {8},
  pages = {688},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  doi = {10.3390/math8050688},
  abstract = {When it comes to game playing, evolutionary and tree-based approaches are the most popular approximate methods for decision making in the artificial intelligence field of game research. The evolutionary domain therefore draws its inspiration for the design of approximate methods from nature, while the tree-based domain builds an approximate representation of the world in a tree-like structure, and then a search is conducted to find the optimal path inside that tree. In this paper, we propose a novel metric for game feature validation in Real-Time Strategy (RTS) games. Firstly, the identification and grouping of Real-Time Strategy game features is carried out, and, secondly, groups are included into weighted classes with regard to their correlation and importance. A novel metric is based on the groups, weighted classes, and how many times the playtesting agent invalidated the game feature in a given game feature scenario. The metric is used in a series of experiments involving recent state-of-the-art evolutionary and tree-based playtesting agents. The experiments revealed that there was no major difference between evolutionary-based and tree-based playtesting agents.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Novak et al. - 2020 - A Comparison of Evolutionary and Tree-Based Approa.pdf;C\:\\Users\\aesou\\Zotero\\storage\\AAGGSRNW\\Novak et al. - 2020 - A Comparison of Evolutionary and Tree-Based Approa.pdf;C\:\\Users\\aesou\\Zotero\\storage\\ZSASLFMH\\688.html},
  journal = {Mathematics},
  keywords = {evolutionary computation,game feature,game simulation,game trees,playtesting,playtesting metric,validation},
  language = {en},
  number = {5}
}

@article{novak_real-time_2013,
  title = {Real-{{Time Strategy Games Bot Based}} on a {{Non}}- {{Simultaneous Human}}-{{Like Movement Characteristic}}},
  author = {Novak, Damijan Novak and Verber, Domen},
  editor = {{Global Science and Technology Forum Pte Ltd}},
  year = {2013},
  month = jul,
  volume = {3},
  issn = {22513043},
  doi = {10.5176/2251-3043_3.2.255},
  abstract = {This paper discusses how to improve the behaviour of artificial intelligence (AI) algorithms during real-time strategy games so as to behave more like human players. If we want to achieve this goal we must take into consideration several aspects of human psychology \textendash{} human characteristics. Here we focused on the limited reaction times of the players in contrast to the enormous speed of modern computers. We propose an approach that mimics the limitations of the human reaction times. In order to work properly, the AI must know the average reaction times of the players. Some techniques and proposed algorithm outline are presented on how to achieve this.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\WQQYSSY2\\Novak and Verber - 2013 - Real-Time Strategy Games Bot Based on a Non- Simul.pdf},
  journal = {GSTF International Journal on Computing (JoC Vol.3 No.2)},
  language = {en},
  number = {2}
}

@article{nunes_learning_nodate,
  title = {Learning Decision Trees through {{Monte Carlo}} Tree Search: {{An}} Empirical Evaluation},
  shorttitle = {Learning Decision Trees through {{Monte Carlo}} Tree Search},
  author = {Nunes, Cec{\'i}lia and Craene, Mathieu De and Langet, H{\'e}l{\`e}ne and Camara, Oscar and Jonsson, Anders},
  volume = {n/a},
  pages = {e1348},
  issn = {1942-4795},
  doi = {10.1002/widm.1348},
  abstract = {Decision trees (DTs) are a widely used prediction tool, owing to their interpretability. Standard learning methods follow a locally optimal approach that trades off prediction performance for computational efficiency. Such methods can however be far from optimal, and it may pay off to spend more computational resources to increase performance. Monte Carlo tree search (MCTS) is an approach to approximate optimal choices in exponentially large search spaces. We propose a DT learning approach based on the Upper Confidence Bound applied to tree (UCT) algorithm, including procedures to expand and explore the space of DTs. To mitigate the computational cost of our method, we employ search pruning strategies that discard some branches of the search tree. The experiments show that proposed approach outperformed the C4.5 algorithm in 20 out of 31 datasets, with statistically significant improvements in the trade-off between prediction performance and DT complexity. The approach improved locally optimal search for datasets with more than 1,000 instances, or for smaller datasets likely arising from complex distributions. This article is categorized under: Algorithmic Development {$>$} Hierarchies and Trees Application Areas {$>$} Data Mining Software Tools Fundamental Concepts of Data and Knowledge {$>$} Data Concepts},
  copyright = {\textcopyright{} 2020 Wiley Periodicals, Inc.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Nunes et al. - Learning decision trees through Monte Carlo tree s.pdf;C\:\\Users\\aesou\\Zotero\\storage\\9TLBWGQY\\widm.html},
  journal = {WIREs Data Mining and Knowledge Discovery},
  keywords = {data mining,decision trees,Monte Carlo tree search},
  language = {en},
  number = {n/a}
}

@incollection{obata_consultation_2011,
  title = {Consultation {{Algorithm}} for {{Computer Shogi}}: {{Move Decisions}} by {{Majority}}},
  shorttitle = {Consultation {{Algorithm}} for {{Computer Shogi}}},
  booktitle = {Computers and {{Games}}},
  author = {Obata, Takuya and Sugiyama, Takuya and Hoki, Kunihito and Ito, Takeshi},
  year = {2011},
  volume = {6515},
  pages = {156--165},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-17928-0_15},
  abstract = {A new algorithm that runs on a computer with interconnected processors has been designed for Shogi. The algorithm adopts consultation between many individual players. A method that can create multiple players from one program is presented. Applying a simple rule to select a decision on a single move, the consultation algorithm improves the performance of computer Shogi engines. It is also demonstrated that a council system consisting of three well-known Shogi programs: YSS, GPS, and BONANZA plays better games than any of the three programs individually.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2010\\20 Consultation Algorithm for Computer Shogi Move Decisions by Majority.pdf},
  isbn = {978-3-642-17927-3 978-3-642-17928-0},
  language = {en}
}

@inproceedings{oh_identifying_2017,
  title = {Identifying the {{Rush Strategies}} in the {{Game Logs}} of the {{Real}}-{{Time Strategy Game StarCraft}}-{{II}}},
  booktitle = {The 31st {{Annual Conference}} of the {{Japanese Society}} for {{Artificial Intelligence}}, 2017},
  author = {Oh, Hyunwoo and Budianto, Teguh and Ding, Yi and Long, Zi and Utsuro, Takehito},
  year = {2017},
  pages = {4},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\47SE9E78\\Oh et al. - 2017 - Identifying the Rush Strategies in the Game Logs o.pdf},
  language = {en}
}

@article{oh_playing_2017,
  title = {Playing Real-Time Strategy Games by Imitating Human Players' Micromanagement Skills Based on Spatial Analysis},
  author = {Oh, In-Seok and Cho, Hochul and Kim, Kyung-Joong},
  year = {2017},
  month = apr,
  volume = {71},
  pages = {192--205},
  issn = {09574174},
  doi = {10.1016/j.eswa.2016.11.026},
  abstract = {Unlike the situation with board games, artificial intelligence (AI) for real-time strategy (RTS) games usually suffers from numerous possible future outcomes because the state of the game is continuously changing in real time. Furthermore, AI is also required to be able to handle the increased complexity within a small amount of time. This constraint makes it difficult to build AI for RTS games with current state-of-the art intelligence techniques. A human player, on the other hand, is proficient in dealing with this level of complexity, making him a better game player than AI bots. Human players are especially good at controlling many units at the same time. It is hard to explain the micro-level control skills needed with only a few rules programmed into the bots. The design of micromanagement skills is one of the most difficult parts in the StarCraft AI design because it must be able to handle different combinations of units, army size, and unit placement. The unit control skills can have a big effect on the final outcome of a full game in professional player matches. For StarCraft AI competitions, they employed a relatively simple scripted AI to implement the unit control strategy. However, it is difficult to generate cooperative behavior using the simple AI strategies. Although there has been a few research done on micromanagement skills, it is still a challenging problem to design human-like high-level control skills. In this paper, we proposed the use of imitation learning based on human replays and influence map representation. In this approach, we extracted huge numbers of cases from the replays of experts and used them to determine the actions of units in the current game case. This was done without using any hand-coded rules. Because this approach is data-driven, it was essential to minimize the case search times. To support fast and accurate matching, we chose to use influence maps and data hashing. They allowed the imitation system to respond within a small amount time (one frame, 0.042 seconds). With a very large number of cases (up to 500,000 cases), we showed that it is possible to respond competitively in real-time, with a high winning percentage in micromanagement scenarios.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\EPIECLZV\\Oh et al. - 2017 - Playing real-time strategy games by imitating huma.pdf},
  journal = {Expert Systems with Applications},
  language = {en}
}

@incollection{oka_systematic_2016,
  title = {Systematic {{Selection}} of {{N}}-{{Tuple Networks}} for 2048},
  booktitle = {Computers and {{Games}}},
  author = {Oka, Kazuto and Matsuzaki, Kiminori},
  year = {2016},
  volume = {10068},
  pages = {81--92},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-50935-8_8},
  abstract = {The puzzle game 2048, a single-player stochastic game played on a 4 \texttimes{} 4 grid, is the most popular among similar slide-and-merge games. One of the strongest computer players for 2048 uses temporal difference learning (TD learning) with N -tuple networks, and it matters a great deal how to design N -tuple networks. In this paper, we study the N -tuple networks for the game 2048. In the first set of experiments, we conduct TD learning by selecting 6- and 7-tuples exhaustively, and evaluate the usefulness of those tuples. In the second set of experiments, we conduct TD learning with high-utility tuples, varying the number of tuples. The best player with ten 7-tuples achieves an average score 234,136 and the maximum score 504,660. It is worth noting that this player utilize no game-tree search and plays a move in about 12 \textmu{}s.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2016\\12 Systematic Selection of N-Tuple Networks for 2048.pdf},
  isbn = {978-3-319-50934-1 978-3-319-50935-8},
  language = {en}
}

@article{ontanon_-line_2010,
  title = {{{ON}}-{{LINE CASE}}-{{BASED PLANNING}}},
  author = {Onta{\~n}{\'o}n, Santi and Mishra, Kinshuk and Sugandh, Neha and Ram, Ashwin},
  year = {2010},
  month = feb,
  volume = {26},
  pages = {84--119},
  issn = {08247935, 14678640},
  doi = {10.1111/j.1467-8640.2009.00344.x},
  abstract = {Some domains, such as real-time strategy (RTS) games, pose several challenges to traditional planning and machine learning techniques. In this article, we present a novel on-line case-based planning architecture that addresses
some of these problems. Our architecture addresses issues of plan acquisition, on-line plan execution, interleaved
planning and execution, and on-line plan adaptation. We also introduce the Darmok system, which implements
this architecture to play WARGUS (an open source clone of the well-known RTS game WARCRAFT II). We present
empirical evaluation of the performance of Darmok and show that it successfully learns to play the WARGUS game.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\99JGDDMS\\Ontañón et al. - 2010 - ON-LINE CASE-BASED PLANNING.pdf},
  journal = {Computational Intelligence},
  language = {en},
  number = {1}
}

@inproceedings{ontanon_adversarial_2015,
  title = {Adversarial {{Hierarchical}}-{{Task Network Planning}} for {{Complex Real}}-{{Time Games}}},
  booktitle = {Proceedings of the {{Twenty}}-{{Fourth International Joint Conference}} on {{Artificial Intelligence}} ({{IJCAI}} 2015)},
  author = {Onta{\~n}{\'o}n, Santiago and Buro, Michael},
  year = {2015},
  pages = {1652--1658},
  abstract = {Real-time strategy (RTS) games are hard from an
AI point of view because they have enormous state
spaces, combinatorial branching factors, allow simultaneous
and durative actions, and players have
very little time to choose actions. For these reasons,
standard game tree search methods such as alphabeta
search or Monte Carlo Tree Search (MCTS)
are not sufficient by themselves to handle these
games. This paper presents an alternative approach
called Adversarial Hierarchical Task Network
(AHTN) planning that combines ideas from
game tree search with HTN planning. We present
the basic algorithm, relate it to existing adversarial
hierarchical planning methods, and present new
extensions for simultaneous and durative actions to
handle RTS games. We also present empirical results
for the  RTS game, comparing it to other state
of the art search algorithms for RTS games.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\V8QDI5UC\\Ontañón and Buro - 2015 - Adversarial Hierarchical-Task Network Planning for.pdf}
}

@inproceedings{ontanon_combinatorial_2013,
  title = {The {{Combinatorial Multi}}-{{Armed Bandit Problem}} and {{Its Application}} to {{Real}}-{{Time Strategy Games}}},
  booktitle = {Proceedings of the {{Ninth AAAI Conference}} on {{Artificial Intelligence}} and {{Interactive Digital Entertainment}}},
  author = {Onta{\~n}{\'o}n, Santiago},
  year = {2013},
  pages = {58--64},
  abstract = {Game tree search in games with large branching factors is a notoriously hard problem. In this paper, we address this problem with a new sampling strategy for Monte Carlo Tree Search (MCTS) algorithms, called Na\textasciidieresis\i{}ve Sampling, based on a variant of the Multi-armed Bandit problem called the Combinatorial Multi-armed Bandit (CMAB) problem. We present a new MCTS algorithm based on Na\textasciidieresis\i{}ve Sampling called Na\textasciidieresis\i{}veMCTS, and evaluate it in the context of real-time strategy (RTS) games. Our results show that as the branching factor grows, Na\textasciidieresis\i{}veMCTS performs significantly better than other algorithms.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\23JZDW6I\\Ontanon - 2013 - The Combinatorial Multi-Armed Bandit Problem and I.pdf},
  language = {en}
}

@article{ontanon_combinatorial_2017,
  title = {Combinatorial {{Multi}}-Armed {{Bandits}} for {{Real}}-{{Time Strategy Games}}},
  author = {Onta{\~n}{\'o}n, Santiago},
  year = {2017},
  month = mar,
  volume = {58},
  pages = {665--702},
  issn = {1076-9757},
  doi = {10.1613/jair.5398},
  abstract = {Games with large branching factors pose a significant challenge for game tree search algorithms. In this paper, we address this problem with a sampling strategy for Monte Carlo Tree Search (MCTS) algorithms called na\textasciidieresis\i{}ve sampling, based on a variant of the Multiarmed Bandit problem called Combinatorial Multi-armed Bandits (CMAB). We analyze the theoretical properties of several variants of na\textasciidieresis\i{}ve sampling, and empirically compare it against the other existing strategies in the literature for CMABs. We then evaluate these strategies in the context of real-time strategy (RTS) games, a genre of computer games characterized by their very large branching factors. Our results show that as the branching factor grows, na\textasciidieresis\i{}ve sampling outperforms the other sampling strategies.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\WAJ56YTP\\Ontañón - 2017 - Combinatorial Multi-armed Bandits for Real-Time St.pdf},
  journal = {Journal of Artificial Intelligence Research},
  language = {en}
}

@article{ontanon_experiments_2012,
  title = {Experiments with {{Game Tree Search}} in {{Real}}-{{Time Strategy Games}}},
  author = {Onta{\~n}{\'o}n, Santiago},
  year = {2012},
  month = aug,
  abstract = {Game tree search algorithms such as minimax have been used with enormous success in turn-based adversarial games such as Chess or Checkers. However, such algorithms cannot be directly applied to real-time strategy (RTS) games because a number of reasons. For example, minimax assumes a turn-taking game mechanics, not present in RTS games. In this paper we present RTMM, a real-time variant of the standard minimax algorithm, and discuss its applicability in the context of RTS games. We discuss its strengths and weaknesses, and evaluate it in two real-time games.},
  archivePrefix = {arXiv},
  eprint = {1208.1940},
  eprinttype = {arxiv},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\6DC2BQN2\\Ontanon - 2012 - Experiments with Game Tree Search in Real-Time Str.pdf},
  journal = {arXiv:1208.1940 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Science and Game Theory},
  language = {en},
  primaryClass = {cs}
}

@inproceedings{ontanon_experiments_2016,
  title = {Experiments on {{Learning Action Probability Models}} from {{Replay Data}} in {{RTS Games}}},
  booktitle = {Artificial {{Intelligence}} in {{Adversarial Games}}: {{Papers}} from the {{AIIDE Workshop AAAI Technical Report WS}}-16-21},
  author = {Onta{\~n}{\'o}n, Santiago},
  year = {2016},
  abstract = {Recent work has shown that incorporating action probability models (models that given a game state can predict the probability with which an expert will play each move) into MCTS can lead to significant performance improvements in a variety of adversarial games, including RTS games. This paper presents a collection of experiments aimed at understanding the relation between the amount of training data, the predictive performance of the action models, the effect of these models in the branching factor of the game and the resulting performance gains in MCTS. Experiments are carried out in the context of the  RTS simulator, showing that more accurate predictive models do not necessarily result in better MCTS performance.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\ST2Z5Z7F\\Ontañón - 2016 - Experiments on Learning Action Probability Models .pdf}
}

@article{ontanon_first_2018,
  title = {The {{First microRTS Artificial Intelligence Competition}}},
  author = {Onta{\~n}{\'o}n, Santiago and Barriga, Nicolas A. and Silva, Cleyton R. and Moraes, Rubens O. and Lelis, Levi H. S.},
  year = {2018},
  month = mar,
  volume = {39},
  pages = {75--83},
  issn = {2371-9621},
  doi = {10.1609/aimag.v39i1.2777},
  copyright = {Copyright (c) 2018 AI Magazine},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\UUKXREHW\\Ontañón et al. - 2018 - The First microRTS Artificial Intelligence Competi.pdf},
  journal = {AI Magazine},
  language = {en},
  number = {1}
}

@inproceedings{ontanon_informed_2016,
  title = {Informed {{Monte Carlo Tree Search}} for {{Real}}-{{Time Strategy}} Games},
  booktitle = {2016 {{IEEE Conference}} on {{Computational Intelligence}} and {{Games}} ({{CIG}})},
  author = {Onta{\~n}{\'o}n, Santiago},
  year = {2016},
  month = sep,
  publisher = {{IEEE}},
  address = {{Santorini, Greece}},
  doi = {10.1109/CIG.2016.7860394},
  abstract = {The recent success of AlphaGO has shown that it is possible to combine machine learning with Monte Carlo Tree Search (MCTS) in order to improve performance in games with large branching factors. This paper explores the question of whether similar ideas can be applied to a genre of games with an even larger branching factor: Real-Time Strategy games. Specifically, this paper studies (1) the use of Bayesian models to estimate the probability distribution of actions played by a strong player, (2) the incorporation of such models into NaiveMCTS, a MCTS algorithm designed for games with combinatorial branching factors. We call this approach informed MCTS, since it exploits prior information about the game in the form of a probability distribution of actions. We evaluate its performance in the \textmu{}RTS game simulator, significantly outperforming the previous state of the art.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\PB764A8D\\Ontanon - 2016 - Informed Monte Carlo Tree Search for Real-Time Str.pdf},
  isbn = {978-1-5090-1883-3},
  language = {en}
}

@article{ontanon_overview_2020,
  title = {An {{Overview}} of {{Distance}} and {{Similarity Functions}} for {{Structured Data}}},
  author = {Onta{\~n}{\'o}n, Santiago},
  year = {2020},
  month = feb,
  abstract = {The notions of distance and similarity play a key role in many machine learning approaches, and artificial intelligence (AI) in general, since they can serve as an organizing principle by which individuals classify objects, form concepts and make generalizations. While distance functions for propositional representations have been thoroughly studied, work on distance functions for structured representations, such as graphs, frames or logical clauses, has been carried out in different communities and is much less understood. Specifically, a significant amount of work that requires the use of a distance or similarity function for structured representations of data usually employs ad-hoc functions for specific applications. Therefore, the goal of this paper is to provide an overview of this work to identify connections between the work carried out in different areas and point out directions for future work.},
  archivePrefix = {arXiv},
  eprint = {2002.07420},
  eprinttype = {arxiv},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\965HTNEH\\Ontañón - 2020 - An Overview of Distance and Similarity Functions f.pdf},
  journal = {arXiv:2002.07420 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  language = {en},
  primaryClass = {cs}
}

@incollection{ontanon_rts_2015,
  title = {{{RTS AI Problems}} and {{Techniques}}},
  booktitle = {Encyclopedia of {{Computer Graphics}} and {{Games}}},
  author = {Onta{\~n}{\'o}n, Santiago and Synnaeve, Gabriel and Uriarte, Alberto and Richoux, Florian and Churchill, David and Preuss, Mike},
  editor = {Lee, Newton},
  year = {2015},
  pages = {1--12},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-08234-9_17-1},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\ERZ2K78L\\Ontañón et al. - 2015 - RTS AI Problems and Techniques 2.pdf},
  isbn = {978-3-319-08234-9},
  language = {en}
}

@article{ontanon_survey_2013,
  title = {A {{Survey}} of {{Real}}-{{Time Strategy Game AI Research}} and {{Competition}} in {{StarCraft}}},
  author = {Onta{\~n}{\'o}n, Santiago and Synnaeve, Gabriel and Uriarte, Alberto and Richoux, Florian and Churchill, David and Preuss, Mike},
  year = {2013},
  month = dec,
  volume = {5},
  pages = {293--311},
  issn = {1943-068X, 1943-0698},
  doi = {10.1109/TCIAIG.2013.2286295},
  abstract = {This paper presents an overview of the existing work on AI for real-time strategy (RTS) games. Specifically, we focus on the work around the game StarCraft, which has emerged in the past few years as the unified test bed for this research. We describe the specific AI challenges posed by RTS games, and overview the solutions that have been explored to address them. Additionally, we also present a summary of the results of the recent StarCraft AI competitions, describing the architectures used by the participants. Finally, we conclude with a discussion emphasizing which problems in the context of RTS game AI have been solved, and which remain open.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\6HGFEAB9\\Ontanon et al. - 2013 - A Survey of Real-Time Strategy Game AI Research an.pdf},
  journal = {IEEE Transactions on Computational Intelligence and AI in Games},
  language = {en},
  number = {4}
}

@article{oord_representation_2019,
  title = {Representation {{Learning}} with {{Contrastive Predictive Coding}}},
  author = {van den Oord, Aaron and Li, Yazhe and Vinyals, Oriol},
  year = {2019},
  month = jan,
  abstract = {While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.},
  archivePrefix = {arXiv},
  eprint = {1807.03748},
  eprinttype = {arxiv},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\8HG8VNMS\\Oord et al. - 2019 - Representation Learning with Contrastive Predictiv.pdf;C\:\\Users\\aesou\\Zotero\\storage\\88QWZ2YM\\1807.html},
  journal = {arXiv:1807.03748 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@inproceedings{osogami_real-time_2019,
  title = {Real-Time Tree Search with Pessimistic Scenarios: {{Winning}} the {{NeurIPS}} 2018 {{Pommerman Competition}}},
  shorttitle = {Real-Time Tree Search with Pessimistic Scenarios},
  booktitle = {Asian {{Conference}} on {{Machine Learning}}},
  author = {Osogami, Takayuki and Takahashi, Toshihiro},
  year = {2019},
  month = oct,
  pages = {583--598},
  abstract = {Autonomous agents need to make decisions in a sequential manner, under partially observable environment, and in consideration of how other agents behave. In critical situations, such decisions need...},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Osogami and Takahashi - 2019 - Real-time tree search with pessimistic scenarios .pdf;C\:\\Users\\aesou\\Zotero\\storage\\DWHN6F22\\Osogami and Takahashi - 2019 - Real-time tree search with pessimistic scenarios .pdf;C\:\\Users\\aesou\\Zotero\\storage\\TN2XVLUY\\osogami19a.html},
  language = {en}
}

@inproceedings{ouessai_online_2019,
  title = {Online {{Adversarial Planning}} in {{$\mu$RTS}} : {{A Survey}}},
  shorttitle = {Online {{Adversarial Planning}} in {{$\mu$RTS}}},
  booktitle = {2019 {{International Conference}} on {{Theoretical}} and {{Applicative Aspects}} of {{Computer Science}} ({{ICTAACS}})},
  author = {Ouessai, Abdessamed and Salem, Mohammed and Mora, Antonio M.},
  year = {2019},
  month = dec,
  volume = {1},
  issn = {null},
  doi = {10.1109/ICTAACS48474.2019.8988124},
  abstract = {Online planning is an important research area focusing on the problem of real-time decision making, using information extracted from the environment. The aim is to compute, at each decision point, the best decision possible that contributes to the realization of a fixed objective. Relevant application domains include robotics, control engineering and computer games. Real-time strategy (RTS) games pose considerable challenges to artificial intelligence techniques, due to their dynamic, complex and adversarial aspects, where online planning plays a prominent role. They also constitute an ideal research platform and test-bed for online planning. {$\mu$}RTS is an open-source AI research platform that features a minimalistic, yet complete RTS implementation, used by AI researchers for developing and testing intelligent RTS game-playing agents. The unique characteristics of {$\mu$}RTS helped for the emergence of interesting online adversarial planning techniques, dealing with multiple levels of abstraction. This paper presents the major {$\mu$}RTS online planning approaches to date, categorized by the degree of abstraction, in fully and partially observable environments.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Ouessai et al. - 2019 - Online Adversarial Planning in μRTS  A Survey.pdf;C\:\\Users\\aesou\\Zotero\\storage\\K5V9HZGP\\8988124.html},
  keywords = {adversarial planning,game ai,game-tree search,Real-time strategy}
}

@phdthesis{paech_one_2018,
  title = {One {{Intelligent Agent}} to {{Rule Them All}}},
  author = {Paech, Laurin},
  year = {2018},
  month = aug,
  abstract = {In this work, we explore the possibility of training agents which are able to show
intelligent behaviour in many different scenarios. We present the effectiveness
of different machine learning algorithms in the StarCraft II Learning Environment[1] and their performance in different scenarios and compare them. In the
end, we recreate DeepMind's FullyConv Agent with slightly better results.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\9X3MIYKY\\Paech - 2018 - One Intelligent Agent to Rule Them All.pdf},
  language = {en},
  school = {ETH Zurich},
  type = {Bachelor}
}

@inproceedings{palencia_psyrts_2019,
  title = {{{PsyRTS}}: A {{Web Platform}} for {{Experiments}} in {{Human Decision}}-{{Making}} in {{RTS Environments}}},
  booktitle = {{{IEEEE Conference}} on {{Games}} 2019},
  author = {Palencia, Denis Omar Verduga and Osman, Magda},
  year = {2019},
  pages = {4},
  abstract = {This paper presents PsyRTS: an open-source web-platform designed to create psychological experiments using a dynamic environment based on real-time strategy games. This platform has characteristics present in Real-Time Strategy (RTS) games and allows the researcher to manipulate variables regarding visibility, resource availability and presence of other agents while at the same time enabling human participation through existing online platforms.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Palencia and Osman - 2019 - PsyRTS a Web Platform for Experiments in Human De.pdf},
  language = {en}
}

@inproceedings{pawlewicz_improving_2007,
  title = {Improving {{Depth}}-{{First PN}}-{{Search}}: 1\,+\,{$\epsilon$} {{Trick}}},
  shorttitle = {Improving {{Depth}}-{{First PN}}-{{Search}}},
  booktitle = {Computers and {{Games}}},
  author = {Pawlewicz, Jakub and Lew, {\L}ukasz},
  editor = {{van den Herik}, H. Jaap and Ciancarini, Paolo and Donkers, H. H. L. M. (Jeroen)},
  year = {2007},
  pages = {160--171},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-75538-8_14},
  abstract = {Various efficient game problem solvers are based on PN-Search. Especially depth-first versions of PN-Search like DF-PN or PDS \textendash{} contrary to other known techniques \textendash{} are able to solve really hard problems. However, the performance of DF-PN and PDS decreases drastically when the search space significantly exceeds the available memory. A straightforward enhancement trick to overcome this problem is presented. Experiments on Atari Go and Lines of Action show great practical value of the proposed enhancement.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2006\\Pawlewicz and Lew - 2007 - Improving Depth-First PN-Search 1 + ε Trick.pdf;C\:\\Users\\aesou\\Zotero\\storage\\TSI3JTFY\\Pawlewicz and Lew - 2007 - Improving Depth-First PN-Search 1 + ε Trick.pdf},
  isbn = {978-3-540-75538-8},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@incollection{pawlewicz_nearly_2011,
  title = {Nearly {{Optimal Computer Play}} in {{Multi}}-Player {{Yahtzee}}},
  booktitle = {Computers and {{Games}}},
  author = {Pawlewicz, Jakub},
  year = {2011},
  volume = {6515},
  pages = {250--262},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-17928-0_23},
  abstract = {Yahtzee is the most popular commercial dice game in the world. It can be played either by one or many players. In case of the single-player version, optimal computer strategies both for maximizing the expected average score and for maximizing the probability of beating a particular score are already known. However, when it comes to the multi-player version, those approaches are far too resource intensive and thus are not able to develop an optimal strategy given the current hardware.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2010\\28 Nearly Optimal Computer Play in Multi-player Yahtzee.pdf},
  isbn = {978-3-642-17927-3 978-3-642-17928-0},
  language = {en}
}

@incollection{pawlewicz_scalable_2014,
  title = {Scalable {{Parallel DFPN Search}}},
  booktitle = {Computers and {{Games}}},
  author = {Pawlewicz, Jakub and Hayward, Ryan B.},
  year = {2014},
  volume = {8427},
  pages = {138--150},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-09165-5_12},
  abstract = {We present Scalable Parallel Depth-First Proof Number Search, a new shared-memory parallel version of depth-first proof number search. Based on the serial DFPN 1 + {$\gamma$} method of Pawlewicz and Lew, SPDFPN searches effiectively even as the transposition table becomes almost full, and so can solve large problems. To assign jobs to threads, SPDFPN uses proof and disproof numbers and two parameters. SPDFPN uses no domain-specific knowledge or heuristics, so it can be used in any domain. Our experiments show that SPDFPN scales well and performs well on hard problems.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2013\\16 Scalable Parallel DFPN Search.pdf},
  isbn = {978-3-319-09164-8 978-3-319-09165-5},
  language = {en}
}

@inproceedings{pelcner_real-time_2020,
  title = {Real-Time {{Learning}} and {{Planning}} in {{Environments}} with {{Swarms}}:{{A Hierarchical}} and a {{Parameter}}-Based {{Simulation Approach}}},
  shorttitle = {Real-Time {{Learning}} and {{Planning}} in {{Environments}} with {{Swarms}}},
  booktitle = {Proceedings of the 19th {{International Conference}} on {{Autonomous Agents}} and {{Multiagent Systems}} ({{AAMAS}} 2020)},
  author = {Pelcner, Lukasz and Li, Shaling and Do Carmo Alves, Matheus and Soriano Marcolino, Leandro and Collins, Alex},
  year = {2020},
  month = jan,
  abstract = {Swarms can be applied in many relevant domains, such as patrolling or rescue. They usually follow simple local rules, leading to complex emergent behavior. Given their wide applicability, an agent may need to take decisions in an environment containing a swarm that is not under its control, and that may even be an antagonist. Predicting the behavior of each swarm member is a great challenge, and must be done under real time constraints, since they usually move constantly following quick reactive algorithms. We propose the first two solutions for this novel problem, showing integrated on-line learning and planning for decision-making with unknown swarms: (i) we learn an ellipse abstraction of the swarm based on statistical models, and predict its future parameters using time-series; (ii) we learn algorithm parameters followed by each swarm member, in order to directly simulate them. We find in our experiments that we are significantly faster to reach an objective than local repulsive forces, at the cost of success rate in some situations. Additionally, we show that this is a challenging problem for reinforcement learning.},
  copyright = {creative\_commons\_attribution\_noncommercial\_4\_0\_international\_license},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\6WLKD7NA\\Pelcner et al. - 2020 - Real-time Learning and Planning in Environments wi.pdf;C\:\\Users\\aesou\\Zotero\\storage\\544FLDCV\\141849.html},
  language = {en}
}

@article{peng_multiagent_2017,
  title = {Multiagent {{Bidirectionally}}-{{Coordinated Nets}}: {{Emergence}} of {{Human}}-Level {{Coordination}} in {{Learning}} to {{Play StarCraft Combat Games}}},
  shorttitle = {Multiagent {{Bidirectionally}}-{{Coordinated Nets}}},
  author = {Peng, Peng and Wen, Ying and Yang, Yaodong and Yuan, Quan and Tang, Zhenkun and Long, Haitao and Wang, Jun},
  year = {2017},
  month = mar,
  abstract = {Real-world artificial intelligence (AI) applications often require multiple agents to work in a collaborative effort. Efficient learning for intra-agent communication and coordination is an indispensable step towards general AI. In this paper, we take StarCraft combat game as the test scenario, where the task is to coordinate multiple agents as a team to defeat their enemies. To maintain a scalable yet effective communication protocol, we introduce a multiagent bidirectionally-coordinated network (BiCNet ['bIknet]) with a vectorised extension of actor-critic formulation. We show that BiCNet can handle different types of combats under diverse terrains with arbitrary numbers of AI agents for both sides. Our analysis demonstrates that without any supervisions such as human demonstrations or labelled data, BiCNet could learn various types of coordination strategies that is similar to these of experienced game players. Moreover, BiCNet is easily adaptable to the tasks with heterogeneous agents. In our experiments, we evaluate our approach against multiple baselines under different scenarios; it shows state-of-the-art performance, and possesses potential values for large-scale real-world applications.},
  archivePrefix = {arXiv},
  eprint = {1703.10069},
  eprinttype = {arxiv},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\A4PTESMX\\Peng et al. - 2017 - Multiagent Bidirectionally-Coordinated Nets Emerg.pdf},
  journal = {arXiv:1703.10069 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  language = {en},
  primaryClass = {cs}
}

@article{pepels_real-time_2014,
  title = {Real-{{Time Monte Carlo Tree Search}} in {{Ms Pac}}-{{Man}}},
  author = {Pepels, Tom and Winands, Mark H. M. and Lanctot, Marc},
  year = {2014},
  month = sep,
  volume = {6},
  pages = {245--257},
  issn = {1943-0698},
  doi = {10.1109/TCIAIG.2013.2291577},
  abstract = {In this paper, Monte Carlo tree search (MCTS) is introduced for controlling the Pac-Man character in the real-time game Ms Pac-Man. MCTS is used to find an optimal path for an agent at each turn, determining the move to make based on the results of numerous randomized simulations. Several enhancements are introduced in order to adapt MCTS to the real-time domain. Ms Pac-Man is an arcade game, in which the protagonist has several goals but no conclusive terminal state. Unlike games such as Chess or Go there is no state in which the player wins the game. Instead, the game has two subgoals, 1) surviving and 2) scoring as many points as possible. Decisions must be made in a strict time constraint of 40 ms. The Pac-Man agent has to compete with a range of different ghost teams, hence limited assumptions can be made about their behavior. In order to expand the capabilities of existing MCTS agents, four enhancements are discussed: 1) a variable-depth tree; 2) simulation strategies for the ghost team and Pac-Man; 3) including long-term goals in scoring; and 4) reusing the search tree for several moves with a decay factor {$\gamma$}. The agent described in this paper was entered in both the 2012 World Congress on Computational Intelligence (WCCI'12, Brisbane, Qld., Australia) and the 2012 IEEE Conference on Computational Intelligence and Games (CIG'12, Granada, Spain) Pac-Man Versus Ghost Team competitions, where it achieved second and first places, respectively. In the experiments, we show that using MCTS is a viable technique for the Pac-Man agent. Moreover, the enhancements improve overall performance against four different ghost teams.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Pepels et al. - 2014 - Real-Time Monte Carlo Tree Search in Ms Pac-Man.pdf;C\:\\Users\\aesou\\Zotero\\storage\\935I3D7D\\Pepels et al. - 2014 - Real-Time Monte Carlo Tree Search in Ms Pac-Man.pdf;C\:\\Users\\aesou\\Zotero\\storage\\4QGR2AFZ\\6731713.html},
  journal = {IEEE Transactions on Computational Intelligence and AI in Games},
  keywords = {2012 IEEE Conference on Computational Intelligence and Games,2012 World Congress on Computational Intelligence,arcade game,Artificial intelligence,Computational intelligence,computer games,digital simulation,Games,Junctions,MCTS,Monte Carlo,Monte Carlo methods,Monte Carlo tree search (MCTS),Ms Pac-Man,Pac-Man,Pac-Man agent,Pac-Man-Ghost Team competitions,Planning,real time,real-time Monte Carlo tree search,Real-time systems,scoring,simulation strategies,surviving,time 40 ms,tree searching,variable-depth tree},
  number = {3}
}

@inproceedings{perez_rolling_2013-1,
  title = {Rolling Horizon Evolution versus Tree Search for Navigation in Single-Player Real-Time Games},
  booktitle = {Proceeding of the Fifteenth Annual Conference on {{Genetic}} and Evolutionary Computation Conference - {{GECCO}} '13},
  author = {Perez, Diego and Samothrakis, Spyridon and Lucas, Simon and Rohlfshagen, Philipp},
  year = {2013},
  pages = {351},
  publisher = {{ACM Press}},
  address = {{Amsterdam, The Netherlands}},
  doi = {10.1145/2463372.2463413},
  abstract = {In real-time games, agents have limited time to respond to environmental cues. This requires either a policy defined up-front or, if one has access to a generative model, a very efficient rolling horizon search. In this paper, different search techniques are compared in a simple, yet interesting, realtime game known as the Physical Travelling Salesman Problem (PTSP). We introduce a rolling horizon version of a simple evolutionary algorithm that handles macro-actions and compare it against Monte Carlo Tree Search (MCTS), an approach known to perform well in practice, as well as random search. The experimental setup employs a variety of settings for both the action space of the agent as well as the algorithms used. We show that MCTS is able to handle very fine-grained searches whereas evolution performs better as we move to coarser-grained actions; the choice of algorithm becomes irrelevant if the actions are even more coarse-grained. We conclude that evolutionary algorithms can be a viable and competitive alternative to MCTS.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Perez et al. - 2013 - Rolling horizon evolution versus tree search for n.pdf},
  isbn = {978-1-4503-1963-8},
  language = {en}
}

@book{perron_video_2009,
  title = {The Video Game Theory Reader 2},
  editor = {Perron, Bernard and Wolf, Mark J. P.},
  year = {2009},
  publisher = {{Routledge}},
  address = {{New York}},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\36H7TPMQ\\Perron and Wolf - 2009 - The video game theory reader 2.pdf},
  isbn = {978-0-415-96282-7 978-0-415-96283-4 978-0-203-88766-0},
  keywords = {Video games},
  language = {en},
  lccn = {GV1469.3 .V574 2009}
}

@incollection{plaat_using_2016,
  title = {Using {{Partial Tablebases}} in {{Breakthrough}}},
  booktitle = {Computers and {{Games}}},
  author = {Isaac, Andrew and Lorentz, Richard},
  editor = {Plaat, Aske and Kosters, Walter and {van den Herik}, Jaap},
  year = {2016},
  volume = {10068},
  pages = {1--10},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-50935-8_1},
  abstract = {In the game of Breakthrough the endgame is reached when there are still many pieces on the board. This means there are too many possible positions to be able to construct a reasonable endgame tablebase on the standard 8 \texttimes{} 8 board, or even on a 6 \texttimes{} 6 board. The fact that Breakthrough pieces only move forward allows us to create partial tablebases on the last n rows of each side of the board. We show how doing this enables us to create a much stronger MCTS based 6 \texttimes{} 6 player and allows us to solve positions that would otherwise be out of reach.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2016\\5 Using Partial Tablebases in Breakthrough.pdf},
  isbn = {978-3-319-50934-1 978-3-319-50935-8},
  language = {en}
}

@inproceedings{ponsen_automatically_2005,
  title = {Automatically {{Acquiring Domain Knowledge For Adaptive Game AI Using Evolutionary Learning}}},
  booktitle = {{{AAAI}}},
  author = {Ponsen, Marc J. V. and {Mu{\~n}oz-Avila}, Hector and Spronck, Pieter and Aha, David W.},
  year = {2005},
  abstract = {Game AI is the decision-making process of computer-controlled opponents in computer games. Adaptive game AI can improve the entertainment value of computer games. It allows computercontrolled opponents to automatically fix weaknesses in the game AI and respond to changes in human-player tactics. Dynamic scripting is a recently developed approach for adaptive game AI that learns which tactics (i.e., action sequences) an opponent should select to play effectively against the human player. In previous work, these tactics were manually generated. We introduce AKADS; it uses an evolutionary algorithm to automatically generate such tactics. Our experiments show that it improves dynamic scripting's performance on a real-time strategy (RTS) game. Therefore, we conclude that high-quality domain knowledge (i.e., tactics) can be automatically generated for strong adaptive AI opponents in RTS games. This reduces the time and effort required by game developers to create intelligent game AI, thus freeing them to focus on other important topics (e.g., storytelling, graphics).},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\9CG84UEM\\Ponsen et al. - 2005 - Automatically Acquiring Domain Knowledge For Adapt.pdf},
  keywords = {Artificial intelligence (video games),Evolutionary algorithm,Experiment,Graphics,PC game,Real-time clock}
}

@article{preuss_conference_2015,
  title = {Conference {{Report}} on {{IEEE CIG}} 2014 [{{Conference Reports}}]},
  author = {Preuss, Mike and Rudolph, Gunter},
  year = {2015},
  month = feb,
  volume = {10},
  pages = {14--15},
  issn = {1556-603X},
  doi = {10.1109/MCI.2014.2369880},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\NPPIDPTD\\Preuss and Rudolph - 2015 - Conference Report on IEEE CIG 2014 [Conference Rep.pdf},
  journal = {IEEE Computational Intelligence Magazine},
  language = {en},
  number = {1}
}

@article{preuss_games_2020,
  title = {A {{Games Industry Perspective}} on {{Recent Game AI Developments}}},
  author = {Preuss, Mike and Risi, Sebastian},
  year = {2020},
  month = feb,
  issn = {1610-1987},
  doi = {10.1007/s13218-020-00643-0},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Preuss and Risi - 2020 - A Games Industry Perspective on Recent Game AI Dev.pdf},
  journal = {KI - K\"unstliche Intelligenz},
  language = {en}
}

@article{preuss_intelligent_2008,
  title = {Intelligent Group Movement and Selection in Realtime Strategy Games},
  author = {Preuss, Mike and Beume, Nicola and Danielsiek, Holger and Hein, Tobias and Naujoks, Boris and Piatkowski, Nico and St{\"u}er, Raphael and Thom, Andreas and Wessing, Simon},
  year = {2008},
  month = dec,
  doi = {http://dx.doi.org/10.17877/DE290R-695},
  abstract = {Movement of groups in realtime strategy games is often a nuisance: Units travel and battle separately, resulting in huge losses and the AI looking dumb. This applies to computer as well as human commanded factions. We suggest to tackle that by using flocking improved by influence-map based pathfinding which leads to a much more natural and intelligent looking behavior. A similar problem occurs if the computer AI has to select groups to combat a specific target: Assignment of units to groups, especially for multiple enemy groups, is often suboptimal when units have very different attack skills. This can be cured by using offline prepared self-organizing feature maps that use all available information for looking up good matches. We demonstrate that these two approaches work well separately, but also that they go together very naturally, thereby leading to an improved and - via parametrization - very flexible group behavior. Opponent AI may be strenghtened that way as well as player-supportive AI. A thorough experimental analysis supports our claims.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\M58ANARH\\Preuss et al. - 2008 - Intelligent group movement and selection in realti.pdf;C\:\\Users\\aesou\\Zotero\\storage\\RXXIPSHW\\26162.html},
  language = {en}
}

@inproceedings{preuss_reactive_2013,
  title = {Reactive Strategy Choice in {{StarCraft}} by Means of {{Fuzzy Control}}},
  booktitle = {2013 {{IEEE Conference}} on {{Computational Inteligence}} in {{Games}} ({{CIG}})},
  author = {Preuss, Mike and Kozakowski, Daniel and Hagelback, Johan and Trautmann, Heike},
  year = {2013},
  month = aug,
  pages = {1--8},
  publisher = {{IEEE}},
  address = {{Niagara Falls, ON, Canada}},
  doi = {10.1109/CIG.2013.6633627},
  abstract = {Current StarCraft bots are not very flexible in their strategy choice, most of them just follow a manually optimized one, usually a rush. We suggest a method of augmenting existing bots via Fuzzy Control in order to make them react on the current game situation. According to the available information, the best matching of a pool of strategies is chosen. While the method is very general and can be applied easily to many bots, we implement it for the existing BTHAI bot and show experimentally how the modifications affects its gameplay, and how it is improved compared to the original version.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Preuss et al. - 2013 - Reactive strategy choice in StarCraft by means of .pdf},
  isbn = {978-1-4673-5311-3 978-1-4673-5308-3},
  language = {en}
}

@book{quiggin_generalized_1993,
  title = {Generalized {{Expected Utility Theory}}: {{The Rank}}-{{Dependent Model}}},
  shorttitle = {Generalized {{Expected Utility Theory}}},
  author = {Quiggin, John},
  year = {1993},
  publisher = {{Springer Science \& Business Media}},
  abstract = {Economic analysis of choice under uncertainty has been dominated by the expected utility (EU) model, yet the EU model has never been without critics. Psychologists accumulated evidence that individual choices under uncertainty were inconsistent with the predictions of the EU model. Applied work in areas such as finance was dominated by the simpler mean-variance analysis. In the 1980s this skepticism was dispelled as a number of generalizations of EU were proposed, most of which were capable of explaining evidence inconsistent with EU, while preserving transitivity and dominance.  Generalized expected utility is now a flourishing subfield of economics, with dozens of competing models and considerable literature exploring their theoretical properties and comparing their empirical performance. But the EU model remains the principal tool for the analysis of choice under uncertainty. There is a view that generalized models are too difficult to handle or incapable of generating sharp results. This creates a need to show that the new models can be used in the kinds of economic analysis for which EU has been used, and that they can yield new and interesting results.  This book meets this need by describing one of the most popular generalized models -- the rank-dependent expected utility model (RDEU), also known as anticipated utility, EU with rank-dependent preferences, the dual theory of choice under uncertainty, and simply as rank-dependent utility. As the many names indicate, the model has been approached in many ways by many scientists and for this reason, consideration of a single model sheds light on many of the concerns that have motivated the development of generalized utility models.  The popularity of the RDEU model rests on its simplicity and tractability. The standard tools of analysis developed for EU theory may be applied to the RDEU model, but since RDEU admits behavior inconsistent with EU, the field of potential applications is widened. As such, the RDEU model is not as much a competitor to EU as an extension based on less restrictive assumptions.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Quiggin - 1993 - Generalized Expected Utility Theory The Rank-Depe.pdf},
  googlebooks = {PXrrCAAAQBAJ},
  isbn = {978-94-011-2182-8},
  keywords = {Business \& Economics / Economics / General,Business \& Economics / Economics / Theory,Business \& Economics / Operations Research,Business \& Economics / Statistics,Mathematics / Probability \& Statistics / General},
  language = {en}
}

@book{rabin_ai_2007,
  title = {{{AI}} Game Programming Wisdom},
  author = {Rabin, Steve},
  year = {2007},
  publisher = {{Charles River Media}},
  address = {{Hingham, Mass.}},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\3QVDYSQH\\Rabin - 2007 - AI game programming wisdom.pdf},
  isbn = {978-1-58450-077-3},
  language = {English}
}

@book{rabin_ai_2008,
  title = {{{AI}} Game Programming Wisdom 4},
  editor = {Rabin, Steve},
  year = {2008},
  publisher = {{Course Technology, Cengage Learning}},
  address = {{Australia ; United States}},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\MR84AHZ6\\Rabin - 2008 - AI game programming wisdom 4.pdf},
  isbn = {978-1-58450-523-5},
  keywords = {Artificial intelligence,Computer games,Computer graphics,Design,Programming},
  language = {en},
  lccn = {QA76.76.C672 A4264 2008}
}

@book{rabin_ai_2008-1,
  title = {{{AI}} Game Programming Wisdom 4},
  editor = {Rabin, Steve},
  year = {2008},
  publisher = {{Course Technology, Cengage Learning}},
  address = {{Australia ; United States}},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\RJBQVZ2E\\Rabin - 2008 - AI game programming wisdom 4 2.pdf},
  isbn = {978-1-58450-523-5},
  keywords = {Artificial intelligence,Computer games,Computer graphics,Design,Programming},
  language = {en},
  lccn = {QA76.76.C672 A4264 2008}
}

@book{rabin_game_2014,
  title = {Game {{AI}} pro: Collected Wisdom of Game {{AI}} Professionals},
  shorttitle = {Game {{AI}} Pro},
  author = {Rabin, Steve},
  year = {2014},
  publisher = {{CRC Press}},
  address = {{Boca Raton}},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\CVKUX5RC\\Rabin - 2014 - Game AI pro collected wisdom of game AI professio.pdf},
  isbn = {978-1-4665-6596-8},
  language = {English}
}

@book{rabin_game_2015,
  title = {Game {{AI Pro}}{$^2$} Collected Wisdom of Game {{AI}} Professionals},
  author = {Rabin, Steve},
  year = {2015},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\8EG6VWXT\\Rabin - 2015 - Game AI Pro² collected wisdom of game AI professio.pdf},
  isbn = {978-1-4822-5480-8},
  language = {English}
}

@book{rabin_game_2017,
  title = {Game {{AI}} pro 3: Collected Wisdom of Game {{AI}} Professionals},
  shorttitle = {Game {{AI}} pro 3},
  author = {Rabin, Steve},
  year = {2017},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\XAPIBUKK\\Rabin - 2017 - Game AI pro 3 collected wisdom of game AI profess.pdf},
  isbn = {978-1-4987-4258-0},
  language = {English}
}

@book{rabin_introduction_2010,
  title = {Introduction to Game Development},
  editor = {Rabin, Steve},
  year = {2010},
  edition = {2nd ed},
  publisher = {{Course Technology Cengage Learning}},
  address = {{Boston, MA}},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\Y4YFAVPS\\Rabin - 2010 - Introduction to game development.pdf},
  isbn = {978-1-58450-679-9},
  keywords = {Computer games,Design,Programming,Video games},
  language = {en},
  lccn = {QA76.76.C672 I58 2010},
  series = {Game Development Series}
}

@inproceedings{rao_bdi_1995,
  title = {{{BDI Agents}}: {{From Theory}} to {{Practice}}},
  booktitle = {{{ICMAS}}'95},
  author = {Rao, Anand S and Georgeff, Michael P},
  year = {1995},
  pages = {8},
  abstract = {The study of computational agents capable of rational behaviour has received a great deal of attention in recent years. Theoretical formalizations of such agents and their implementations have proceeded in parallel with little or no connection between them. Tkis paper exploresa particular typeof rational agent,a BeliefDesire-Intention (BDI) agent. The primary aim of this paper is to integrate (a) the theoretical foundations of BDIagents from both a quantitative decision-theoretic perspective anda symbolic reasoning perspective; (b) the implementations of BDIagents from an ideal theoretical perspective anda morepractical perspective; and (c) the building of large-scale applications basedon BDI agents. In particular, an air-trafflc management application will be described from both a theoretical and an implementationperspective.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Rao and Georgeff - 1995 - BDI Agents From Theory to Practice.pdf},
  language = {en}
}

@article{real_automl-zero_2020,
  title = {{{AutoML}}-{{Zero}}: {{Evolving Machine Learning Algorithms From Scratch}}},
  shorttitle = {{{AutoML}}-{{Zero}}},
  author = {Real, Esteban and Liang, Chen and So, David R. and Le, Quoc V.},
  year = {2020},
  month = mar,
  abstract = {Machine learning research has advanced in multiple aspects, including model structures and learning methods. The effort to automate such research, known as AutoML, has also made significant progress. However, this progress has largely focused on the architecture of neural networks, where it has relied on sophisticated expert-designed layers as building blocks---or similarly restrictive search spaces. Our goal is to show that AutoML can go further: it is possible today to automatically discover complete machine learning algorithms just using basic mathematical operations as building blocks. We demonstrate this by introducing a novel framework that significantly reduces human bias through a generic search space. Despite the vastness of this space, evolutionary search can still discover two-layer neural networks trained by backpropagation. These simple neural networks can then be surpassed by evolving directly on tasks of interest, e.g. CIFAR-10 variants, where modern techniques emerge in the top algorithms, such as bilinear interactions, normalized gradients, and weight averaging. Moreover, evolution adapts algorithms to different task types: e.g., dropout-like techniques appear when little data is available. We believe these preliminary successes in discovering machine learning algorithms from scratch indicate a promising new direction for the field.},
  archivePrefix = {arXiv},
  eprint = {2003.03384},
  eprinttype = {arxiv},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\RE8VYNAM\\Real et al. - 2020 - AutoML-Zero Evolving Machine Learning Algorithms .pdf;C\:\\Users\\aesou\\Zotero\\storage\\B7IK9U78\\2003.html},
  journal = {arXiv:2003.03384 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,I.2.2,I.2.6,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{richoux_ghost_2016,
  title = {Ghost: {{A Combinatorial Optimization Framework}} for {{Real}}-{{Time Problems}}},
  shorttitle = {Ghost},
  author = {Richoux, F. and Uriarte, A. and Baffier, J.},
  year = {2016},
  month = dec,
  volume = {8},
  pages = {377--388},
  issn = {1943-068X},
  doi = {10.1109/TCIAIG.2016.2573199},
  abstract = {This paper presents GHOST, a combinatorial optimization framework that a real-time strategy (RTS) AI developer can use to model and solve any problem encoded as a constraint satisfaction/optimization problem (CSP/COP). We show a way to model three different problems as a CSP/COP, using instances from the RTS game StarCraft as test beds. Each problem belongs to a specific level of abstraction (the target selection as reactive control problem, the wall-in as a tactics problem, and the build order planning as a strategy problem). In our experiments, GHOST shows good results computed within some tens of milliseconds. We also show that GHOST outperforms state-of-the-art constraint solvers, matching them on the resources allocation problem, a common combinatorial optimization problem.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Richoux et al. - 2016 - ghost A Combinatorial Optimization Framework for .pdf;C\:\\Users\\aesou\\Zotero\\storage\\5QCT7L6B\\7479528.html},
  journal = {IEEE Transactions on Computational Intelligence and AI in Games},
  keywords = {Artificial intelligence,build order planning,Buildings,combinatorial mathematics,combinatorial optimization framework,Combinatorics,computer games,constraint optimization problem (COP),constraint satisfaction problem (CSP),constraint satisfaction problems,constraint satisfaction/optimization problem,constraint solvers,CSP/COP,game artificial intelligence (AI),Games,GHOST,Mathematical model,optimisation,optimization,Optimization,Programming,reactive control problem,real-time problems,real-time strategy (RTS),real-time strategy AI developer,Real-time systems,resource allocation,resource allocation problem,RTS AI developer,RTS game StarCraft,solver,StarCraft,strategy problem,tactics problem,target selection,wall-in},
  number = {4}
}

@inproceedings{richoux_walling_2014,
  title = {Walling in {{Strategy Games}} via {{Constraint Optimization}}},
  booktitle = {Proceedings of the {{Tenth Annual AAAI Conference}} on {{Artificial Intelligence}} and {{Interactive Digital Entertainment}} ({{AIIDE}})},
  author = {Richoux, Florian and Uriarte, Alberto and Ontanon, Santiago},
  year = {2014},
  pages = {52--58},
  abstract = {This paper presents a constraint optimization approach to walling in real-time strategy (RTS) games. Walling is a specific type of spatial reasoning, typically employed by human expert players and not currently fully exploited in RTS game AI, consisting on finding configurations of buildings to completely or partially block paths. Our approach is based on local search, and is specifically designed for the real-time nature of RTS games. We present experiments in the context of the RTS game StarCraft showing promising results.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\NQUP6LN6\\Richoux et al. - 2014 - Walling in Strategy Games via Constraint Optimizat.pdf},
  language = {en}
}

@incollection{rimmel_biasing_2011,
  title = {Biasing {{Monte}}-{{Carlo Simulations}} through {{RAVE Values}}},
  booktitle = {Computers and {{Games}}},
  author = {Rimmel, Arpad and Teytaud, Fabien and Teytaud, Olivier},
  year = {2011},
  volume = {6515},
  pages = {59--68},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-17928-0_6},
  abstract = {The Monte-Carlo Tree Search algorithm has been successfully applied in various domains. However, its performance heavily depends on the Monte-Carlo part. In this paper, we propose a generic way of improving the Monte-Carlo simulations by using RAVE values, which already strongly improved the tree part of the algorithm. We prove the generality and efficiency of our approach by showing improvements on two different applications: the game of Havannah and the game of Go.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2010\\11 Biasing Monte-Carlo Simulations through RAVE Values.pdf},
  isbn = {978-3-642-17927-3 978-3-642-17928-0},
  language = {en}
}

@article{risi_behind_2020,
  title = {Behind {{DeepMind}}'s {{AlphaStar AI}} That {{Reached Grandmaster Level}} in {{StarCraft II}}},
  author = {Risi, Sebastian and Preuss, Mike},
  year = {2020},
  month = feb,
  issn = {1610-1987},
  doi = {10.1007/s13218-020-00642-1},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Risi and Preuss - 2020 - Behind DeepMind’s AlphaStar AI that Reached Grandm.pdf},
  journal = {KI - K\"unstliche Intelligenz},
  language = {en}
}

@article{risi_chess_2020,
  title = {From {{Chess}} and {{Atari}} to {{StarCraft}} and {{Beyond}}: {{How Game AI}} Is {{Driving}} the {{World}} of {{AI}}},
  shorttitle = {From {{Chess}} and {{Atari}} to {{StarCraft}} and {{Beyond}}},
  author = {Risi, Sebastian and Preuss, Mike},
  year = {2020},
  month = feb,
  issn = {1610-1987},
  doi = {10.1007/s13218-020-00647-w},
  abstract = {This paper reviews the field of Game AI, which not only deals with creating agents that can play a certain game, but also with areas as diverse as creating game content automatically, game analytics, or player modelling. While Game AI was for a long time not very well recognized by the larger scientific community, it has established itself as a research area for developing and testing the most advanced forms of AI algorithms and articles covering advances in mastering video games such as StarCraft 2 and Quake III appear in the most prestigious journals. Because of the growth of the field, a single review cannot cover it completely. Therefore, we put a focus on important recent developments, including that advances in Game AI are starting to be extended to areas outside of games, such as robotics or the synthesis of chemicals. In this article, we review the algorithms and methods that have paved the way for these breakthroughs, report on the other important areas of Game AI research, and also point out exciting directions for the future of Game AI.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Risi and Preuss - 2020 - From Chess and Atari to StarCraft and Beyond How .pdf},
  journal = {KI - K\"unstliche Intelligenz},
  language = {en}
}

@article{risi_neuroevolution_2015,
  title = {Neuroevolution in {{Games}}: {{State}} of the {{Art}} and {{Open Challenges}}},
  shorttitle = {Neuroevolution in {{Games}}},
  author = {Risi, Sebastian and Togelius, Julian},
  year = {2015},
  month = nov,
  abstract = {This paper surveys research on applying neuroevolution (NE) to games. In neuroevolution, artificial neural networks are trained through evolutionary algorithms, taking inspiration from the way biological brains evolved. We analyse the application of NE in games along five different axes, which are the role NE is chosen to play in a game, the different types of neural networks used, the way these networks are evolved, how the fitness is determined and what type of input the network receives. The article also highlights important open research challenges in the field.},
  archivePrefix = {arXiv},
  eprint = {1410.7326},
  eprinttype = {arxiv},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Risi and Togelius - 2015 - Neuroevolution in Games State of the Art and Open.pdf},
  journal = {arXiv:1410.7326 [cs]},
  keywords = {Computer Science - Neural and Evolutionary Computing},
  language = {en},
  primaryClass = {cs}
}

@inproceedings{roberts_proceedings_2019,
  title = {Proceedings of the 3rd {{Workshop}} on {{Integrated Planning}}, {{Acting}}, and {{Execution}} - {{ICAPS}}},
  booktitle = {Proceedings of the 3rd {{Workshop}} on {{Integrated Planning}}, {{Acting}}, and {{Execution}}},
  author = {Roberts, Mark and Vaquero, Tiago and Niemueller, Tim},
  year = {2019},
  month = jul,
  pages = {81},
  abstract = {We describe three approaches to enabling an extremely computationally limited embedded scheduler to consider a small number of alternative activities based on resource availability. We consider the case where the scheduler is so computationally limited that it cannot backtrack search. The first two approaches precompile resource checks (called guards) that only enable selection of a preferred alternative activity if sufficient resources are estimated to be available to schedule the remaining activities. The final approach mimics backtracking by invoking the scheduler multiple times with the alternative activities. We present an evaluation of these techniques on mission scenarios (called sol types) from NASA's next planetary rover where these techniques are being evaluated for inclusion in an onboard scheduler.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Roberts et al. - 2019 - Proceedings of the 3rd Workshop on Integrated Plan.pdf},
  language = {en}
}

@inproceedings{robertson_building_2015,
  title = {Building Behavior Trees from Observations in Real-Time Strategy Games},
  booktitle = {2015 {{International Symposium}} on {{Innovations}} in {{Intelligent SysTems}} and {{Applications}} ({{INISTA}})},
  author = {Robertson, Glen and Watson, Ian},
  year = {2015},
  month = sep,
  pages = {1--7},
  publisher = {{IEEE}},
  address = {{Madrid, Spain}},
  doi = {10.1109/INISTA.2015.7276774},
  abstract = {This paper presents a novel use of motif-finding techniques from computational biology to find recurring action sequences across many observations of expert humans carrying out a complex task. Information about recurring action sequences is used to produce a behavior tree without any additional domain information besides a simple similarity metric \textendash{} no action models or reward functions are provided. This technique is applied to produce a behavior tree for strategic-level actions in the real-time strategy game StarCraft. The behavior tree was able to represent and summarise a large amount of information from the expert behavior examples much more compactly. The method could still be improved by discovering reactive actions present in the expert behavior and encoding these in the behavior tree.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\JABS8EQT\\Robertson and Watson - 2015 - Building behavior trees from observations in real-.pdf},
  isbn = {978-1-4673-7751-5},
  language = {en}
}

@article{robertson_review_2014,
  title = {A {{Review}} of {{Real}}-{{Time Strategy Game AI}}},
  author = {Robertson, Glen and Watson, Ian},
  year = {2014},
  month = dec,
  volume = {35},
  pages = {75--104},
  issn = {0738-4602, 0738-4602},
  doi = {10.1609/aimag.v35i4.2478},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\8ETELNN3\\Robertson and Watson - 2014 - A Review of Real-Time Strategy Game AI.pdf},
  journal = {AI Magazine},
  language = {en},
  number = {4}
}

@inproceedings{rocki_large-scale_2011,
  title = {Large-{{Scale Parallel Monte Carlo Tree Search}} on {{GPU}}},
  booktitle = {2011 {{IEEE International Symposium}} on {{Parallel}} and {{Distributed Processing Workshops}} and {{Phd Forum}}},
  author = {Rocki, Kamil and Suda, Reiji},
  year = {2011},
  month = may,
  pages = {2034--2037},
  publisher = {{IEEE}},
  address = {{Anchorage, AK, USA}},
  doi = {10.1109/IPDPS.2011.370},
  abstract = {Monte Carlo Tree Search (MCTS) is a method for making optimal decisions in artificial intelligence (AI) problems, typically move planning in combinatorial games. It combines the generality of random simulation with the precision of tree search. The motivation behind this work is caused by the emerging GPU-based systems and their high computational potential combined with relatively low power usage compared to CPUs. As a problem to be solved I chose to develop an AI GPU(Graphics Processing Unit)-based agent in the game of Reversi (Othello) which provides a sufficiently complex problem for tree searching with non-uniform structure and an average branching factor of over 8. I present an efficient parallel GPU MCTS implementation based on the introduced 'block-parallelism' scheme which combines GPU SIMD thread groups and performs independent searches without any need of intra-GPU or inter-GPU communication. I compare it with a simple leaf parallel scheme which implies certain performance limitations. The obtained results show that using my GPU MCTS implementation on the TSUBAME 2.0 system one GPU can be compared to 100-200 CPU threads depending on factors such as the search time and other MCTS parameters in terms of obtained results. I propose and analyze simultaneous CPU/GPU execution which improves the overall result.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Rocki and Suda - 2011 - Large-Scale Parallel Monte Carlo Tree Search on GP.pdf},
  isbn = {978-1-61284-425-1},
  language = {en}
}

@inproceedings{rocki_massively_2010,
  title = {Massively {{Parallel Monte Carlo Tree Search}}},
  booktitle = {Proceedings of the 9th {{International Meeting High Performance Computing}} for {{Computational Science}}.},
  author = {Rocki, Kamil and Suda, Reiji},
  year = {2010},
  pages = {8},
  abstract = {Monte Carlo Tree Search is a method of finding near-optimal solutions for large state-space problems. Currently, is it very important to develop algorithms being able to take advantage of great number of processors in such areas. In this paper MCTS parallel implementations for thousand of cores are presented and discussed. The MCTS parallelization method used is the root parallelization. Implementation of the distributed scheme uses MPI. Results presented are based on the Reversi game rules.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Rocki and Suda - 2010 - Massively Parallel Monte Carlo Tree Search.pdf},
  language = {en}
}

@inproceedings{rocki_parallel_2011,
  title = {Parallel {{Monte Carlo Tree Search}} on {{GPU}}},
  booktitle = {Frontiers in {{Artificial Intelligence}} and {{Applications}}},
  author = {Rocki, Kamil and Suda, Reiji},
  year = {2011},
  pages = {10},
  abstract = {Monte Carlo Tree Search (MCTS) is a method for making optimal decisions in artificial intelligence (AI) problems, typically move planning in combinatorial games. It combines the generality of random simulation with the precision of tree search. It can theoretically be applied to any domain that can be described in terms of state, action pairs and simulation used to forecast outcomes such as decision support, control, delayed reward problems or complex optimization. The motivation behind this work is caused by the emerging GPU-based systems and their high computational potential combined with relatively low power usage compared to CPUs. As a problem to be solved we chose to develop an AI GPU(Graphics Processing Unit)-based agent in the game of Reversi (Othello) which provides a sufficiently complex problem for tree searching with non-uniform structure and an average branching factor of over 8. We present an efficient parallel GPU MCTS implementation based on the introduced 'block-parallelism' scheme which combines GPU SIMD thread groups and performs independent searches without any need of intra-GPU or inter-GPU communication. We compare it with a simple leaf parallel scheme which implies certain performance limitations. The obtained results show that using my GPU MCTS implementation on the TSUBAME 2.0 system one GPU can be compared to 100-200 CPU threads depending on factors such as the search time and other MCTS parameters in terms of obtained results. We propose and analyze simultaneous CPU/GPU execution which improves the overall result.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Rocki and Suda - 2011 - Parallel Monte Carlo Tree Search on GPU.pdf},
  language = {en}
}

@article{rojas-dominguez_modeling_2019,
  title = {Modeling the {{Game}} of {{Go}} by {{Ising Hamiltonian}}, {{Deep Belief Networks}} and {{Common Fate Graphs}}},
  author = {{Rojas-Dominguez}, Alfonso and {Barradas-Baustista}, Didier and Alvarado, Matias},
  year = {2019},
  volume = {7},
  pages = {120117--120127},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2019.2917442},
  abstract = {Three different models of the game of Go are developed by establishing an analogy between this game and physical systems susceptible to analysis under the well-known Ising model in two dimensions. The Ising Hamiltonian is adapted to measure the `energy' of the Go boards generated by the interaction of the game pieces (stones) as players make their moves in an attempt to control the board or to capture rival stones. The proposed models are increasingly complex. The first or Atomic-Go model, consists of the straightforward measurement of local energy employing the adapted Ising Hamiltonian. The second or Generative Atomic-Go model, employs a Deep Belief Network (a generative graphical model popular in machine learning) to generate board configurations and compensate for the lack of information in mostly-empty boards. The third or Molecular-Go model, incorporates Common Fate Graphs, which are an alternative representation of the Go board that offers advantages in pattern analysis. Simulated games between different Go playing systems were used to test whether the models are able to capture the energy changes produced by moves between players of different skills. The results indicate that the latter two models reflect said energy differences correctly. These positive results encourage further development of analysis tools based on the techniques discussed.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Rojas-Dominguez et al. - 2019 - Modeling the Game of Go by Ising Hamiltonian, Deep.pdf},
  journal = {IEEE Access},
  language = {en}
}

@article{ross_online_2008,
  title = {Online {{Planning Algorithms}} for {{POMDPs}}},
  author = {Ross, S. and Pineau, J. and Paquet, S. and {Chaib-draa}, B.},
  year = {2008},
  month = jul,
  volume = {32},
  pages = {663--704},
  issn = {1076-9757},
  doi = {10.1613/jair.2567},
  abstract = {Partially Observable Markov Decision Processes (POMDPs) provide a rich framework for sequential decision-making under uncertainty in stochastic domains. However, solving a POMDP is often intractable except for small problems due to their complexity. Here, we focus on online approaches that alleviate the computational complexity by computing good local policies at each decision step during the execution. Online algorithms generally consist of a lookahead search to find the best action to execute at each time step in an environment. Our objectives here are to survey the various existing online POMDP methods, analyze their properties and discuss their advantages and disadvantages; and to thoroughly evaluate these online approaches in different environments under various metrics (return, error bound reduction, lower bound improvement). Our experimental results indicate that state-of-the-art online heuristic search methods can handle large POMDP domains efficiently.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Ross et al. - 2008 - Online Planning Algorithms for POMDPs.pdf},
  journal = {Journal of Artificial Intelligence Research},
  language = {en}
}

@incollection{sadikov_automated_2007,
  title = {Automated {{Chess Tutor}}},
  booktitle = {Computers and {{Games}}},
  author = {Sadikov, Aleksander and Mo{\v z}ina, Martin and Guid, Matej and Krivec, Jana and Bratko, Ivan},
  year = {2007},
  volume = {4630},
  pages = {13--25},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-75538-8_2},
  abstract = {While recently the strength of chess-playing programs has grown immensely, their capability of explaining in human understandable terms why some moves are good or bad has enjoyed little attention. Progress towards programs with an ability to provide intelligent commentary on chess games, either played by a program or by a human, has been negligible in comparison with the progress concerning playing strength. The typical style of a program's ``comments'' (in terms of the best variations and their numerical scores) is of little use to a human who wants to learn important concepts behind the variations.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2006\\7 Automated Chess Tutor.pdf},
  isbn = {978-3-540-75537-1 978-3-540-75538-8},
  language = {en}
}

@incollection{sadikov_search_2007,
  title = {Search {{Versus Knowledge Revisited Again}}},
  booktitle = {Computers and {{Games}}},
  author = {Sadikov, Aleksander and Bratko, Ivan},
  year = {2007},
  volume = {4630},
  pages = {172--180},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-75538-8_15},
  abstract = {The questions focusing on diminishing returns for additional search effort have been a burning issue in computer chess. Despite a lot of research in this field, there are still some open questions, e.g., what happens at search depths beyond 12 plies, and what is the effect of the program's knowledge on diminishing returns? The paper presents an experiment that attempts to answer these questions. The results (a) confirm that diminishing returns in chess exist, and more importantly (b) show that the amount of knowledge a program has influences when diminishing returns will start to manifest themselves.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2006\\20 Search Versus Knowledge Revisited Again.pdf},
  isbn = {978-3-540-75537-1 978-3-540-75538-8},
  language = {en}
}

@inproceedings{saffidine_alpha-beta_2012,
  title = {Alpha-{{Beta Pruning}} for {{Games}} with {{Simultaneous Moves}}},
  booktitle = {Proceedings of the {{Twenty}}-{{Sixth AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Saffidine, Abdallah and Finnsson, Hilmar and Buro, Michael},
  year = {2012},
  pages = {556--562},
  abstract = {Alpha-Beta pruning is one of the most powerful and fundamental MiniMax search improvements. It was designed for sequential two-player zero-sum perfect information games. In this paper we introduce an Alpha-Beta-like sound pruning method for the more general class of ``stacked matrix games'' that allow for simultaneous moves by both players. This is accomplished by maintaining upper and lower bounds for achievable payoffs in states with simultaneous actions and dominated action pruning based on the feasibility of certain linear programs. Empirical data shows considerable savings in terms of expanded nodes compared to naive depth-first move computation without pruning.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Saffidine et al. - 2012 - Alpha-Beta Pruning for Games with Simultaneous Mov.pdf},
  language = {en}
}

@incollection{saffidine_developments_2014,
  title = {Developments on {{Product Propagation}}},
  booktitle = {Computers and {{Games}}},
  author = {Saffidine, Abdallah and Cazenave, Tristan},
  year = {2014},
  volume = {8427},
  pages = {100--109},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-09165-5_9},
  abstract = {Product Propagation (pp) is an algorithm to backup probabilistic evaluations for abstract two-player games. It was shown that pp could solve Go problems as efficiently as Proof Number Search (pns). In this paper, we exhibit three domains where, for generic non-optimized versions, pp performs better (see the nuances in the paper) than previously known algorithms for solving games. The compared approaches include alpha-beta search, pns, and Monte-Carlo Tree Search. We also extend pp to deal with its memory consumption and to improve its solving time.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2013\\13 Developments on Product Propagation.pdf},
  isbn = {978-3-319-09164-8 978-3-319-09165-5},
  language = {en}
}

@incollection{saffidine_material_2014,
  title = {Material {{Symmetry}} to {{Partition Endgame Tables}}},
  booktitle = {Computers and {{Games}}},
  author = {Saffidine, Abdallah and Jouandeau, Nicolas and Buron, C{\'e}dric and Cazenave, Tristan},
  year = {2014},
  volume = {8427},
  pages = {187--198},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-09165-5_16},
  abstract = {Many games display some kind of material symmetry. That is, some sets of game elements can be exchanged for another set of game elements, so that the resulting position will be equivalent to the original one, no matter how the elements were arranged on the board. Material symmetry is routinely used in card game engines when they normalize their internal representation of the cards.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2013\\20 Material Symmetry to Partition Endgame Tables.pdf},
  isbn = {978-3-319-09164-8 978-3-319-09165-5},
  language = {en}
}

@inproceedings{sailer_adversarial_2007,
  title = {Adversarial {{Planning Through Strategy Simulation}}},
  booktitle = {2007 {{IEEE Symposium}} on {{Computational Intelligence}} and {{Games}}},
  author = {Sailer, Frantisek and Buro, Michael and Lanctot, Marc},
  year = {2007},
  pages = {80--87},
  publisher = {{IEEE}},
  address = {{Honolulu, HI, USA}},
  doi = {10.1109/CIG.2007.368082},
  abstract = {Adversarial planning in highly complex decision domains, such as modern video games, has not yet received much attention from AI researchers. In this paper, we present a planning framework that uses strategy simulation in conjunction with Nash-equilibrium strategy approximation. We apply this framework to an army deployment problem in a real-time strategy game setting and present experimental results that indicate a performance gain over the scripted strategies that the system is built on. This technique provides an automated way of increasing the decision quality of scripted AI systems and is therefore ideally suited for video games and combat simulators.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Sailer et al. - 2007 - Adversarial Planning Through Strategy Simulation.pdf},
  isbn = {978-1-4244-0709-5},
  language = {en}
}

@inproceedings{saito_grouping_2007,
  title = {Grouping {{Nodes}} for {{Monte}}-{{Carlo Tree Search}}},
  booktitle = {19th {{Belgian}}-{{Dutch Conference}} on {{Artificial Intelligence}}},
  author = {Saito, Jahn-Takeshi and Winands, Mark H.M.},
  year = {2007},
  pages = {276--283},
  publisher = {{Informatica}},
  abstract = {Only recently Monte-Carlo Tree Search (MCTS) has substantially contributed to the field of computer Go. So far, in standard MCTS there is only one type of node: every node of the tree represents a single move. Instead of maintaining only this type of node, we propose a second type of node representing groups of moves. Thus, the tree may contain move nodes and group nodes. This article documents how such group nodes can be utilised for including domain knowledge in MCTS. Furthermore, we present a technique, called Alternating-Layer UCT, for managing move nodes and group nodes in a tree with alternating layers of move nodes and group nodes. A self-play experiment performed in the game of Go demonstrates that group nodes can be used effectively to integrate domain knowledge in a MCTS program and thereby improve its playing strength.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Saito and Winands - 2007 - Grouping Nodes for Monte-Carlo Tree Search.pdf},
  language = {en}
}

@incollection{saito_monte-carlo_2007,
  title = {Monte-{{Carlo Proof}}-{{Number Search}} for {{Computer Go}}},
  booktitle = {Computers and {{Games}}},
  author = {Saito, Jahn-Takeshi and Chaslot, Guillaume and Uiterwijk, Jos W. H. M. and {van den Herik}, H. Jaap},
  year = {2007},
  volume = {4630},
  pages = {50--61},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-75538-8_5},
  abstract = {In the last decade, proof-number search and Monte-Carlo methods have successfully been applied to the combinatorial-games domain. Proof-number search is a reliable algorithm. It requires a well defined goal to prove. This can be seen as a disadvantage. In contrast to proof-number search, Monte-Carlo evaluation is a flexible stochastic evaluation for game-tree search. In order to improve the efficiency of proof-number search, we introduce a new algorithm, Monte-Carlo ProofNumber search. It enhances proof-number search by adding the flexible Monte-Carlo evaluation. We present the new algorithm and evaluate it on a sub-problem of Go, the Life-and-Death problem. The results show a clear improvement in time efficiency and memory usage: the test problems are solved two times faster and four times less nodes are expanded on average. Future work will assess possibilities to extend this method to other enhanced Proof-Number techniques.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2006\\10 Monte-Carlo Proof-Number Search for Computer Go.pdf},
  isbn = {978-3-540-75537-1 978-3-540-75538-8},
  language = {en}
}

@book{salen_rules_2004,
  title = {Rules of Play: Game Design Fundamentals},
  shorttitle = {Rules of Play},
  author = {Salen, Katie and Zimmerman, Eric},
  year = {2004},
  publisher = {{MIT Press}},
  address = {{Cambridge, Mass.}},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\A787PB2W\\Salen and Zimmerman - 2004 - Rules of play game design fundamentals.chm},
  isbn = {978-0-262-24045-1},
  language = {English}
}

@inproceedings{sarratt_converging_2014,
  title = {Converging to a Player Model in {{Monte}}-{{Carlo Tree Search}}},
  booktitle = {2014 {{IEEE Conference}} on {{Computational Intelligence}} and {{Games}}},
  author = {Sarratt, Trevor and Pynadath, David V. and Jhala, Arnav},
  year = {2014},
  month = aug,
  pages = {1--7},
  issn = {2325-4289},
  doi = {10.1109/CIG.2014.6932881},
  abstract = {Player models allow search algorithms to account for differences in agent behavior according to player's preferences and goals. However, it is often not until the first actions are taken that an agent can begin assessing which models are relevant to its current opponent. This paper investigates the integration of belief distributions over player models in the Monte-Carlo Tree Search (MCTS) algorithm. We describe a method of updating belief distributions through leveraging information sampled during the MCTS. We then characterize the effect of tuning parameters of the MCTS to convergence of belief distributions. Evaluation of this approach is done in comparison with value iteration for an iterated version of the prisoner's dilemma problem. We show that for a sufficient quantity of iterations, our approach converges to the correct model faster than the same model under value iteration.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Sarratt et al. - 2014 - Converging to a player model in Monte-Carlo Tree S.pdf;C\:\\Users\\aesou\\Zotero\\storage\\DMEADMCY\\6932881.html},
  keywords = {agent behavior,Backpropagation,belief distribution,belief maintenance,computer games,game theory,Irrigation,MCTS algorithm,Monte Carlo methods,Monte-Carlo tree search,player goals,player model convergence,player preference,prisoners dilemma problem,search algorithm,trees (mathematics),tuning parameter,value iteration}
}

@inproceedings{sato_three_2016,
  title = {Three Types of Forward Pruning Techniques to Apply the Alpha Beta Algorithm to Turn-Based Strategy Games},
  booktitle = {2016 {{IEEE Conference}} on {{Computational Intelligence}} and {{Games}} ({{CIG}})},
  author = {Sato, Naoyuki and Ikeda, Kokolo},
  year = {2016},
  month = sep,
  pages = {1--8},
  issn = {2325-4289},
  doi = {10.1109/CIG.2016.7860427},
  abstract = {Turn-based strategy games are interesting testbeds for developing artificial players because their rules present developers with several challenges. Currently, Monte-Carlo tree search variants are often utilized to address these challenges. However, we consider it worthwhile introducing minimax search variants with pruning techniques because a turn-based strategy is in some points similar to the games of chess and Shogi, in which minimax variants are known to be effective. Thus, we introduced three forward-pruning techniques to enable us to apply alpha beta search (as a minimax search variant) to turn-based strategy games. This type of search involves fixing unit action orders, generating unit actions selectively, and limiting the number of moving units in a search. We applied our proposed pruning methods by implementing an alpha beta-based artificial player in the Turn-based strategy Academic Package (TUBSTAP) open platform of our institute. This player competed against first- and second-rank players in the TUBSTAP AI competition in 2016. Our proposed player won against the other players in five different maps with an average winning ratio exceeding 70\%.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Sato and Ikeda - 2016 - Three types of forward pruning techniques to apply.pdf;C\:\\Users\\aesou\\Zotero\\storage\\CFQI537D\\7860427.html},
  keywords = {alpha beta algorithm,Artificial intelligence,artificial player development,chess game,computer games,Computers,Electronic mail,forward pruning techniques,Games,Limiting,minimax search variants,minimax techniques,Monte Carlo methods,Monte-Carlo tree search,open platform,Shogi,tree searching,TUBSTAP AI competition,turn-based strategy academic package,turn-based strategy games,unit action generation,unit action orders,Vegetation}
}

@incollection{schadd_single-player_2008,
  title = {Single-{{Player Monte}}-{{Carlo Tree Search}}},
  booktitle = {Computers and {{Games}}},
  author = {Schadd, Maarten P. D. and Winands, Mark H. M. and {van den Herik}, H. Jaap and Chaslot, Guillaume M. J. -B. and Uiterwijk, Jos W. H. M.},
  year = {2008},
  volume = {5131},
  pages = {1--12},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-87608-3_1},
  abstract = {Classical methods such as A* and IDA* are a popular and successful choice for one-player games. However, they fail without an accurate admissible evaluation function. In this paper we investigate whether Monte-Carlo Tree Search (MCTS) is an interesting alternative for oneplayer games where A* and IDA* methods do not perform well. Therefore, we propose a new MCTS variant, called Single-Player Monte-Carlo Tree Search (SP-MCTS). The selection and backpropagation strategy in SPMCTS are different from standard MCTS. Moreover, SP-MCTS makes use of a straightforward Meta-Search extension. We tested the method on the puzzle SameGame. It turned out that our SP-MCTS program gained the highest score so far on the standardized test set.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2008\\6 Single-Player Monte-Carlo Tree Search.pdf},
  isbn = {978-3-540-87607-6 978-3-540-87608-3},
  language = {en}
}

@book{schell_art_2015,
  title = {The Art of Game Design},
  author = {Schell, Jesse},
  year = {2015},
  abstract = {Good game design happens when you view your game from as many perspectives as possible. Written by one of the world's top game designers, The Art of Game Design presents 100+ sets of questions, or different lenses, for viewing a game's design, encompassing diverse fields such as psychology, architecture, music, visual design, film, software engineering, theme park design, mathematics, puzzle design, and anthropology. This Second Edition of a Game Developer Front Line Award winner: Describes the deepest and most fundamental principles of game design Demonstrates how tactics used in board, c.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\R62RMZ2D\\Schell - 2015 - The art of game design.pdf},
  isbn = {978-1-4665-9867-6},
  language = {English}
}

@article{schrittwieser_mastering_2019,
  title = {Mastering {{Atari}}, {{Go}}, {{Chess}} and {{Shogi}} by {{Planning}} with a {{Learned Model}}},
  author = {Schrittwieser, Julian and Antonoglou, Ioannis and Hubert, Thomas and Simonyan, Karen and Sifre, Laurent and Schmitt, Simon and Guez, Arthur and Lockhart, Edward and Hassabis, Demis and Graepel, Thore and Lillicrap, Timothy and Silver, David},
  year = {2019},
  month = nov,
  abstract = {Constructing agents with planning capabilities has long been one of the main challenges in the pursuit of artificial intelligence. Tree-based planning methods have enjoyed huge success in challenging domains, such as chess and Go, where a perfect simulator is available. However, in real-world problems the dynamics governing the environment are often complex and unknown. In this work we present the MuZero algorithm which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics. MuZero learns a model that, when applied iteratively, predicts the quantities most directly relevant to planning: the reward, the actionselection policy, and the value function. When evaluated on 57 different Atari games - the canonical video game environment for testing AI techniques, in which model-based planning approaches have historically struggled our new algorithm achieved a new state of the art. When evaluated on Go, chess and shogi, without any knowledge of the game rules, MuZero matched the superhuman performance of the AlphaZero algorithm that was supplied with the game rules.},
  archivePrefix = {arXiv},
  eprint = {1911.08265},
  eprinttype = {arxiv},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Schrittwieser et al. - 2019 - Mastering Atari, Go, Chess and Shogi by Planning w.pdf},
  journal = {arXiv:1911.08265 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@book{schwab_ai_2004,
  title = {{{AI}} Game Engine Programming 1st},
  author = {Schwab, Brian},
  year = {2004},
  edition = {1st},
  publisher = {{Charles River Media}},
  address = {{Hingham, MA}},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\RGQLSCNM\\Schwab - 2004 - AI game engine programming.pdf},
  isbn = {978-1-58450-344-6},
  keywords = {Artificial intelligence,Computer games,Programming},
  language = {en},
  lccn = {QA76.76.C672 S39 2004}
}

@book{schwab_ai_2009,
  title = {{{AI}} Game Engine Programming 2nd},
  author = {Schwab, Brian},
  year = {2009},
  edition = {2nd ed},
  publisher = {{Course Technology, Cengage Learning}},
  address = {{Boston, MA}},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\GMSGHRLK\\Schwab - 2009 - AI game engine programming.pdf},
  isbn = {978-1-58450-572-3},
  keywords = {Artificial intelligence,Computer games,Programming},
  language = {en},
  lccn = {QA76.76.C672 S39 2009}
}

@incollection{segal_scalability_2011,
  title = {On the {{Scalability}} of {{Parallel UCT}}},
  booktitle = {Computers and {{Games}}},
  author = {Segal, Richard B.},
  year = {2011},
  volume = {6515},
  pages = {36--47},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-17928-0_4},
  abstract = {The parallelization of MCTS across multiple-machines has proven surprisingly difficult. The limitations of existing algorithms were evident in the 2009 Computer Olympiad where Zen using a single fourcore machine defeated both Fuego with ten eight-core machines, and Mogo with twenty thirty-two core machines. This paper investigates the limits of parallel MCTS in order to understand why distributed parallelism has proven so difficult and to pave the way towards future distributed algorithms with better scaling. We first analyze the singlethreaded scaling of Fuego and find that there is an upper bound on the play-quality improvements which can come from additional search. We then analyze the scaling of an idealized N-core shared memory machine to determine the maximum amount of parallelism supported by MCTS. We show that parallel speedup depends critically on how much time is given to each player. We use this relationship to predict parallel scaling for time scales beyond what can be empirically evaluated due to the immense computation required. Our results show that MCTS can scale nearly perfectly to at least 64 threads when combined with virtual loss, but without virtual loss scaling is limited to just eight threads. We also find that for competition time controls scaling to thousands of threads is impossible not necessarily due to MCTS not scaling, but because high levels of parallelism can start to bump up against the upper performance bound of Fuego itself.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2010\\9 On the Scalability of Parallel UCT.pdf},
  isbn = {978-3-642-17927-3 978-3-642-17928-0},
  language = {en}
}

@inproceedings{sephton_heuristic_2014,
  title = {Heuristic Move Pruning in {{Monte Carlo Tree Search}} for the Strategic Card Game {{Lords}} of {{War}}},
  booktitle = {2014 {{IEEE Conference}} on {{Computational Intelligence}} and {{Games}}},
  author = {Sephton, Nick and Cowling, Peter I. and Powley, Edward and Slaven, Nicholas H.},
  year = {2014},
  month = aug,
  publisher = {{IEEE}},
  address = {{Dortmund, Germany}},
  doi = {10.1109/CIG.2014.6932892},
  abstract = {Move pruning is a technique used in game tree search which incorporates heuristic knowledge to reduce the number of moves under consideration from a particular game state. This paper investigates Heuristic Move Pruning on the strategic card game Lords of War. We use heuristics to guide our pruning and experiment with different techniques of applying pruning and their relative effectiveness. We also present a technique of artificially rolling forward a game state in an attempt to more accurately determine which moves are appropriate to prune from the decision tree. We demonstrate that heuristic move pruning is efl"ective in Lords of War, and also that artificially rolling forward the game state can increase the effectiveness of heuristic move pruning.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Sephton et al. - 2014 - Heuristic move pruning in Monte Carlo Tree Search .pdf},
  isbn = {978-1-4799-3547-5},
  language = {en}
}

@article{sethy_real_2015,
  title = {Real {{Time Strategy Games}}: {{A Reinforcement Learning Approach}}},
  shorttitle = {Real {{Time Strategy Games}}},
  author = {Sethy, Harshit and Patel, Amit and Padmanabhan, Vineet},
  year = {2015},
  volume = {54},
  pages = {257--264},
  issn = {18770509},
  doi = {10.1016/j.procs.2015.06.030},
  abstract = {In this paper we proposed reinforcement learning algorithms with the generalized reward function. In our proposed method we use Q-learning1 and SARSA1 algorithms with generalised reward function to train the reinforcement learning agent. We evaluated the performance of our proposed algorithms on Real Time Strategy (RTS) game called BattleCity. There are two main advantages of having such an approach as compared to other works in RTS. (1) We can ignore the concept of a simulator which is often game specific and is usually hard coded in any type of RTS games (2) our system can learn from interaction with any opponents and quickly change the strategy according to the opponents and do not need any human traces as used in previous works.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\CNDETT5E\\Sethy et al. - 2015 - Real Time Strategy Games A Reinforcement Learning.pdf},
  journal = {Procedia Computer Science},
  language = {en}
}

@article{shao_survey_2019-1,
  title = {A {{Survey}} of {{Deep Reinforcement Learning}} in {{Video Games}}},
  author = {Shao, Kun and Tang, Zhentao and Zhu, Yuanheng and Li, Nannan and Zhao, Dongbin},
  year = {2019},
  month = dec,
  abstract = {Deep reinforcement learning (DRL) has made great achievements since proposed. Generally, DRL agents receive high-dimensional inputs at each step, and make actions according to deep-neural-network-based policies. This learning mechanism updates the policy to maximize the return with an end-toend method. In this paper, we survey the progress of DRL methods, including value-based, policy gradient, and model-based algorithms, and compare their main techniques and properties. Besides, DRL plays an important role in game artificial intelligence (AI). We also take a review of the achievements of DRL in various video games, including classical Arcade games, first-person perspective games and multi-agent real-time strategy games, from 2D to 3D, and from single-agent to multi-agent. A large number of video game AIs with DRL have achieved super-human performance, while there are still some challenges in this domain. Therefore, we also discuss some key points when applying DRL methods to this field, including explorationexploitation, sample efficiency, generalization and transfer, multiagent learning, imperfect information, and delayed spare rewards, as well as some research directions.},
  archivePrefix = {arXiv},
  eprint = {1912.10944},
  eprinttype = {arxiv},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Shao et al. - 2019 - A Survey of Deep Reinforcement Learning in Video G 2.pdf},
  journal = {arXiv:1912.10944 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Multiagent Systems},
  language = {en},
  primaryClass = {cs}
}

@phdthesis{sharples_simplied_2018,
  title = {A {{Simplified API}} for the {{Creation}} of {{Bots}} for {{Real Time Strategy Games}}},
  author = {Sharples, Timothy},
  year = {2018},
  abstract = {Artificial Intelligence research in the past few years has been increasingly focusing on games, with Real Time Strategy games being of particular interest. However, one of the main tools used in the creation of agents in these environments has quite a steep learning curve for entry into development, leading to some potential entry barriers that new AI developers have to overcome to get into the field.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\8Z5QP3Z2\\Sharples - 2018 - A Simpliﬁed API for the Creation of Bots for Real .PDF},
  language = {en}
}

@inproceedings{shen_self-attention_2019,
  title = {Self-{{Attention}} for {{Deep Reinforcement Learning}}},
  booktitle = {Proceedings of the 2019 4th {{International Conference}} on {{Mathematics}} and {{Artificial Intelligence}}},
  author = {Shen, Xiangxiang and Yin, Chuanhuan and Hou, Xinwen},
  year = {2019},
  month = apr,
  pages = {71--75},
  publisher = {{Association for Computing Machinery}},
  address = {{Chegndu, China}},
  doi = {10.1145/3325730.3325743},
  abstract = {Reinforcement learning is concerned with how software agents ought to take actions according to the state of the environment so as to maximize some notion of cumulative reward. Therefore, in-depth study and mining of the state of the environment will be more conducive to the agent to make better decisions. Motivated by the advantages of self-attention mechanism in machine translation, this paper presents a new scheme. In this scheme, the state in deep reinforcement learning algorithms can be combined with self-attention mechanism. After that agents will pay more attention to the internal structure of state especially in a complex game environment, like real-time strategy game StarCraft. StarCraft is a huge challenge platform for AI researchers because of its huge state spaces and action spaces. Some baseline agents of reinforcement learning provided by DeepMind for mini-games in StarCraft II have not reached the level of an amateur player. Our agents use fewer features than DeepMind's baseline agents and have made significant improvement.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Shen et al. - 2019 - Self-Attention for Deep Reinforcement Learning.pdf},
  isbn = {978-1-4503-6258-0},
  keywords = {A3C,Deep reinforcement learning,Self-Attention,StarCraft II mini-games},
  series = {{{ICMAI}} 2019}
}

@inproceedings{shleyfman_combinatorial_2014,
  title = {On {{Combinatorial Actions}} and {{CMABs}} with {{Linear Side Information}}},
  booktitle = {{{ECAI}} 2014: 21st {{European Conference}} on {{Artificial Intelligence}}},
  author = {Shleyfman, Alexander and Komenda, Anton{\'i}n and Domshlak, Carmel},
  year = {2014},
  pages = {825--830},
  abstract = {Online planning algorithms are typically a tool of choice for dealing with sequential decision problems in combinatorial search spaces. Many such problems, however, also exhibit combinatorial actions, yet standard planning algorithms do not cope well with this type of ``the curse of dimensionality." Following a recently opened line of related work on combinatorial multi-armed bandit (CMAB) problems, we propose a novel CMAB planning scheme, as well as two specific instances of this scheme, dedicated to exploiting what is called linear side information. Using a representative strategy game as a benchmark, we show that the resulting algorithms very favorably compete with the state-of-the-art.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\RY756XLW\\Shleyfman et al. - 2014 - On Combinatorial Actions and CMABs with Linear Sid.pdf},
  language = {en}
}

@book{shonkwiler_explorations_2009,
  title = {Explorations in {{Monte Carlo}} Methods},
  author = {Shonkwiler, Ronald W. and Mendivil, Franklin},
  year = {2009},
  publisher = {{Springer}},
  address = {{Dordrecht ; New York}},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Shonkwiler and Mendivil - 2009 - Explorations in Monte Carlo methods.pdf},
  isbn = {978-0-387-87836-2 978-0-387-87837-9},
  keywords = {Monte Carlo method,Monte-Carlo-Simulation},
  language = {en},
  lccn = {QA298 .S55 2009},
  series = {Undergraduate Texts in Mathematics}
}

@inproceedings{shoulson_parameterizing_2011-1,
  title = {Parameterizing {{Behavior Trees}}},
  booktitle = {Motion in {{Games}}},
  author = {Shoulson, Alexander and Garcia, Francisco M. and Jones, Matthew and Mead, Robert and Badler, Norman I.},
  year = {2011},
  month = nov,
  pages = {144--155},
  publisher = {{Springer, Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-25090-3_13},
  abstract = {This paper introduces and motivates the application of parameterization to behavior trees. As a framework, behavior trees are becoming more commonly used for agent controllers in interactive game...},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\LY9U2RZD\\Shoulson et al. - 2011 - Parameterizing Behavior Trees.pdf;C\:\\Users\\aesou\\Zotero\\storage\\YJAPVJ3Q\\10.html},
  language = {en}
}

@inproceedings{si_scouting_2014,
  title = {A {{Scouting Strategy}} for {{Real}}-{{Time Strategy Games}}},
  booktitle = {Proceedings of the 2014 {{Conference}} on {{Interactive Entertainment}} - {{IE2014}}},
  author = {Si, Chen and Pisan, Yusuf and Tan, Chek Tien},
  year = {2014},
  pages = {1--8},
  publisher = {{ACM Press}},
  address = {{Newcastle, NSW, Australia}},
  doi = {10.1145/2677758.2677772},
  abstract = {Real-time strategy (RTS) is a sub-genre of strategy video games. RTS games are more realistic with dynamic and time-constraint game playing, by abandoning the turn-based rule of its ancestors. Playing with and against computer-controlled players is a pervasive phenomenon in RTS games, due to the convenience and the preference of groups of players. Hence, better game-playing agents are able to enhance game-playing experience by acting as smart opponents or collaborators. One-way of improving gameplaying agents' performance, in terms of their economicexpansion and tactical battlefield-arrangement aspects, is to understand the game environment. Traditional commercial RTS game-playing agents address this issue by directly accessing game maps and extracting strategic features. Since human players are unable to access the same information, this is a form of ``cheating AI'', which has been known to negatively affect player experiences. Thus, we develop a scouting mechanism for RTS game-playing agents, in order to enable game units to explore game environments automatically in a realistic fashion. Our research is grounded in prior robotic exploration work by which we present a hierarchical multi-criterion decision-making (MCDM) strategy to address the incomplete information problem in RTS settings.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\YICBEFHW\\Si et al. - 2014 - A Scouting Strategy for Real-Time Strategy Games.pdf},
  isbn = {978-1-4503-2790-9},
  language = {en}
}

@inproceedings{si_understanding_2016,
  title = {Understanding Players' Map Exploration Styles},
  booktitle = {Proceedings of the {{Australasian Computer Science Week Multiconference}} on - {{ACSW}} '16},
  author = {Si, Chen and Pisan, Yusuf and Tan, Chek Tien},
  year = {2016},
  pages = {1--6},
  publisher = {{ACM Press}},
  address = {{Canberra, Australia}},
  doi = {10.1145/2843043.2843480},
  abstract = {Exploration is an essential part of play in modern video games. It refers to the discovery-based activities, in which players explore mechanisms, as well as spatiality of virtual world. Exploration games and games with exploration plots are booming in gamer communities. In this paper, we focus on spatial exploration, which is central to play in role-playing games (RPG) and real time strategy (RTS) games. We investigate the game-playing behaviors of human players in exploration games, so as to discover behavior patterns and understand gamer styles. The intention is to contribute to the design and development of believable agents. We conducted an experiment where 25 participants played three types of exploration games. In-game data, think-aloud data, questionnaire responses and post-game interview data were collected to gain a deeper understanding of exploration preferences. We used thematic analysis to analyze data and mapped out four game exploration archetypes: Wanderers, Seers, Pathers and Targeters. An analysis from the four highlight aspects: strategy, reasoning, conception and hesitation, is conducted to investigate the behavioral traits of these four archetypes.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\8MIYHUQI\\Si et al. - 2016 - Understanding players' map exploration styles.pdf},
  isbn = {978-1-4503-4042-7},
  language = {en}
}

@inproceedings{siljebrat_towards_2018,
  title = {Towards Human-like Artificial Intelligence Using {{StarCraft}} 2},
  booktitle = {Proceedings of the 13th {{International Conference}} on the {{Foundations}} of {{Digital Games}}  - {{FDG}} '18},
  author = {Siljebr{\aa}t, Henrik and Addyman, Caspar and Pickering, Alan},
  year = {2018},
  pages = {1--4},
  publisher = {{ACM Press}},
  address = {{Malm\&\#246;, Sweden}},
  doi = {10.1145/3235765.3235811},
  abstract = {On our path towards artificial general intelligence, video games have become excellent tools for research. Reinforcement learning (RL) algorithms are particularly successful in this domain, with the added benefit of having fairly well established biological foundations. To improve how artificial intelligence research and the cognitive sciences can inform each other, we argue the StarCraft II Learning Environment is an ideal candidate for an environment where humans and artificial agents can be tested on the same tasks. We present an upcoming study using this environment, where the goal is to investigate how RL can be extended to enable abstract human abilities such as moments of insight. We claim this is valuable for advancing our understanding of both artificial and natural intelligence, thereby leading to improved models of player behaviour and for general video game playing.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\AE8UGN6J\\Siljebråt et al. - 2018 - Towards human-like artificial intelligence using S.pdf},
  isbn = {978-1-4503-6571-0},
  language = {en}
}

@article{silva_development_2017,
  title = {On the {{Development}} of {{Intelligent Agents}} for {{MOBA Games}}},
  author = {Silva, Victor do Nascimento and Chaimowicz, Luiz},
  year = {2017},
  month = jun,
  abstract = {Multiplayer Online Battle Arena (MOBA) is one of the most played game genres nowadays. With the increasing growth of this genre, it becomes necessary to develop effective intelligent agents to play alongside or against human players. In this paper we address the problem of agent development for MOBA games. We implement a two-layered architecture agent that handles both navigation and game mechanics. This architecture relies on the use of Influence Maps, a widely used approach for tactical analysis. Several experiments were performed using League of Legends as a testbed, and show promising results in this highly dynamic real-time context.},
  archivePrefix = {arXiv},
  eprint = {1706.02789},
  eprinttype = {arxiv},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\JFRVPJUG\\Silva and Chaimowicz - 2017 - On the Development of Intelligent Agents for MOBA .pdf},
  journal = {arXiv:1706.02789 [cs]},
  keywords = {Computer Science - Artificial Intelligence},
  language = {en},
  primaryClass = {cs}
}

@article{silva_strategy_2018,
  title = {Strategy {{Generation}} for {{Multi}}-{{Unit Real}}-{{Time Games}} via {{Voting}}},
  author = {Silva, Cleyton R. and Moraes, Rubens O. and Lelis, Levi H. S. and Gal, Kobi},
  year = {2018},
  issn = {2475-1502, 2475-1510},
  doi = {10.1109/TG.2018.2848913},
  abstract = {Real-time strategy (RTS) games are a challenging application for Artificial Intelligence (AI) methods. This is because they involve simultaneous play and adversarial reasoning that is conducted in real time in large state spaces. Many AI methods for playing RTS games rely on hard-coded strategies designed by human experts. The drawback of using such strategies is that they are often unable to adapt to new scenarios during gameplay. The contribution of this paper is a new approach, called Strategy Creation via Voting (SCV), that uses a voting method to generate a large set of novel strategies from existing expert-based ones. Then, SCV uses an opponent modeling scheme during the game to choose which strategy from the generated pool of possibilities to use. By repeatedly choosing which strategy to use, SCV is able to adapt to different scenarios that might arise during the game. We implemented SCV as a bot for \textmu{}RTS, a recognized RTS testbed. The results of a detailed empirical study show that SCV outperforms all approaches tested in matches played on large maps and is competitive in matches played on smaller maps.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\SIVPVTQM\\Silva et al. - 2018 - Strategy Generation for Multi-Unit Real-Time Games.pdf},
  journal = {IEEE Transactions on Games},
  language = {en}
}

@article{silver_general_2018,
  title = {A General Reinforcement Learning Algorithm That Masters Chess, Shogi, and {{Go}} through Self-Play},
  author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
  year = {2018},
  month = dec,
  volume = {362},
  pages = {1140--1144},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aar6404},
  abstract = {The game of chess is the longest-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. By contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go by reinforcement learning from self-play. In this paper, we generalize this approach into a single AlphaZero algorithm that can achieve superhuman performance in many challenging games. Starting from random play and given no domain knowledge except the game rules, AlphaZero convincingly defeated a world champion program in the games of chess and shogi (Japanese chess), as well as Go.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\UWYBWZ6S\\science_alphazero_december_2018.pdf;F\:\\Direct Downloads\\science_alphazero_december_2018.pdf},
  journal = {Science},
  language = {en},
  number = {6419}
}

@article{silver_mastering_2016,
  title = {Mastering the Game of {{Go}} with Deep Neural Networks and Tree Search},
  author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and {van den Driessche}, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
  year = {2016},
  month = jan,
  volume = {529},
  pages = {484--489},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature16961},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\9Z5D57R5\\Silver et al. - 2016 - Mastering the game of Go with deep neural networks.pdf},
  journal = {Nature},
  language = {en},
  number = {7587}
}

@article{silver_mastering_2017,
  title = {Mastering {{Chess}} and {{Shogi}} by {{Self}}-{{Play}} with a {{General Reinforcement Learning Algorithm}}},
  author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
  year = {2017},
  month = dec,
  abstract = {The game of chess is the most widely-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. In contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go, by tabula rasa reinforcement learning from games of self-play. In this paper, we generalise this approach into a single AlphaZero algorithm that can achieve, tabula rasa, superhuman performance in many challenging domains. Starting from random play, and given no domain knowledge except the game rules, AlphaZero achieved within 24 hours a superhuman level of play in the games of chess and shogi (Japanese chess) as well as Go, and convincingly defeated a world-champion program in each case.},
  archivePrefix = {arXiv},
  eprint = {1712.01815},
  eprinttype = {arxiv},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Silver et al. - 2017 - Mastering Chess and Shogi by Self-Play with a Gene.pdf},
  journal = {arXiv:1712.01815 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  language = {en},
  primaryClass = {cs}
}

@article{silver_mastering_2017-1,
  title = {Mastering the Game of {{Go}} without Human Knowledge},
  author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and {van den Driessche}, George and Graepel, Thore and Hassabis, Demis},
  year = {2017},
  month = oct,
  volume = {550},
  pages = {354--359},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature24270},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Silver et al. - 2017 - Mastering the game of Go without human knowledge.pdf},
  journal = {Nature},
  language = {en},
  number = {7676}
}

@inproceedings{silvey_embodied_2018-1,
  title = {Embodied {{Cognition}} and {{Multi}}-{{Agent Behavioral Emergence}}},
  booktitle = {Unifying {{Themes}} in {{Complex Systems IX}}},
  author = {Silvey, Paul E. and Norman, Michael D.},
  editor = {Morales, Alfredo J. and Gershenson, Carlos and Braha, Dan and Minai, Ali A. and {Bar-Yam}, Yaneer},
  year = {2018},
  pages = {189--201},
  publisher = {{Springer International Publishing}},
  abstract = {Autonomous systems embedded in our physical world need real-world interaction in order to function, but they also depend on it as a means to learn. This is the essence of artificial Embodied Cognition, in which machine intelligence is tightly coupled to sensors and effectors and where learning happens from continually experiencing the dynamic world as time-series data, received and processed from a situated and contextually-relative perspective. From this stream, our engineered agents must perceptually discriminate, deal with noise and uncertainty, recognize the causal influence of their actions (sometimes with significant and variable temporal lag), pursue multiple and changing goals that are often incompatible with each other, and make decisions under time pressure. To further complicate matters, unpredictability caused by the actions of other adaptive agents makes this experiential data stochastic and statistically non-stationary. Reinforcement Learning approaches to these problems often oversimplify many of these aspects, e.g., by assuming stationarity, collapsing multiple goals into a single reward signal, using repetitive discrete training episodes, or removing real-time requirements. Because we are interested in developing dependable and trustworthy autonomy, we have been studying these problems by retaining all these inherent complexities and only simplifying the agent's environmental bandwidth requirements. The Multi-Agent Research Basic Learning Environment (MARBLE) is a computational framework for studying the nuances of cooperative, competitive, and adversarial learning, where emergent behaviors can be better understood through carefully controlled experiments. In particular, we are using it to evaluate a novel reinforcement learning long-term memory data structure based on probabilistic suffix trees. Here, we describe this research methodology, and report on the results of some early experiments.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\9RDDBQF7\\Silvey and Norman - 2018 - Embodied Cognition and Multi-Agent Behavioral Emer 2.pdf},
  isbn = {978-3-319-96661-8},
  keywords = {Agent-based modeling,Embodied cognition,Emergence,Multi-agent systems,Reinforcement learning},
  language = {en},
  series = {Springer {{Proceedings}} in {{Complexity}}}
}

@inproceedings{sironi_self-adaptive_2018,
  title = {Self-Adaptive {{MCTS}} for {{General Video Game Playing}}},
  booktitle = {Applications of {{Evolutionary Computation}}},
  author = {Sironi, Chiara F. and Liu, Jialin and {Perez-Liebana}, Diego and Gaina, Raluca D. and Bravi, Ivan and Lucas, Simon M. and Winands, Mark H. M.},
  year = {2018},
  month = apr,
  pages = {358--375},
  publisher = {{Springer, Cham}},
  doi = {10.1007/978-3-319-77538-8_25},
  abstract = {Monte-Carlo Tree Search (MCTS) has shown particular success in General Game Playing (GGP) and General Video Game Playing (GVGP) and many enhancements and variants have been developed. Recently, an...},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\LPWLWGYR\\Sironi et al. - 2018 - Self-adaptive MCTS for General Video Game Playing.pdf},
  language = {en}
}

@phdthesis{smejkal_artificial_2018,
  title = {Artificial {{Intelligence}} for {{Children}} of the {{Galaxy Computer Game}}},
  author = {{\v S}mejkal, Pavel},
  year = {2018},
  abstract = {Even though artificial intelligence (AI) agents are now able to solve many classical games, in the field of computer strategy games, the AI opponents still leave much to be desired. In this work we tackle a problem of combat in strategy video games by adapting existing search approaches: Portfolio greedy search (PGS) and Monte-Carlo tree search (MCTS). We also introduce an improved version of MCTS called MCTS considering hit points (MCTS\_HP). These methods are evaluated in context of a recently released 4X strategy game Children of the Galaxy. We implement a combat simulator for the game and a benchmarking framework where various AI approaches can be compared. We show that for small to medium combat MCTS methods are superior to PGS. In all scenarios MCTS\_HP is equal or better than regular MCTS due to its better search guidance. In smaller scenarios MCTS\_HP with only 100 millisecond time limit outperforms regular MCTS with 2 second time limit. By combining fast greedy search for large combats and more precise MCTS\_HP for smaller scenarios a universal AI player can be created.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\8EFIQYNH\\Šmejkal - 2018 - Artificial Intelligence for Children of the Galaxy.pdf},
  school = {Charles University Prague},
  type = {Master}
}

@article{smith_continuous_2010,
  title = {Continuous and {{Reinforcement Learning Methods}} for {{First}}-{{Person Shooter Games}}},
  author = {Smith, Tony C and Miles, Jonathan},
  year = {2010},
  volume = {1},
  pages = {6},
  abstract = {Machine learning is now widely studied as the basis for artificial intelligence systems within computer games. Most existing work focuses on methods for learning static expert systems, typically emphasizing candidate selection. This paper extends this work by exploring the use of continuous and reinforcement learning techniques to develop fully-adaptive game AI for first-person shooter bots. We begin by outlining a framework for learning static control models for tanks within the game BZFlag, then extend that framework using continuous learning techniques that allow computer controlled tanks to adapt to the game style of other players, extending overall playability by thwarting attempts to infer the underlying AI. We further show how reinforcement learning can be used to create bots that learn how to play based solely through trial and error, providing game engineers with a practical means to produce large numbers of bots, each with individual intelligences and unique behaviours; all from a single initial AI model.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\7ZB53BNX\\Smith and Miles - 2010 - Continuous and Reinforcement Learning Methods for .pdf},
  journal = {GSTF INTERNATIONAL JOURNAL ON COMPUTING},
  language = {en},
  number = {1}
}

@inproceedings{soemers_hierarchical_2016,
  title = {Hierarchical {{Task Network Plan Reuse}} for Video Games},
  booktitle = {2016 {{IEEE Conference}} on {{Computational Intelligence}} and {{Games}} ({{CIG}})},
  author = {Soemers, Dennis J. N. J. and Winands, Mark H. M.},
  year = {2016},
  month = sep,
  pages = {1--8},
  publisher = {{IEEE}},
  address = {{Santorini, Greece}},
  doi = {10.1109/CIG.2016.7860395},
  abstract = {Hierarchical Task Network Planning is an Automated Planning technique. It is, among other domains, used in Artificial Intelligence for video games. Generated plans cannot always be fully executed, for example due to nondeterminism or imperfect information. In such cases, it is often desirable to re-plan. This is typically done completely from scratch, or done using techniques that require conditions and effects of tasks to be defined in a specific format (typically based on First-Order Logic). In this paper, an approach for Plan Reuse is proposed that manipulates the order in which the search tree is traversed by using a similarity function. It is tested in the SimpleFPS domain, which simulates a First-Person Shooter game, and shown to be capable of finding (optimal) plans with a decreased amount of search effort on average when re-planning for variations of previously solved problems.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\6AZ2K2DA\\Soemers and Winands - 2016 - Hierarchical Task Network Plan Reuse for video gam.pdf},
  isbn = {978-1-5090-1883-3},
  language = {en}
}

@inproceedings{soemers_learning_2019,
  title = {Learning {{Policies}} from {{Self}}-{{Play}} with {{Policy Gradients}} and {{MCTS Value Estimates}}},
  booktitle = {2019 {{IEEE Conference}} on {{Games}} ({{CoG}})},
  author = {Soemers, Dennis J. N. J. and Piette, {\'E}ric and Stephenson, Matthew and Browne, Cameron},
  year = {2019},
  month = aug,
  pages = {1--8},
  issn = {2325-4270},
  doi = {10.1109/CIG.2019.8848037},
  abstract = {In recent years, state-of-the-art game-playing agents often involve policies that are trained in self-playing processes where Monte Carlo tree search (MCTS) algorithms and trained policies iteratively improve each other. The strongest results have been obtained when policies are trained to mimic the search behaviour of MCTS by minimising a cross-entropy loss. Because MCTS, by design, includes an element of exploration, policies trained in this manner are also likely to exhibit a similar extent of exploration. In this paper, we are interested in learning policies for a project with future goals including the extraction of interpretable strategies, rather than state-of-the-art game-playing performance. For these goals, we argue that such an extent of exploration is undesirable, and we propose a novel objective function for training policies that are not exploratory. We derive a policy gradient expression for maximising this objective function, which can be estimated using MCTS value estimates, rather than MCTS visit counts. We empirically evaluate various properties of resulting policies, in a variety of board games.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Soemers et al. - 2019 - Learning Policies from Self-Play with Policy Gradi.pdf;C\:\\Users\\aesou\\Zotero\\storage\\4JAIUD66\\Soemers et al. - 2019 - Learning Policies from Self-Play with Policy Gradi.pdf;C\:\\Users\\aesou\\Zotero\\storage\\GSEWTY8I\\8848037.html},
  keywords = {cross-entropy loss,entropy,Feature extraction,game theory,game-playing agents,Games,learning (artificial intelligence),Linear programming,MCTS value estimates,Monte Carlo methods,Monte Carlo tree search algorithms,policy gradient expression,policy gradients,reinforcement learning,Reinforcement learning,search,self-play,Standards,Training,tree searching}
}

@phdthesis{soemers_tactical_2014,
  title = {Tactical {{Planning Using MCTS}} in the {{Game}} of {{StarCraft}}},
  author = {Soemers, Dennis},
  year = {2014},
  abstract = {This thesis describes how Monte-Carlo Tree Search (MCTS) can be applied to perform tactical planning for an intelligent agent playing full games of StarCraft: Brood War. StarCraft is a Real-Time Strategy game, which has a large state-space, is played in real-time, and commonly features two opposing players, capable of acting simultaneously. Using the MCTS algorithm for tactical planning is shown to increase the performance of the agent, compared to a scripted approach, when competing on a bot ladder. A combat model, based on Lanchester's Square Law, is described, and shown to achieve another gain in performance when used in Monte-Carlo simulations as replacement for a heuristic linear model. Finally, the MAST enhancement to the Playout Policy of MCTS is described, but it is found not to have a significant impact on the agent's performance.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\UC38H3JZ\\Soemers - 2014 - Tactical Planning Using MCTS in the Game of StarCr.pdf},
  language = {en},
  school = {Department of Knowledge Engineering, Maastricht University},
  type = {{{BsC}}}
}

@article{spina_1_nodate,
  title = {1 {{PUBLICATION}} 0 {{CITATIONS SEE PROFILE}}},
  author = {Spina, Sandro},
  pages = {9},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\paper_preprint.pdf},
  language = {en}
}

@incollection{spoerer_further_2014,
  title = {Further {{Investigations}} of 3-{{Member Simple Majority Voting}} for {{Chess}}},
  booktitle = {Computers and {{Games}}},
  author = {Spoerer, Kristian Toby and Okaneya, Toshihisa and Ikeda, Kokolo and Iida, Hiroyuki},
  year = {2014},
  volume = {8427},
  pages = {199--207},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-09165-5_17},
  abstract = {The 3-member simple majority voting is investigated for the game of Chess. The programs Stockfish, TogaII, andBobcat are used. Games are played against the strongest member of the group and against the group using simple majority voting. We show that the group is stronger than the strongest program. Subsequently, we investigate the research question, ``under what conditions is 3-member simple majority voting stronger than the strongest member?'' To answer this question we perform experiments on 27 groups. Statistics are gathered on the situations where the group outvoted the group leader. We found two conditions as an answer to the research question. First, group members should be almost equal in strength whilst still showing a small, but significant strength difference. Second, the denial percentage of the leaders candidate move depends on the strength of the members.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2013\\21 Further Investigations of 3-Member Simple Majority Voting for Chess.pdf},
  isbn = {978-3-319-09164-8 978-3-319-09165-5},
  language = {en}
}

@inproceedings{stanescu_evaluating_2016,
  title = {Evaluating Real-Time Strategy Game States Using Convolutional Neural Networks},
  booktitle = {2016 {{IEEE Conference}} on {{Computational Intelligence}} and {{Games}} ({{CIG}})},
  author = {Stanescu, Marius and Barriga, Nicolas A. and Hess, Andy and Buro, Michael},
  year = {2016},
  month = sep,
  publisher = {{IEEE}},
  address = {{Santorini, Greece}},
  doi = {10.1109/CIG.2016.7860439},
  abstract = {Real-time strategy (RTS) games, such as Blizzard's StarCraft, are fast paced war simulation games in which players have to manage economies, control many dozens of units, and deal with uncertainty about opposing unit locations in real-time. Even in perfect information settings, constructing strong AI systems has been difficult due to enormous state and action spaces and the lack of good state evaluation functions and high-level action abstractions. To this day, good human players are still handily defeating the best RTS game AI systems, but this may change in the near future given the recent success of deep convolutional neural networks (CNNs) in computer Go, which demonstrated how networks can be used for evaluating complex game states accurately and to focus look-ahead search.
In this paper we present a CNN for RTS game state evaluation
that goes beyond commonly used material based evaluations
by also taking spatial relations between units into account. We
evaluate the CNN's performance by comparing it with various
other evaluation functions by means of tournaments played by
several state-of-the-art search algorithms. We find that, despite
its much slower evaluation speed, on average the CNN based
search performs significantly better compared to simpler but
faster evaluations. These promising initial results together with
recent advances in hierarchical search suggest that dominating
human players in RTS games may not be far off.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\E3FBYN34\\Stanescu et al. - 2016 - Evaluating real-time strategy game states using co.pdf},
  isbn = {978-1-5090-1883-3},
  language = {en}
}

@inproceedings{stanescu_hierarchical_2014,
  title = {Hierarchical {{Adversarial Search Applied}} to {{Real}}-{{Time Strategy Games}}},
  booktitle = {Proceedings of the {{Tenth Annual AAAI Conference}} on {{Artificial Intelligence}} and {{Interactive Digital Entertainment}} ({{AIIDE}})},
  author = {Stanescu, Marius and Barriga, Nicolas A and Buro, Michael},
  year = {2014},
  pages = {66--72},
  abstract = {Real-Time Strategy (RTS) video games have proven to be a very challenging application area for artificial intelligence research. Existing AI solutions are limited by vast state and action spaces and real-time constraints. Most implementations efficiently tackle various tactical or strategic sub-problems, but there is no single algorithm fast enough to be successfully applied to big problem sets (such as a complete instance of the StarCraft RTS game). This paper presents a hierarchical adversarial search framework which more closely models the human way of thinking \textemdash{} much like the chain of command employed by the military. Each level implements a different abstraction \textemdash{} from deciding how to win the game at the top of the hierarchy to individual unit orders at the bottom. We apply a 3-layer version of our model to SparCraft \textemdash{} a StarCraft combat simulator \textemdash{} and show that it outperforms state of the art algorithms such as Alpha-Beta, UCT, and Portfolio Search in large combat scenarios featuring multiple bases and up to 72 mobile units per player under real-time constraints of 40 ms per search episode.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\D4P72HM2\\Stanescu et al. - 2014 - Hierarchical Adversarial Search Applied to Real-Ti.pdf},
  language = {en}
}

@inproceedings{stanescu_introducing_2014,
  title = {Introducing {{Hierarchical Adversarial Search}}, a {{Scalable Search Procedure}} for {{Real}}-Time {{Strategy Games}}},
  booktitle = {Proceedings of the {{Twenty}}-First {{European Conference}} on {{Artificial Intelligence}}},
  author = {Stanescu, Marius and Barriga, Nicolas A. and Buro, Michael},
  year = {2014},
  pages = {1099--1100},
  publisher = {{IOS Press}},
  address = {{Amsterdam, The Netherlands, The Netherlands}},
  doi = {10.3233/978-1-61499-419-0-1099},
  abstract = {Real-Time Strategy (RTS) video games have proven to be a very challenging application area for Artificial Intelligence research. Existing AI solutions are limited by vast state and action spaces and real-time constraints. Most implementations efficiently tackle various tactical or strategic sub-problems, but there is no single algorithm fast enough to be successfully applied to full RTS games. This paper introduces a hierarchical adversarial search framework which implements a different abstraction at each level \textemdash{} from deciding how to win the game at the top of the hierarchy to individual unit orders at the bottom.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\4U4VTN4W\\Stanescu et al. - 2014 - Introducing Hierarchical Adversarial Search, a Sca.pdf},
  isbn = {978-1-61499-418-3},
  series = {{{ECAI}}'14}
}

@phdthesis{stanescu_outcome_2019,
  title = {Outcome {{Prediction}} and {{Hierarchical Models}} in {{Real}}-{{Time Strategy Games}}},
  author = {Stanescu, Adrian Marius},
  year = {2019},
  abstract = {For many years, traditional boardgames such as Chess, Checkers or Go have been the standard environments to test new Artificial Intelligence (AI) algorithms for achieving robust game-playing agents capable of defeating the best human players. Presently, the focus has shifted towards games that offer even larger action and state spaces, such as Atari and other video games. 
With a unique combination of strategic thinking and fine-grained tactical combat management, Real-Time Strategy (RTS) games have emerged as one of the most popular and challenging research environments. Besides state space complexity, RTS properties such as simultaneous actions, partial observability and real-time computing constraints make them an excellent testbed for decision making algorithms under dynamic conditions.
This thesis makes contributions towards achieving human-level AI in these complex games. Specifically, we focus on  learning, using abstractions and performing adversarial search in real-time domains with extremely large action and state spaces, for which forward models might not be available. We present two abstract models for combat outcome prediction that are accurate while reasonably computationally inexpensive.
These models can inform high level strategic decisions such as when to force or avoid fighting or be used as evaluation functions for look-ahead search algorithms. In both cases we obtained stronger results compared to at the time state-of-the-art heuristics. We introduce two approaches to designing adversarial look-ahead search algorithms that are based on abstractions to reduce the search complexity.
Firstly, Hierarchical Adversarial Search uses multiple search layers that work
at different abstraction levels to decompose the original problem. Secondly, Puppet Search methods use configurable scripts as an action abstraction mechiianism and offer more design flexibility and control. Both methods show similar performance compared to top scripted and state-of-the-art search based agents in small maps, while outperforming them on larger ones. We show how to use Convolutional Neural Networks (CNNs) to effectively improve spatial awareness and evaluate game outcomes more accurately than our previous combat models. When incorporated into adversarial look-ahead search algorithms, this evaluation function increased their playing strength considerably. In these complex domains forward models might be very slow or even unavailable, which makes search methods more difficult to use. We show how policy networks can be used to mimic our Puppet Search algorithm and to bypass the need of a forward model during gameplay. We combine the much faster resulting method with other search-based tactical algorithms to produce RTS game playing agents that are stronger than state-of-the-art algorithms. We then describe how to eliminate the need for simulators or forward models entirely by using Reinforcement Learning (RL) to learn autonomous, self-improving behaviors. The resulting agents defeated the built-in AI convincingly and showed complex cooperative behaviors in small scale scenarios of a fully fledged RTS game.
Finally, learning becomes more difficult when controlling increasing numbers of agents. We introduce a new approach that uses CNNs to produce a spatial decomposition mechanism and makes credit assignment from a single team reward signal more tractable. Applied to a standard Q-learning method, this approach resulted in increased performance over the original algorithm in both small and large scale scenarios.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Stanescu - 2019 - Outcome Prediction and Hierarchical Models in Real.pdf},
  language = {en},
  school = {University of Alberta},
  type = {{{PhD}}}
}

@inproceedings{stanescu_predicting_2013,
  title = {Predicting {{Army Combat Outcomes}} in {{StarCraft}}},
  booktitle = {Proceedings of the {{Ninth AAAI Conference}} on {{Artificial Intelligence}} and {{Interactive Digital Entertainment}}},
  author = {Stanescu, Marius and Hernandez, Sergio Poo and Erickson, Graham and Greiner, Russel and Buro, Michael},
  year = {2013},
  pages = {7},
  abstract = {Smart decision making at the tactical level is important for Artificial Intelligence (AI) agents to perform well in the domain of real-time strategy (RTS) games. This paper presents a Bayesian model that can be used to predict the outcomes of isolated battles, as well as predict what units are needed to defeat a given army. Model parameters are learned from simulated battles, in order to minimize the dependency on player skill. We apply our model to the game of StarCraft, with the end-goal of using the predictor as a module for making high-level combat decisions, and show that the model is capable of making accurate predictions.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\AIV6UFV8\\Stanescu et al. - 2013 - Predicting Army Combat Outcomes in StarCraft.pdf},
  language = {en}
}

@article{stanescu_predicting_2016,
  title = {Predicting {{Opponent}}'s {{Production}} in {{Real}}-{{Time Strategy Games With Answer Set Programming}}},
  author = {Stanescu, Marius and Certicky, Michal},
  year = {2016},
  month = mar,
  volume = {8},
  pages = {89--94},
  issn = {1943-068X, 1943-0698},
  doi = {10.1109/TCIAIG.2014.2365414},
  abstract = {The adversarial character of real-time strategy (RTS) games is one of the main sources of uncertainty within this domain. Since players lack exact knowledge about their opponent's actions, they need a reasonable representation of alternative possibilities and their likelihood. In this article we propose a method of predicting the most probable combination of units produced by the opponent during a certain time period. We employ a logic programming paradigm called Answer Set Programming, since its semantics is well suited for reasoning with uncertainty and incomplete knowledge. In contrast with typical, purely probabilistic approaches, the presented method takes into account the background knowledge about the game and only considers the combinations that are consistent with the game mechanics and with the player's partial observations. Experiments, conducted during different phases of StarCraft: Brood War and Warcraft III: The Frozen Throne games, show that the prediction accuracy for time intervals of 1-3 minutes seems to be surprisingly high, making the method useful in practice. Rootmean-square error grows only slowly with increasing prediction intervals \textendash{} almost in a linear fashion.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\FL6BLJUW\\Stanescu and Certicky - 2016 - Predicting Opponent's Production in Real-Time Stra.pdf},
  journal = {IEEE Transactions on Computational Intelligence and AI in Games},
  language = {en},
  number = {1}
}

@inproceedings{stanescu_spatial_2018,
  title = {Spatial {{Action Decomposition Learning Applied}} to {{RTS Combat Games}}},
  booktitle = {The 2018 {{Workshop}} on {{Artificial Intelligence}} for {{Strategy Games AIIDE}}-18},
  author = {Stanescu, Marius and Buro, Michael},
  year = {2018},
  pages = {8},
  abstract = {Learning good policies for multi-agent systems is a complex task. Existing methods are often limited to a small number of agents, as learning becomes intractable when the agent number increases considerably. In this paper we describe Spatial Action Decomposition Learning that tries to overcome inefficiencies of standard multi-agent Q-learning methods by exploiting existing spatial action correlations. We apply our method to real-time strategy (RTS) game combat scenarios and show that Spatial Action Decomposition Learning based systems can outperform handcrafted scripts and policies optimized by independent Q-learning.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Stanescu and Buro - 2018 - Spatial Action Decomposition Learning Applied to R.pdf},
  language = {en}
}

@inproceedings{stanescu_using_2015,
  title = {Using {{Lanchester Attrition Laws}} for {{Combat Prediction}} in {{StarCraft}}},
  booktitle = {Proceedings, {{The Eleventh AAAI Conference}} on {{Artificial Intelligence}} and {{Interactive Digital Entertainment}} ({{AIIDE}}-15)},
  author = {Stanescu, Marius},
  year = {2015},
  pages = {86--92},
  abstract = {Smart decision making at the tactical level is important for Artificial Intelligence (AI) agents to perform well in the domain of real-time strategy (RTS) games. Winning battles is crucial in RTS games, and while humans can decide when and how to attack based on their experience, it is challenging for AI agents to estimate combat outcomes accurately.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\2JR29E7P\\Stanescu - 2015 - Using Lanchester Attrition Laws for Combat Predict.pdf},
  language = {en}
}

@book{steedman_syntactic_2000,
  title = {The Syntactic Process},
  author = {Steedman, Mark},
  year = {2000},
  publisher = {{MIT Press}},
  address = {{Cambridge, Mass}},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Steedman - 2000 - The syntactic process.pdf},
  isbn = {978-0-262-19420-4},
  keywords = {Grammar; Comparative and general,Syntax},
  language = {en},
  lccn = {P291 .S67 2000},
  series = {Language, Speech, and Communication}
}

@article{steedman_very_1996,
  title = {A {{Very Short Introduction}} to {{CCG}}},
  author = {Steedman, Mark},
  year = {1996},
  pages = {8},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\XUIQEJ8K\\Steedman - 1996 - A Very Short Introduction to CCG.pdf},
  language = {en}
}

@techreport{stoutamire_machine_1991,
  title = {Machine {{Learning}}, {{Game Play}}, and {{Go}}},
  author = {Stoutamire, David},
  year = {1991},
  abstract = {The game of go is an ideal problem domain for exploring machine learning: it is easy to define and there are many human experts, yet existing programs have failed to emulate their level of play to date. Existing literature on go playing programs and applications of machine learning to games are surveyed. An error function based on a database of master games is defined which is used to formulate the learning of go as an optimization problem. A classification technique called pattern preference is presented which is able to automatically derive patterns representative of good moves; a hashing technique allows pattern preference to run efficiently on conventional hardware with graceful degradation as memory size decreases.  Contents 1 Machine Learning 6  1.1 What is machine learning? : : : : : : : : : : : : : : : : : : : : : : : : : : 6 1.2 Learning as optimization : : : : : : : : : : : : : : : : : : : : : : : : : : : 7 1.3 The Bias of Generalization : : : : : : : : : : : : : : : : : : :...},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\HW5BXY7G\\Stoutamire - 1991 - Machine Learning, Game Play, and Go.pdf;C\:\\Users\\aesou\\Zotero\\storage\\ZTAIQGPM\\Stoutamire - 1991 - Machine Learning, Game Play, and Go.pdf;C\:\\Users\\aesou\\Zotero\\storage\\NH6GW64L\\summary.html}
}

@inproceedings{sturtevant_analysis_2008,
  title = {An {{Analysis}} of {{UCT}} in {{Multi}}-Player {{Games}}},
  booktitle = {Computers and {{Games}}},
  author = {Sturtevant, Nathan R.},
  editor = {{van den Herik}, H. Jaap and Xu, Xinhe and Ma, Zongmin and Winands, Mark H. M.},
  year = {2008},
  pages = {37--49},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-87608-3_4},
  abstract = {The UCT algorithm has been exceedingly popular for Go, a two-player game, significantly increasing the playing strength of Go programs in a very short time. This paper provides an analysis of the UCT algorithm in multi-player games, showing that UCT, when run in a multi-player game, is computing a mixed-strategy equilibrium, as opposed to maxn, which computes a pure-strategy equilibrium. We analyze the performance of UCT in several known domains and show that it performs as well or better than existing algorithms.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2008\\9 An Analysis of UCT in Multi-player Games.pdf;C\:\\Users\\aesou\\Zotero\\storage\\QCVE4NAR\\Sturtevant - 2008 - An Analysis of UCT in Multi-player Games.pdf},
  isbn = {978-3-540-87608-3},
  keywords = {Card Game,Game Tree,Opponent Model,Player Type,Random Play},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@incollection{sturtevant_feature_2007,
  title = {Feature {{Construction}} for {{Reinforcement Learning}} in {{Hearts}}},
  booktitle = {Computers and {{Games}}},
  author = {Sturtevant, Nathan R. and White, Adam M.},
  year = {2007},
  volume = {4630},
  pages = {122--134},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-75538-8_11},
  abstract = {Temporal difference (TD) learning has been used to learn strong evaluation functions in a variety of two-player games. TD-gammon illustrated how the combination of game tree search and learning methods can achieve grand-master level play in backgammon. In this work, we develop a player for the game of hearts, a 4-player game, based on stochastic linear regression and TD learning. Using a small set of basic game features we exhaustively combined features into a more expressive representation of the game state. We report initial results on learning with various combinations of features and training under self-play and against search-based players. Our simple learner was able to beat one of the best search-based hearts programs.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2006\\16 Feature Construction for Reinforcement Learning in Hearts.pdf},
  isbn = {978-3-540-75537-1 978-3-540-75538-8},
  language = {en}
}

@article{such_deep_2017,
  title = {Deep {{Neuroevolution}}: {{Genetic Algorithms Are}} a {{Competitive Alternative}} for {{Training Deep Neural Networks}} for {{Reinforcement Learning}}},
  shorttitle = {Deep {{Neuroevolution}}},
  author = {Such, Felipe Petroski and Madhavan, Vashisht and Conti, Edoardo and Lehman, Joel and Stanley, Kenneth O. and Clune, Jeff},
  year = {2017},
  month = dec,
  abstract = {Deep artificial neural networks (DNNs) are typically trained via gradient-based learning algorithms, namely backpropagation. Evolution strategies (ES) can rival backprop-based algorithms such as Q-learning and policy gradients on challenging deep reinforcement learning (RL) problems. However, ES can be considered a gradient-based algorithm because it performs stochastic gradient descent via an operation similar to a finite-difference approximation of the gradient. That raises the question of whether non-gradient-based evolutionary algorithms can work at DNN scales. Here we demonstrate they can: we evolve the weights of a DNN with a simple, gradient-free, populationbased genetic algorithm (GA) and it performs well on hard deep RL problems, including Atari and humanoid locomotion. The Deep GA successfully evolves networks with over four million free parameters, the largest neural networks ever evolved with a traditional evolutionary algorithm. These results (1) expand our sense of the scale at which GAs can operate, (2) suggest intriguingly that in some cases following the gradient is not the best choice for optimizing performance, and (3) make immediately available the multitude of neuroevolution techniques that improve performance. We demonstrate the latter by showing that combining DNNs with novelty search, which encourages exploration on tasks with deceptive or sparse reward functions, can solve a high-dimensional problem on which reward-maximizing algorithms (e.g. DQN, A3C, ES, and the GA) fail. Additionally, the Deep GA is faster than ES, A3C, and DQN (it can train Atari in {$\sim$}4 hours on one desktop or {$\sim$}1 hour distributed on 720 cores), and enables a stateof-the-art, up to 10,000-fold compact encoding technique.},
  archivePrefix = {arXiv},
  eprint = {1712.06567},
  eprinttype = {arxiv},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\T9JVCZIL\\Such et al. - 2017 - Deep Neuroevolution Genetic Algorithms Are a Comp.pdf},
  journal = {arXiv:1712.06567 [cs]},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  language = {en},
  primaryClass = {cs}
}

@incollection{sugiyama_optimistic_2011,
  title = {Optimistic {{Selection Rule Better Than Majority Voting System}}},
  booktitle = {Computers and {{Games}}},
  author = {Sugiyama, Takuya and Obata, Takuya and Hoki, Kunihito and Ito, Takeshi},
  year = {2011},
  volume = {6515},
  pages = {166--175},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-17928-0_16},
  abstract = {A recently proposed ensemble approach to game-tree search has attracted a great deal of attention. The ensemble system consists of M computer players, where each player uses a different series of pseudo-random numbers. A combination of multiple players under the majority voting system would improve the performance of a Shogi-playing computer. We present a new strategy of move selection based on the search values of a number of players. The move decision is made by selecting one player from all M players. Each move is selected by referring to the evaluation value of the tree search of each player. The performance and mechanism of the strategy are examined. We show that the optimistic selection rule, which selects the player that yields the highest evaluation value, outperforms the majority voting system. By grouping 16 or more computer players straightforwardly, the winning rates of the strongest Shogi programs increase from 50 to 60\% or even higher.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2010\\21 Optimistic Selection Rule Better Than Majority Voting System.pdf},
  isbn = {978-3-642-17927-3 978-3-642-17928-0},
  language = {en}
}

@article{sun_modified_2017,
  title = {Modified {{Adversarial Hierarchical Task Network Planning}} in {{Real}}-{{Time Strategy Games}}},
  author = {Sun, Lin and Jiao, Peng and Xu, Kai and Yin, Quanjun and Zha, Yabing},
  year = {2017},
  month = aug,
  volume = {7},
  pages = {872},
  issn = {2076-3417},
  doi = {10.3390/app7090872},
  abstract = {The application of artificial intelligence (AI) to real-time strategy (RTS) games includes considerable challenges due to the very large state spaces and branching factors, limited decision times, and dynamic adversarial environments involved. To address these challenges, hierarchical task network (HTN) planning has been extended to develop a method denoted as adversarial HTN (AHTN), and this method has achieved favorable performance. However, the HTN description employed cannot express complex relationships among tasks and accommodate the impacts of the environment on tasks. Moreover, AHTN cannot address task failures during plan execution. Therefore, this paper proposes a modified AHTN planning algorithm with failed task repair functionality denoted as AHTN-R. The algorithm extends the HTN description by introducing three additional elements: essential task, phase, and exit condition. If any task fails during plan execution, the AHTN-R algorithm identifies and terminates all affected tasks according to the extended HTN description, and applies a novel task repair strategy based on a prioritized listing of alternative plans to maintain the validity of the previous plan. In the planning process, AHTN-R generates the priorities of alternative plans by sorting all nodes of the game search tree according to their primary features. Finally, empirical results are presented based on a \textmu{}RTS game, and the performance of AHTN-R is compared to that of AHTN and to the performances of other state-of-the-art search algorithms developed for RTS games.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\77T27878\\Sun et al. - 2017 - Modified Adversarial Hierarchical Task Network Pla.pdf},
  journal = {Applied Sciences},
  language = {en},
  number = {9}
}

@article{sun_survey_2019,
  title = {A {{Survey}} of {{Optimization Methods From}} a {{Machine Learning Perspective}}},
  author = {Sun, Shiliang and Cao, Zehui and Zhu, Han and Zhao, Jing},
  year = {2019},
  pages = {1--14},
  issn = {2168-2275},
  doi = {10.1109/TCYB.2019.2950779},
  abstract = {Machine learning develops rapidly, which has made many theoretical breakthroughs and is widely applied in various fields. Optimization, as an important part of machine learning, has attracted much attention of researchers. With the exponential growth of data amount and the increase of model complexity, optimization methods in machine learning face more and more challenges. A lot of work on solving optimization problems or improving optimization methods in machine learning has been proposed successively. The systematic retrospect and summary of the optimization methods from the perspective of machine learning are of great significance, which can offer guidance for both developments of optimization and machine learning research. In this article, we first describe the optimization problems in machine learning. Then, we introduce the principles and progresses of commonly used optimization methods. Finally, we explore and give some challenges and open problems for the optimization in machine learning.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Sun et al. - 2019 - A Survey of Optimization Methods From a Machine Le.pdf;C\:\\Users\\aesou\\Zotero\\storage\\HMN5799Z\\Sun et al. - 2019 - A Survey of Optimization Methods From a Machine Le.pdf;C\:\\Users\\aesou\\Zotero\\storage\\DYJ7W8BW\\8903465.html},
  journal = {IEEE Transactions on Cybernetics},
  keywords = {Approximate Bayesian inference,deep neural network (DNN),machine learning,optimization method,reinforcement learning (RL)}
}

@article{sun_tstarbots_2018,
  title = {{{TStarBots}}: {{Defeating}} the {{Cheating Level Builtin AI}} in {{StarCraft II}} in the {{Full Game}}},
  shorttitle = {{{TStarBots}}},
  author = {Sun, Peng and Sun, Xinghai and Han, Lei and Xiong, Jiechao and Wang, Qing and Li, Bo and Zheng, Yang and Liu, Ji and Liu, Yongsheng and Liu, Han and Zhang, Tong},
  year = {2018},
  month = sep,
  abstract = {Starcraft II (SC2) is widely considered as the most challenging Real Time Strategy (RTS) game. The underlying challenges include a large observation space, a huge (continuous and infinite) action space, partial observations, simultaneous move for all players, and long horizon delayed rewards for local decisions. To push the frontier of AI research, Deepmind and Blizzard jointly developed the StarCraft II Learning Environment (SC2LE) as a testbench of complex decision making systems. SC2LE provides a few mini games such as MoveToBeacon, CollectMineralShards, and DefeatRoaches, where some AI agents have achieved the performance level of human professional players. However, for full games, the current AI agents are still far from achieving human professional level performance. To bridge this gap, we present two full game AI agents in this paper \textemdash{} the AI agent TStarBot1 is based on deep reinforcement learning over a flat action structure, and the AI agent TStarBot2 is based on hard-coded rules over a hierarchical action structure. Both TStarBot1 and TStarBot2 are able to defeat the built-in AI agents from level 1 to level 10 in a full game (1v1 Zerg-vs-Zerg game on the AbyssalReef map), noting that level 8, level 9, and level 10 are cheating agents with unfair advantages such as full vision on the whole map and resource harvest boosting 1. To the best of our knowledge, this is the first public work to investigate AI agents that can defeat the built-in AI in the StarCraft II full game.},
  archivePrefix = {arXiv},
  eprint = {1809.07193},
  eprinttype = {arxiv},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\BKE48JEM\\Sun et al. - 2018 - TStarBots Defeating the Cheating Level Builtin AI.pdf},
  journal = {arXiv:1809.07193 [cs]},
  keywords = {Computer Science - Artificial Intelligence},
  language = {en},
  primaryClass = {cs}
}

@article{sun_tstarbots_2018-1,
  title = {{{TStarBots}}: {{Defeating}} the {{Cheating Level Builtin AI}} in {{StarCraft II}} in the {{Full Game}}},
  shorttitle = {{{TStarBots}}},
  author = {Sun, Peng and Sun, Xinghai and Han, Lei and Xiong, Jiechao and Wang, Qing and Li, Bo and Zheng, Yang and Liu, Ji and Liu, Yongsheng and Liu, Han and Zhang, Tong},
  year = {2018},
  month = dec,
  abstract = {Starcraft II (SC2) is widely considered as the most challenging Real Time Strategy (RTS) game. The underlying challenges include a large observation space, a huge (continuous and infinite) action space, partial observations, simultaneous move for all players, and long horizon delayed rewards for local decisions. To push the frontier of AI research, Deepmind and Blizzard jointly developed the StarCraft II Learning Environment (SC2LE) as a testbench of complex decision making systems. SC2LE provides a few mini games such as MoveToBeacon, CollectMineralShards, and DefeatRoaches, where some AI agents have achieved the performance level of human professional players. However, for full games, the current AI agents are still far from achieving human professional level performance. To bridge this gap, we present two full game AI agents in this paper - the AI agent TStarBot1 is based on deep reinforcement learning over a flat action structure, and the AI agent TStarBot2 is based on hard-coded rules over a hierarchical action structure. Both TStarBot1 and TStarBot2 are able to defeat the built-in AI agents from level 1 to level 10 in a full game (1v1 Zerg-vs-Zerg game on the AbyssalReef map), noting that level 8, level 9, and level 10 are cheating agents with unfair advantages such as full vision on the whole map and resource harvest boosting. To the best of our knowledge, this is the first public work to investigate AI agents that can defeat the built-in AI in the StarCraft II full game.},
  archivePrefix = {arXiv},
  eprint = {1809.07193},
  eprinttype = {arxiv},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\VN9TKDC6\\Sun et al. - 2018 - TStarBots Defeating the Cheating Level Builtin AI.pdf;C\:\\Users\\aesou\\Zotero\\storage\\W4F4866Q\\1809.html},
  journal = {arXiv:1809.07193 [cs]},
  keywords = {Computer Science - Artificial Intelligence},
  primaryClass = {cs}
}

@inproceedings{synnaeve_bayesian_2011,
  title = {A {{Bayesian Model}} for {{Plan Recognition}} in {{RTS Games Applied}} to {{StarCraft}}},
  booktitle = {Seventh {{Artificial Intelligence}} and {{Interactive Digital Entertainment Conference}}},
  author = {Synnaeve, Gabriel and Bessi{\`e}re, Pierre},
  year = {2011},
  month = oct,
  abstract = {The task of keyhole (unobtrusive) plan recognition is central to adaptive game AI. \&ldquo;Tech trees\&rdquo; or \&ldquo;build trees\&rdquo; are the core of real-time strategy (RTS) game strategic (long term) planning. This paper presents a generic and simple Bayesian model for RTS build tree prediction from noisy observations, which parameters are learned from replays (game logs). This unsupervised machine learning approach involves minimal work for the game developers as it leverage players\&rsquo; data (com- mon in RTS). We applied it to StarCraft1 and showed that it yields high quality and robust predictions, that can feed an adaptive AI.},
  copyright = {Authors who publish a paper in this conference agree to the following terms:     Author(s) agree to transfer their copyrights in their article/paper to the Association for the Advancement of Artificial Intelligence (AAAI), in order to deal with future requests for reprints, translations, anthologies, reproductions, excerpts, and other publications. This grant will include, without limitation, the entire copyright in the article/paper in all countries of the world, including all renewals, extensions, and reversions thereof, whether such rights current exist or hereafter come into effect, and also the exclusive right to create electronic versions of the article/paper, to the extent that such right is not subsumed under copyright.   The author(s) warrants that they are the sole author and owner of the copyright in the above article/paper, except for those portions shown to be in quotations; that the article/paper is original throughout; and that the undersigned right to make the grants set forth above is complete and unencumbered   The author(s) agree that if anyone brings any claim or action alleging facts that, if true, constitute a breach of any of the foregoing warranties, the author(s) will hold harmless and indemnify AAAI, their grantees, their licensees, and their distributors against any liability, whether under judgment, decree, or compromise, and any legal fees and expenses arising out of that claim or actions, and the undersigned will cooperate fully in any defense AAAI may make to such claim or action. Moreover, the undersigned agrees to cooperate in any claim or other action seeking to protect or enforce any right the undersigned has granted to AAAI in the article/paper. If any such claim or action fails because of facts that constitute a breach of any of the foregoing warranties, the undersigned agrees to reimburse whomever brings such claim or action for expenses and attorneys' fees incurred therein.   Author(s) retain all proprietary rights other than copyright (such as patent rights).   Author(s) may make personal reuse of all or portions of the above article/paper in other works of their own authorship.   Author(s) may reproduce, or have reproduced, their article/paper for the author's personal use, or for company use provided that AAAI copyright and the source are indicated, and that the copies are not used in a way that implies AAAI endorsement of a product or service of an employer, and that the copies per se are not offered for sale. The foregoing right shall not permit the posting of the article/paper in electronic or digital form on any computer network, except by the author or the author's employer, and then only on the author's or the employer's own web page or ftp site. Such web page or ftp site, in addition to the aforementioned requirements of this Paragraph, must provide an electronic reference or link back to the AAAI electronic server, and shall not post other AAAI copyrighted materials not of the author's or the employer's creation (including tables of contents with links to other papers) without AAAI's written permission.   Author(s) may make limited distribution of all or portions of their article/paper prior to publication.   In the case of work performed under U.S. Government contract, AAAI grants the U.S. Government royalty-free permission to reproduce all or portions of the above article/paper, and to authorize others to do so, for U.S. Government purposes.   In the event the above article/paper is not accepted and published by AAAI, or is withdrawn by the author(s) before acceptance by AAAI, this agreement becomes null and void.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Synnaeve and Bessière - 2011 - A Bayesian Model for Plan Recognition in RTS Games.pdf;C\:\\Users\\aesou\\Zotero\\storage\\CN9IS56I\\Synnaeve and Bessière - 2011 - A Bayesian Model for Plan Recognition in RTS Games.pdf;C\:\\Users\\aesou\\Zotero\\storage\\T2IPMWVX\\4062.html},
  language = {en}
}

@phdthesis{synnaeve_bayesian_2012,
  title = {{Bayesian programming and learning for multi-player video games: application to RTS AI}},
  author = {Synnaeve, Gabriel},
  year = {2012},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\RKY9T9GQ\\Synnaeve - Programmation et apprentissage bayésien pour les j.pdf},
  language = {fr},
  school = {Universit\'e de Grenoble},
  type = {{PhD}}
}

@article{synnaeve_multiscale_2016,
  title = {Multiscale {{Bayesian Modeling}} for {{RTS Games}}: {{An Application}} to {{StarCraft AI}}},
  shorttitle = {Multiscale {{Bayesian Modeling}} for {{RTS Games}}},
  author = {Synnaeve, Gabriel and Bessiere, Pierre},
  year = {2016},
  month = dec,
  volume = {8},
  pages = {338--350},
  issn = {1943-068X, 1943-0698},
  doi = {10.1109/TCIAIG.2015.2487743},
  abstract = {This paper showcases the use of Bayesian models for real-time strategy (RTS) games AI in three distinct corecomponents: micro-management (units control), tactics (army moves and positions), and strategy (economy, technology, production, army types). The strength of having end-to-end probabilistic models is that distributions on specific variables can be used to inter-connect different models at different levels of abstraction. We applied this modeling to StarCraft, and evaluated each model independently. Along the way, we produced and released a comprehensive dataset for RTS machine learning.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\2NDKR42N\\Synnaeve and Bessiere - 2016 - Multiscale Bayesian Modeling for RTS Games An App 2.pdf},
  journal = {IEEE Transactions on Computational Intelligence and AI in Games},
  language = {en},
  number = {4}
}

@article{synnaeve_torchcraft_2016,
  title = {{{TorchCraft}}: A {{Library}} for {{Machine Learning Research}} on {{Real}}-{{Time Strategy Games}}},
  shorttitle = {{{TorchCraft}}},
  author = {Synnaeve, Gabriel and Nardelli, Nantas and Auvolat, Alex and Chintala, Soumith and Lacroix, Timoth{\'e}e and Lin, Zeming and Richoux, Florian and Usunier, Nicolas},
  year = {2016},
  month = nov,
  abstract = {We present TorchCraft, a library that enables deep learning research on Real-Time Strategy (RTS) games such as StarCraft: Brood War, by making it easier to control these games from a machine learning framework, here Torch [9]. This white paper argues for using RTS games as a benchmark for AI research, and describes the design and components of TorchCraft.},
  archivePrefix = {arXiv},
  eprint = {1611.00625},
  eprinttype = {arxiv},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\J6PI8PHZ\\Synnaeve et al. - 2016 - TorchCraft a Library for Machine Learning Researc.pdf},
  journal = {arXiv:1611.00625 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,I.2.1},
  language = {en},
  primaryClass = {cs}
}

@article{szlam_why_2019,
  title = {Why {{Build}} an {{Assistant}} in {{Minecraft}}?},
  author = {Szlam, Arthur and Gray, Jonathan and Srinet, Kavya and Jernite, Yacine and Joulin, Armand and Synnaeve, Gabriel and Kiela, Douwe and Yu, Haonan and Chen, Zhuoyuan and Goyal, Siddharth and Guo, Demi and Rothermel, Danielle and Zitnick, C. Lawrence and Weston, Jason},
  year = {2019},
  month = jul,
  abstract = {In this document we describe a rationale for a research program aimed at building an open ``assistant'' in the game Minecraft, in order to make progress on the problems of natural language understanding and learning from dialogue.},
  archivePrefix = {arXiv},
  eprint = {1907.09273},
  eprinttype = {arxiv},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Szlam et al. - 2019 - Why Build an Assistant in Minecraft.pdf},
  journal = {arXiv:1907.09273 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  language = {en},
  primaryClass = {cs}
}

@article{tabibi_verified_2002,
  title = {{{VERIFIED NULL}}-{{MOVE PRUNING}}},
  author = {Tabibi, Omid David and Netanyahu, Nathan S.},
  year = {2002},
  month = jan,
  volume = {25},
  pages = {153--161},
  publisher = {{IOS Press}},
  issn = {1389-6911},
  doi = {10.3233/ICG-2002-25305},
  abstract = {In this article we review standard null-move pruning and introduce our extended version of it, which we call verified null-move pruning. In verified null-move pruning, whenever the shallow null-move search indicates a fail-high, instead of cutting of},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Tabibi and Netanyahu - 2002 - VERIFIED NULL-MOVE PRUNING.pdf;C\:\\Users\\aesou\\Zotero\\storage\\3P23VKF7\\icg25305.html},
  journal = {ICGA Journal},
  language = {en},
  number = {3}
}

@inproceedings{tang_review_2018,
  title = {A {{Review}} of {{Computational Intelligence}} for {{StarCraft AI}}},
  booktitle = {2018 {{IEEE Symposium Series}} on {{Computational Intelligence}} ({{SSCI}})},
  author = {Tang, Zhentao and Shao, Kun and Zhu, Yuanheng and Li, Dong and Zhao, Dongbin and Huang, Tingwen},
  year = {2018},
  month = nov,
  pages = {1167--1173},
  publisher = {{IEEE}},
  address = {{Bangalore, India}},
  doi = {10.1109/SSCI.2018.8628682},
  abstract = {After artificial intelligent (AI) scientists have conquered Go game, StarCraft has been the next biggest challenge. A highly intelligent AI system that is able to beat human professional players is expected. In this paper, we review the recent development of computational intelligence (CI) in the field of StarCraft AI. Successful applications of CI techniques are analyzed and compared from different levels of AI functionality. It should be noted that current StarCraft AI is highly dependent on human experience and is far away from a completely intelligent one. New frameworks and techniques are still expected to improve the intelligence.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Tang et al. - 2018 - A Review of Computational Intelligence for StarCra.pdf},
  isbn = {978-1-5386-9276-9},
  language = {en}
}

@inproceedings{tavares_evolving_2014,
  title = {Evolving {{Swarm Intelligence}} for {{Task Allocation}} in a {{Real Time Strategy Game}}},
  booktitle = {2014 {{Brazilian Symposium}} on {{Computer Games}} and {{Digital Entertainment}}},
  author = {Tavares, Anderson R. and Azpurua, Hector and Chaimowicz, Luiz},
  year = {2014},
  month = nov,
  pages = {99--108},
  publisher = {{IEEE}},
  address = {{Porto Alegre, Brazil}},
  doi = {10.1109/SBGAMES.2014.17},
  abstract = {Real time strategy games are complex scenarios where multiple agents must be coordinated in a dynamic, partially observable environment. In this work, we model the coordination of these agents as a task allocation problem, in which specific tasks are given to the agents that are more suited to execute them. We employ a task allocation algorithm based on swarm intelligence and adjust its parameters using a genetic algorithm. To evaluate this approach, we implement this coordination mechanism in the AI of a popular video game: StarCraft: BroodWar. Experiment results show that the genetic algorithm enhances performance of the task allocation algorithm. Besides, performance of the proposed approach in matches against StarCraft's native AI is comparable to that of a tournament-level software-controlled player for StarCraft.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\GFAP5ZYG\\Tavares et al. - 2014 - Evolving Swarm Intelligence for Task Allocation in.pdf},
  isbn = {978-1-4799-8065-9},
  language = {en}
}

@inproceedings{tavares_rock_2016,
  title = {Rock, {{Paper}}, {{StarCraft}}: {{Strategy Selection}} in {{Real}}-{{Time Strategy Games}}},
  booktitle = {Proceedings, {{The Twelfth AAAI Conference}} on {{Artificial Intelligence}} and {{Interactive Digital Entertainment}} ({{AIIDE}})},
  author = {Tavares, Anderson and Azpurua, Hector and Santos, Amanda and Chaimowicz, Luiz},
  year = {2016},
  pages = {7},
  abstract = {The correct choice of strategy is crucial for a successful realtime strategy (RTS) game player. Generally speaking, a strategy determines the sequence of actions the player will take in order to defeat his/her opponents. In this paper we present a systematic study of strategy selection in the popular RTS game StarCraft. We treat the choice of strategy as a game itself and test several strategy selection techniques, including Nash Equilibrium and safe opponent exploitation. We adopt a subset of AIIDE 2015 StarCraft AI tournament bots as the available strategies and our results suggest that it is useful to deviate from Nash Equilibrium to exploit sub-optimal opponents on strategy selection, confirming insights from computer rock-paper-scissors tournaments.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\LKG3XVCP\\Tavares et al. - 2016 - Rock, Paper, StarCraft Strategy Selection in Real.pdf},
  language = {en}
}

@inproceedings{tavares_tabular_2018,
  title = {Tabular {{Reinforcement Learning}} in {{Real}}-{{Time Strategy Games}} via {{Options}}},
  booktitle = {2018 {{IEEE Conference}} on {{Computational Intelligence}} and {{Games}} ({{CIG}})},
  author = {Tavares, Anderson R and Chaimowicz, Luiz},
  year = {2018},
  abstract = {Real-Time Strategy (RTS) games are complex domains with huge state and action spaces. In such games, humans usually pursue long-term plans, which take long sequences of actions to achieve. In this work, we implement this behavior by reasoning over options, which are temporally-extended actions in Markov Decision Processes. Our options are defined with the aid of a state aggregation scheme and a portfolio of game-playing algorithms. Experimentally, we show that our approach leverages the capabilities of traditional reinforcement-learning techniques, which become competitive against state-of-the-art search methods in \textmu{}RTS.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\5Y8LSUSV\\Tavares and Chaimowicz - 2018 - Tabular Reinforcement Learning in Real-Time Strate.pdf},
  language = {en}
}

@article{taylor_reinforcement_2014,
  title = {Reinforcement Learning Agents Providing Advice in Complex Video Games},
  author = {Taylor, Matthew E. and Carboni, Nicholas and Fachantidis, Anestis and Vlahavas, Ioannis and Torrey, Lisa},
  year = {2014},
  month = jan,
  volume = {26},
  pages = {45--63},
  issn = {0954-0091, 1360-0494},
  doi = {10.1080/09540091.2014.885279},
  abstract = {This article introduces a teacher-student framework for reinforcement learning, synthesizing and extending material that appeared in conference proceedings [22] and in a non-archival workshop paper [6]. In this framework, a teacher agent instructs a student agent by suggesting actions the student should take as it learns. However, the teacher may only give such advice a limited number of times. We present several novel algorithms that teachers can use to budget their advice effectively, and we evaluate them in two complex video games: StarCraft and Pac-Man. Our results show that the same amount of advice, given at different moments, can have different effects on student learning, and that teachers can significantly affect student learning even when students use different learning methods and state representations.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\TFXJ8XUD\\Taylor et al. - 2014 - Reinforcement learning agents providing advice in .pdf},
  journal = {Connection Science},
  language = {en},
  number = {1}
}

@incollection{thiele_analysis_2016,
  title = {An {{Analysis}} of {{Majority Systems}} with {{Dependent Agents}} in a {{Simple Subtraction Game}}},
  booktitle = {Computers and {{Games}}},
  author = {Thiele, Raphael and Alth{\"o}fer, Ingo},
  year = {2016},
  volume = {10068},
  pages = {202--211},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-50935-8_19},
  abstract = {It is common knowledge that a majority system is typically better than its components, when the components are stochastically independent. However, in practice the independency assumption is often not justified. We investigate systems of experts which are constituted by couples of dependent agents. Based on recent theoretical work we analyse their performance in a simple 2-player subtraction game. It turns out that systems with negatively correlated couples perform better than those with positive correlation within the couples. From computer chess practice it was at least known that systems of very positively correlated bots were not too successful.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2016\\23 An Analysis of Majority Systems with Dependent Agents in a Simple Subtraction Game.pdf},
  isbn = {978-3-319-50934-1 978-3-319-50935-8},
  language = {en}
}

@book{thomopoulos_essentials_2013,
  title = {Essentials of {{Monte Carlo}} Simulation: Statistical Methods for Building Simulation Models},
  shorttitle = {Essentials of {{Monte Carlo}} Simulation},
  author = {Thomopoulos, Nicholas T.},
  year = {2013},
  publisher = {{Springer}},
  address = {{New York}},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Thomopoulos - 2013 - Essentials of Monte Carlo simulation statistical .pdf},
  isbn = {978-1-4614-6021-3 978-1-4614-6022-0},
  keywords = {Monte Carlo method},
  language = {en},
  lccn = {QA298 .T497 2013}
}

@inproceedings{thorne_procdefense_2017,
  title = {{{ProcDefense}} -- {{A Game Framework}} for {{Procedural Player Skill Training}}},
  booktitle = {The {{AIIDE}}-17 {{Workshop}} on {{Experimental AI}} in {{Games WS}}-17-19},
  author = {Thorne, Brandon R and Nelakkutti, Hiru and Reinhart, Joseph and Jhala, Arnav},
  year = {2017},
  pages = {3},
  abstract = {A challenge of game design is in providing affordances to players so that they can learn and improve their skills. Advances in Procedural Content Generation (PCG) suggest this type of game content is a candidate for automatic creation. Some work in PCG has been successful in customizing game difficulty to achieve desired player experience; however, this often involves bringing the difficulty of the game to a level appropriate for the player's current skills. Players desiring to improve their performance in a particular game may be willing to tolerate relatively higher levels of frustration and anxiety than are targeted in experience-based approaches. As an initial step in this line of inquiry, we introduce ProcDefense, an action game with a modular difficulty control interface, as a platform for future inquiry into the effectiveness of differing PCG techniques for performance-training, dynamic difficulty adjustment.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\ZWANUSY9\\Thorne et al. - 2017 - ProcDefense -- A Game Framework for Procedural Pla.pdf},
  language = {en}
}

@inproceedings{tian_elf:_2017,
  title = {{{ELF}}: {{An Extensive}}, {{Lightweight}} and {{Flexible Research Platform}} for {{Real}}-Time {{Strategy Games}}},
  booktitle = {31st {{Conference}} on {{Neural Information Processing Systems}} ({{NIPS}} 2017), {{Long Beach}}, {{CA}}, {{USA}}.},
  author = {Tian, Yuandong and Gong, Qucheng and Shang, Wenling and Wu, Yuxin and Zitnick, C Lawrence},
  year = {2017},
  abstract = {In this paper, we propose ELF, an Extensive, Lightweight and Flexible platform for fundamental reinforcement learning research. Using ELF, we implement a highly customizable real-time strategy (RTS) engine with three game environments (Mini-RTS, Capture the Flag and Tower Defense). Mini-RTS, as a miniature version of StarCraft, captures key game dynamics and runs at 40K frameper-second (FPS) per core on a laptop. When coupled with modern reinforcement learning methods, the system can train a full-game bot against built-in AIs endto-end in one day with 6 CPUs and 1 GPU. In addition, our platform is flexible in terms of environment-agent communication topologies, choices of RL methods, changes in game parameters, and can host existing C/C++-based game environments like ALE [4]. Using ELF, we thoroughly explore training parameters and show that a network with Leaky ReLU [17] and Batch Normalization [11] coupled with long-horizon training and progressive curriculum beats the rule-based built-in AI more than 70\% of the time in the full game of Mini-RTS. Strong performance is also achieved on the other two games. In game replays, we show our agents learn interesting strategies. ELF, along with its RL platform, is open sourced at https://github.com/facebookresearch/ELF.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\Z658PHSW\\Tian et al. - 2017 - ELF An Extensive, Lightweight and Flexible Resear.pdf},
  language = {en}
}

@article{tian_evolutionary_2020,
  title = {An {{Evolutionary Algorithm}} for {{Large}}-{{Scale Sparse Multiobjective Optimization Problems}}},
  author = {Tian, Ye and Zhang, Xingyi and Wang, Chao and Jin, Yaochu},
  year = {2020},
  month = apr,
  volume = {24},
  pages = {380--393},
  issn = {1941-0026},
  doi = {10.1109/TEVC.2019.2918140},
  abstract = {In the last two decades, a variety of different types of multiobjective optimization problems (MOPs) have been extensively investigated in the evolutionary computation community. However, most existing evolutionary algorithms encounter difficulties in dealing with MOPs whose Pareto optimal solutions are sparse (i.e., most decision variables of the optimal solutions are zero), especially when the number of decision variables is large. Such large-scale sparse MOPs exist in a wide range of applications, for example, feature selection that aims to find a small subset of features from a large number of candidate features, or structure optimization of neural networks whose connections are sparse to alleviate overfitting. This paper proposes an evolutionary algorithm for solving large-scale sparse MOPs. The proposed algorithm suggests a new population initialization strategy and genetic operators by taking the sparse nature of the Pareto optimal solutions into consideration, to ensure the sparsity of the generated solutions. Moreover, this paper also designs a test suite to assess the performance of the proposed algorithm for large-scale sparse MOPs. The experimental results on the proposed test suite and four application examples demonstrate the superiority of the proposed algorithm over seven existing algorithms in solving large-scale sparse MOPs.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Tian et al. - 2020 - An Evolutionary Algorithm for Large-Scale Sparse M.pdf;C\:\\Users\\aesou\\Zotero\\storage\\DQPGA7I3\\8720021.html},
  journal = {IEEE Transactions on Evolutionary Computation},
  keywords = {decision variables,Evolutionary algorithm,evolutionary algorithms,Evolutionary computation,evolutionary computation community,evolutionary neural network,Feature extraction,feature selection,genetic algorithms,large-scale multiobjective optimization (MOP),large-scale sparse multiobjective optimization problems,Neural networks,Pareto optimal solutions,Pareto optimisation,Pareto optimization,search problems,Sociology,sparse nature,sparse Pareto optimal solutions,structure optimization,Training},
  number = {2}
}

@article{tian_evolutionary_2020-1,
  title = {An {{Evolutionary Algorithm}} for {{Large}}-{{Scale Sparse Multiobjective Optimization Problems}}},
  author = {Tian, Ye and Zhang, Xingyi and Wang, Chao and Jin, Yaochu},
  year = {2020},
  month = apr,
  volume = {24},
  pages = {380--393},
  issn = {1089-778X, 1089-778X, 1941-0026},
  doi = {10.1109/TEVC.2019.2918140},
  journal = {IEEE Transactions on Evolutionary Computation},
  language = {en},
  number = {2}
}

@inproceedings{togelius_multiobjective_2010,
  title = {Multiobjective Exploration of the {{StarCraft}} Map Space},
  booktitle = {Proceedings of the 2010 {{IEEE Conference}} on {{Computational Intelligence}} and {{Games}}},
  author = {Togelius, Julian and Preuss, Mike and Beume, Nicola and Wessing, Simon and Hagelback, Johan and Yannakakis, Georgios N.},
  year = {2010},
  month = aug,
  pages = {265--272},
  publisher = {{IEEE}},
  address = {{Copenhagen, Denmark}},
  doi = {10.1109/ITW.2010.5593346},
  abstract = {This paper presents a search-based method for generating maps for the popular real-time strategy (RTS) game StarCraft. We devise a representation of StarCraft maps suitable for evolutionary search, along with a set of fitness functions based on predicted entertainment value of those maps, as derived from theories of player experience. A multiobjective evolutionary algorithm is then used to evolve complete StarCraft maps based on the representation and selected fitness functions. The output of this algorithm is a Pareto front approximation visualizing the tradeoff between the several fitness functions used, and where each point on the front represents a viable map. We argue that this method is useful for both automatic and machine-assisted map generation, and in particular that the Pareto fronts are excellent design support tools for human map designers.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Togelius et al. - 2010 - Multiobjective exploration of the StarCraft map sp.pdf},
  isbn = {978-1-4244-6295-7},
  language = {en}
}

@incollection{tom_computational_2011,
  title = {Computational {{Experiments}} with the {{RAVE Heuristic}}},
  booktitle = {Computers and {{Games}}},
  author = {Tom, David and M{\"u}ller, Martin},
  year = {2011},
  volume = {6515},
  pages = {69--80},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-17928-0_7},
  abstract = {The Monte-Carlo tree search algorithm Upper Confidence bounds applied to Trees (UCT) has become extremely popular in computer games research. The Rapid Action Value Estimation (RAVE) heuristic is a strong estimator that often improves the performance of UCT-based algorithms. However, there are situations where RAVE misleads the search whereas pure UCT search can find the correct solution. Two games, the simple abstract game Sum of Switches (SOS) and the game of Go, are used to study the behavior of the RAVE heuristic. In SOS, RAVE updates are manipulated to mimic game situations where RAVE misleads the search. Such false RAVE updates are used to create RAVE overestimates and underestimates. A study of the distributions of mean and RAVE values reveals great differences between Go and SOS. While the RAVE-max update rule is able to correct extreme cases of RAVE underestimation, it is not effective in closer to practical settings and in Go.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2010\\12 Computational Experiments with the RAVE Heuristic.pdf},
  isbn = {978-3-642-17927-3 978-3-642-17928-0},
  language = {en}
}

@article{tom_quality-based_2014,
  title = {Quality-Based {{Rewards}} for {{Monte}}-{{Carlo Tree Search Simulations}}},
  author = {Tom, Pepels and J.W, Tak Mandy and Marc, Lanctot and M, Winands Mark H.},
  year = {2014},
  pages = {705--710},
  publisher = {{IOS Press}},
  address = {{Netherlands}},
  issn = {0922-6389},
  doi = {10.3233/978-1-61499-419-0-705},
  abstract = {Monte-Carlo Tree Search is a best-first search technique based on simulations to sample the state space of a decision-making problem. In games, positions are evaluated based on estimates obtained from rewards of numerous randomized play-outs. Generally, rewards from play-outs are discrete values representing the outcome of the game (loss, draw, or win), e.g., r {$\in$} \{-1, 0, 1\}, which are backpropagated from expanded leaf nodes to the root node. However, a play-out may provide additional information. In this paper, we introduce new measures for assessing the a posteriori quality of a simulation. We show that altering the rewards of play-outs based on their assessed quality improves results in six distinct two-player games and in the General Game Playing agent CADIAPLAYER. We propose two specific enhancements, the Relative Bonus and Qualitative Bonus. Both are used as control variates, a variance reduction method for statistical simulation. Relative Bonus is based on the number of moves made during a simulation and Qualitative Bonus relies on a domain-dependent assessment of the game's terminal state. We show that the proposed enhancements, both separate and combined, lead to significant performance increases in the domains discussed.},
  copyright = {\textcopyright{}2014 \&copy; The authors and IOS Press.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Tom et al. - 2014 - Quality-based Rewards for Monte-Carlo Tree Search .pdf},
  journal = {Frontiers in Artificial Intelligence and Applications},
  language = {en}
}

@inproceedings{tong_enhancing_2019,
  title = {Enhancing {{Rolling Horizon Evolution}} with {{Policy}} and {{Value Networks}}},
  booktitle = {2019 {{IEEE Conference}} on {{Games}} ({{CoG}})},
  author = {Tong, Xin and Liu, Weiming and Li, Bin},
  year = {2019},
  month = aug,
  pages = {1--8},
  issn = {2325-4270},
  doi = {10.1109/CIG.2019.8848041},
  abstract = {Rolling Horizon Evolutionary Algorithm (RHEA) is an online planning method for real-time game playing; its performance is closely related to the planning horizon and the search cost allowed. In this paper, we propose to learn a prior for RHEA in an offline manner by training a value network and a policy network. The value network is used to reduce the planning horizon by providing an estimation of future rewards, and the policy network is used to initialize the population, which helps to narrow down the search scope. The proposed algorithm, named prior-based RHEA (p-RHEA), trains policy and value networks by performing planning and learning iteratively. In the planning stage, the horizon-limited search is performed to improve the policies and collect training samples with the help of the learned networks. In the learning stage, the policy network and value network are trained with the collected samples to learn better prior knowledge. Experimental results on OpenAI MuJoCo tasks show that the performance of the proposed p- RHEA is significantly improved compared to that of RHEA.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Tong et al. - 2019 - Enhancing Rolling Horizon Evolution with Policy an.pdf;C\:\\Users\\aesou\\Zotero\\storage\\LQJSHK68\\8848041.html},
  keywords = {computer games,Covariance Matrix Adaptation Evolution Strategy,evolutionary computation,Games,horizon evolutionary algorithm,horizon-limited search,Knowledge engineering,learned networks,learning (artificial intelligence),learning stage,MuJoCo tasks,Neural networks,online planning method,OpenAI MuJoCo tasks,p-RHEA,Planning,planning (artificial intelligence),planning horizon,planning stage,policy network,real-time game playing,Reinforcement Learning,rolling horizon evolution,Rolling Horizon Evolutionary Algorithm,search cost,search problems,Sociology,Statistics,Training,value network,value networks}
}

@incollection{tromp_combinatorics_2007,
  title = {Combinatorics of {{Go}}},
  booktitle = {Computers and {{Games}}},
  author = {Tromp, John and Farneb{\"a}ck, Gunnar},
  year = {2007},
  volume = {4630},
  pages = {84--99},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-75538-8_8},
  abstract = {We present several results concerning the number of positions and games of Go. We derive recurrences for L(m, n), the number of legal positions on an m \texttimes{} n board, and develop a dynamic programming algorithm which computes L(m, n) in time O(m3n2{$\lambda$}m) and space O(m{$\lambda$}m), for some constant {$\lambda$} {$<$} 5.4. An implementation of this algorithm enables us to list L(n, n) for n {$\leq$} 17. For larger boards, we prove the existence of a base of liberties lim mn L(m, n) of {$\sim$} 2.9757341920433572493. Based on a conjecture about vanishing error terms, we derive an asymptotic formula for L(m, n), which is shown to be highly accurate.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2006\\13 Combinatorics of Go.pdf},
  isbn = {978-3-540-75537-1 978-3-540-75538-8},
  language = {en}
}

@incollection{tromp_number_2016,
  title = {The {{Number}} of {{Legal Go Positions}}},
  booktitle = {Computers and {{Games}}},
  author = {Tromp, John},
  year = {2016},
  volume = {10068},
  pages = {183--190},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-50935-8_17},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2016\\21 The Number of Legal Go Positions.pdf},
  isbn = {978-3-319-50934-1 978-3-319-50935-8},
  language = {en}
}

@inproceedings{tyagi_role_2020,
  title = {Role of {{AI}} in {{Gaming}} and {{Simulation}}},
  booktitle = {Proceeding of the {{International Conference}} on {{Computer Networks}}, {{Big Data}} and {{IoT}} ({{ICCBI}} - 2019)},
  author = {Tyagi, Shivam and Sengupta, Sudhriti},
  editor = {Pandian, A. Pasumpon and Palanisamy, Ram and Ntalianis, Klimis},
  year = {2020},
  pages = {259--266},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-43192-1_29},
  abstract = {The overall purpose pursued in this paper is to learn more about gaming and simulation in general and how artificial intelligence is being implemented into them and all the possibilities for future in this scenery. The mechanism of simulation and gaming go hand in hand where artificial intelligence has been coming in clutch to be related these two in all the aspects we can think of. All the possibilities for the AI in simulation have been discussed along with gaming while learning about the problems we face when trying to pester AI in applications as it obviously may have some drawbacks too. All the different roads that can be taken with these combinations are something the paper has focused on. The trend of virtual reality has taken off recently and thus it is only logical that AI may be implemented into that too therefore we discuss that here. AI helps in optimization of programs and also saves labor as the AI can be taught to learn and work alone. AI can imitate human ways while avoiding the drawbacks a human may have. Mapping environments, making levels and characters are just about some examples that are being helped by AI in gaming while in simulation the AI helps in human interaction and creating scenarios while trying to ignore the danger that an actual experiment might have against a simulation. Social realism in gaming and simulation is just another important thing that is discussed as it is something really important to have that if an AI can learn then we might go a long way.},
  isbn = {978-3-030-43192-1},
  keywords = {AI (artificial intelligence),Bio-sensing,Simulation,VR (virtual reality)},
  language = {en},
  series = {Lecture {{Notes}} on {{Data Engineering}} and {{Communications Technologies}}}
}

@incollection{ueda_weak_2008,
  title = {Weak {{Proof}}-{{Number Search}}},
  booktitle = {Computers and {{Games}}},
  author = {Ueda, Toru and Hashimoto, Tsuyoshi and Hashimoto, Junichi and Iida, Hiroyuki},
  year = {2008},
  volume = {5131},
  pages = {157--168},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-87608-3_15},
  abstract = {The paper concerns an AND/OR-tree search algorithm to solve hard problems. Proof-number search is a well-known powerful search algorithm for that purpose. Its depth-first variants such as PN*, PDS, and df-pn work very well, in particular in the domain of shogi mating problems. However, there are still possible drawbacks. The most prevailing one is the double-counting problem. To handle this problem the paper proposes a new search idea using proof number and branching factor as search estimators. We call the new method Weak Proof-Number Search. The experiments performed in the domain of shogi and Othello show that the proposed search algorithm is potentially more powerful than the original proof-number search or its depth-first variants.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2008\\20 Weak Proof-Number Search.pdf},
  isbn = {978-3-540-87607-6 978-3-540-87608-3},
  language = {en}
}

@inproceedings{uiterwijk_11_2016,
  title = {11~x~11 {{Domineering Is Solved}}: {{The First Player Wins}}},
  shorttitle = {11~\$\$\textbackslash{}times \$\$~11 {{Domineering Is Solved}}},
  booktitle = {Computers and {{Games}}},
  author = {Uiterwijk, Jos W. H. M.},
  editor = {Plaat, Aske and Kosters, Walter and {van den Herik}, Jaap},
  year = {2016},
  pages = {129--136},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-50935-8_12},
  abstract = {We have developed a program called MUDoS (Maastricht University Domineering Solver) that solves Domineering positions in a very efficient way. It enables the solution of known positions (up to the 10\texttimes{}1010\texttimes{}1010\textbackslash{}times 10 board) to be much quicker.More importantly, it enables the solution of 11\texttimes{}1111\texttimes{}1111\textbackslash{}times 11 Domineering, a board size that up till now was far out of the reach of previous Domineering solvers. The solution needed the investigation of 259,689,994,008 nodes, using almost half a year of computation time on a single simple desktop computer. The results show that under optimal play the first player wins 11\texttimes{}1111\texttimes{}1111\textbackslash{}times 11 Domineering, irrespective whether Vertical or Horizontal starts.In addition, several other new boards were solved. Using the convention that Vertical starts, the 8\texttimes{}158\texttimes{}158\textbackslash{}times 15, 11\texttimes{}911\texttimes{}911\textbackslash{}times 9, 12\texttimes{}812\texttimes{}812\textbackslash{}times 8, 12\texttimes{}1512\texttimes{}1512\textbackslash{}times 15, 14\texttimes{}814\texttimes{}814\textbackslash{}times 8, and 17\texttimes{}617\texttimes{}617\textbackslash{}times 6 boards are all won by Vertical, whereas the 6\texttimes{}176\texttimes{}176\textbackslash{}times 17, 8\texttimes{}128\texttimes{}128\textbackslash{}times 12, 9\texttimes{}119\texttimes{}119\textbackslash{}times 11, and 11\texttimes{}1011\texttimes{}1011\textbackslash{}times 10 boards are all won by Horizontal.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2016\\16 1111 Domineering Is Solved The First Player Wins.pdf},
  isbn = {978-3-319-50935-8},
  keywords = {Board Size,Knowledge Rule,Outcome Class,Replacement Scheme,Standard Desktop Computer},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@incollection{ura_comparison_2014,
  title = {Comparison {{Training}} of {{Shogi Evaluation Functions}} with {{Self}}-{{Generated Training Positions}} and {{Moves}}},
  booktitle = {Computers and {{Games}}},
  author = {Ura, Akira and Miwa, Makoto and Tsuruoka, Yoshimasa and Chikayama, Takashi},
  year = {2014},
  volume = {8427},
  pages = {208--220},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-09165-5_18},
  abstract = {Automated tuning of parameters in computer game playing is an important technique for building strong computer programs. Comparison training is a supervised learning method for tuning the parameters of an evaluation function. It has proven to be effective in the game of Chess and Shogi. The training method requires a large number of training positions and moves extracted from game records of human experts; however, the number of such game records is limited. In this paper, we propose a practical approach to create additional training data for comparison training by using the program itself. We investigate three methods for generating additional positions and moves. Then we evaluate them using a Shogi program. Experimental results show that the self-generated training data can improve the playing strength of the program.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2013\\22 Comparison Training of Shogi Evaluation Functions with Self-Generated Training Positions and Moves.pdf},
  isbn = {978-3-319-09164-8 978-3-319-09165-5},
  language = {en}
}

@phdthesis{uriarte_adversarial_2017,
  title = {Adversarial {{Search}} and {{Spatial Reasoning}}  in {{Real Time Strategy Games}}},
  author = {Uriarte, Alberto},
  year = {2017},
  abstract = {For many years, Chess was the standard game to test new Arti cial Intelligence (AI) algorithms for achieving robust game-playing agents capable of defeating the best human players. Nowadays, games
like Go or Poker are used since they o er new challenges like larger state spaces, or non-determinism.
Among these testbed games, Real-Time Strategy (RTS) games have raised as one of the most
challenging. The unique properties of RTS games (simultaneous and durative actions, large state
spaces, partial observability) make them a perfect scenario to test algorithms able to make decisions
in dynamic and complex situations. This thesis makes a contribution towards achieving human-level
AI in these complex games. Speci cally, I focus on the problems of performing adversarial search in
domains (1) with extremely large decision and state spaces, (2) where no forward model is available,
and (3) the game state is partially observable. Additionally, I also study how spatial reasoning can
be used to reduce the search space and to improve the RTS playing bots.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\GPUQA6NV\\Uriarte - 2017 - Adversarial Search and Spatial Reasoning  in Real .pdf},
  language = {en},
  school = {Drexel Univeristy},
  type = {{{PhD}}}
}

@inproceedings{uriarte_automatic_2015,
  title = {Automatic {{Learning}} of {{Combat Models}} for {{RTS Games}}},
  booktitle = {Eleventh {{Artificial Intelligence}} and {{Interactive Digital Entertainment Conference}}},
  author = {Uriarte, Alberto and Onta{\~n}{\'o}n, Santiago},
  year = {2015},
  month = sep,
  abstract = {Game tree search algorithms, such as Monte Carlo Tree Search (MCTS), require access to a forward model (or "simulator") of the game at hand. However, in some games such forward model is not readily available. In this paper we address the problem of automatically learning forward models (more specifically, combats models) for two-player attrition games. We report experiments comparing several approaches to learn such combat model from replay data to models generated by hand. We use StarCraft, a Real-Time Strategy (RTS) game, as our application domain. Specifically, we use a large collection of already collected replays, and focus on learning a combat model for tactical combats.},
  copyright = {Authors who publish a paper in this conference agree to the following terms:     Author(s) agree to transfer their copyrights in their article/paper to the Association for the Advancement of Artificial Intelligence (AAAI), in order to deal with future requests for reprints, translations, anthologies, reproductions, excerpts, and other publications. This grant will include, without limitation, the entire copyright in the article/paper in all countries of the world, including all renewals, extensions, and reversions thereof, whether such rights current exist or hereafter come into effect, and also the exclusive right to create electronic versions of the article/paper, to the extent that such right is not subsumed under copyright.   The author(s) warrants that they are the sole author and owner of the copyright in the above article/paper, except for those portions shown to be in quotations; that the article/paper is original throughout; and that the undersigned right to make the grants set forth above is complete and unencumbered   The author(s) agree that if anyone brings any claim or action alleging facts that, if true, constitute a breach of any of the foregoing warranties, the author(s) will hold harmless and indemnify AAAI, their grantees, their licensees, and their distributors against any liability, whether under judgment, decree, or compromise, and any legal fees and expenses arising out of that claim or actions, and the undersigned will cooperate fully in any defense AAAI may make to such claim or action. Moreover, the undersigned agrees to cooperate in any claim or other action seeking to protect or enforce any right the undersigned has granted to AAAI in the article/paper. If any such claim or action fails because of facts that constitute a breach of any of the foregoing warranties, the undersigned agrees to reimburse whomever brings such claim or action for expenses and attorneys' fees incurred therein.   Author(s) retain all proprietary rights other than copyright (such as patent rights).   Author(s) may make personal reuse of all or portions of the above article/paper in other works of their own authorship.   Author(s) may reproduce, or have reproduced, their article/paper for the author's personal use, or for company use provided that AAAI copyright and the source are indicated, and that the copies are not used in a way that implies AAAI endorsement of a product or service of an employer, and that the copies per se are not offered for sale. The foregoing right shall not permit the posting of the article/paper in electronic or digital form on any computer network, except by the author or the author's employer, and then only on the author's or the employer's own web page or ftp site. Such web page or ftp site, in addition to the aforementioned requirements of this Paragraph, must provide an electronic reference or link back to the AAAI electronic server, and shall not post other AAAI copyrighted materials not of the author's or the employer's creation (including tables of contents with links to other papers) without AAAI's written permission.   Author(s) may make limited distribution of all or portions of their article/paper prior to publication.   In the case of work performed under U.S. Government contract, AAAI grants the U.S. Government royalty-free permission to reproduce all or portions of the above article/paper, and to authorize others to do so, for U.S. Government purposes.   In the event the above article/paper is not accepted and published by AAAI, or is withdrawn by the author(s) before acceptance by AAAI, this agreement becomes null and void.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Uriarte and Ontañón - 2015 - Automatic Learning of Combat Models for RTS Games.pdf;C\:\\Users\\aesou\\Zotero\\storage\\PMRCDHJP\\Uriarte and Ontañón - 2015 - Automatic Learning of Combat Models for RTS Games.pdf;C\:\\Users\\aesou\\Zotero\\storage\\VWW4WZG4\\11516.html},
  language = {en}
}

@article{uriarte_combat_2016,
  title = {Combat {{Models}} for {{RTS Games}}},
  author = {Uriarte, Alberto and Onta{\~n}{\'o}n, Santiago},
  year = {2016},
  month = may,
  abstract = {Game tree search algorithms, such as Monte Carlo Tree Search (MCTS), require access to a forward model (or "simulator") of the game at hand. However, in some games such forward model is not readily available. This paper presents three forward models for two-player attrition games, which we call "combat models", and show how they can be used to simulate combat in RTS games. We also show how these combat models can be learned from replay data. We use StarCraft as our application domain. We report experiments comparing our combat models predicting a combat output and their impact when used for tactical decisions during a real game.},
  archivePrefix = {arXiv},
  eprint = {1605.05305},
  eprinttype = {arxiv},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\84ZFEMZH\\Uriarte and Ontañón - 2016 - Combat Models for RTS Games.pdf},
  journal = {arXiv:1605.05305 [cs]},
  keywords = {Computer Science - Artificial Intelligence},
  language = {en},
  primaryClass = {cs}
}

@inproceedings{uriarte_game-tree_2014,
  title = {Game-{{Tree Search}} over {{High}}-{{Level Game States}} in {{RTS Games}}},
  booktitle = {Tenth {{Artificial Intelligence}} and {{Interactive Digital Entertainment}} ({{AIIDE}}) {{Conference}}},
  author = {Uriarte, Alberto and Onta{\~n}{\'o}n, Santiago},
  year = {2014},
  pages = {73--79},
  abstract = {From an AI point of view, Real-Time Strategy (RTS) games are hard because they have enormous state spaces, they are real-time and partially observable. In this paper, we present an approach to deploy gametree search in RTS games by using game state abstraction. We propose a high-level abstract representation of the game state, that significantly reduces the branching factor when used for game-tree search algorithms. Using this high-level representation, we evaluate versions of alpha-beta search and of Monte Carlo Tree Search (MCTS). We present experiments in the context of StarCraft showing promising results in dealing with the large branching factors present in RTS games.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Uriarte and Ontañón - 2014 - Game-Tree Search over High-Level Game States in RT.pdf},
  language = {en}
}

@inproceedings{uriarte_high-level_2014,
  title = {High-Level {{Representations}} for {{Game}}-{{Tree Search}} in {{RTS Games}}},
  booktitle = {Tenth {{Artificial Intelligence}} and {{Interactive Digital Entertainment Conference}}, {{Artificial Intelligence}} in {{Adversarial Real}}-{{Time Games Workshop}}},
  author = {Uriarte, Alberto and Onta{\~n}{\'o}n, Santiago},
  year = {2014},
  pages = {14--18},
  abstract = {From an AI point of view, Real-Time Strategy (RTS) games are hard because they have enormous state spaces, they are real-time and partially observable. In this paper, we explore an approach to deploy gametree search in RTS games by using game state abstraction, and explore the effect of using different abstractions over the game state. Different abstractions capture different parts of the game state, and result in different branching factors when used for game-tree search algorithms. We evaluate the different representations using Monte Carlo Tree Search in the context of StarCraft.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Uriarte and Ontañón - 2014 - High-level Representations for Game-Tree Search in.pdf},
  language = {en}
}

@inproceedings{uriarte_improving_2016,
  title = {Improving {{Monte Carlo Tree Search Policies}} in {{StarCraft}} via {{Probabilistic Models Learned}} from {{Replay Data}}},
  booktitle = {Proceedings, {{The Twelfth AAAI Conference}} on {{Artificial Intelligence}} and {{Interactive Digital Entertainment}} ({{AIIDE}}-16)},
  author = {Uriarte, Alberto and Onta{\~n}{\'o}n, Santiago},
  year = {2016},
  pages = {7},
  abstract = {Applying game-tree search techniques to RTS games poses a significant challenge, given the large branching factors involved. This paper studies an approach to incorporate knowledge learned offline from game replays to guide the search process. Specifically, we propose to learn Naive Bayesian models predicting the probability of action execution in different game states, and use them to inform the search process of Monte Carlo Tree Search. We evaluate the effect of incorporating these models into several Multiarmed Bandit policies for MCTS in the context of STARCRAFT, showing a significant improvement in gameplay performance.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\D36P7MHK\\Uriarte and Ontañón - 2016 - Improving Monte Carlo Tree Search Policies in Star.pdf},
  language = {en}
}

@inproceedings{uriarte_kiting_2012,
  title = {Kiting in {{RTS Games Using Influence Maps}}},
  booktitle = {Artificial {{Intelligence}} in {{Adversarial Real}}-{{Time Games}}: {{Papers}} from the 2012 {{AIIDE Workshop AAAI Technical Report WS}}-12-15},
  author = {Uriarte, Alberto and Onta{\~n}{\'o}n, Santiago},
  year = {2012},
  pages = {6},
  abstract = {Influence Maps have been successfully used in controlling the navigation of multiple units. In this paper, we apply the idea to the problem of simulating a kiting behavior (also known as ``attack and flee'') in the context of real-time strategy (RTS) games. We present our approach and evaluate it in the popular RTS game StarCraft, where we analyze the benefits that our approach brings to a StarCraft playing bot.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\9JTI7MV6\\Uriarte and Ontañón - 2012 - Kiting in RTS Games Using Influence Maps.pdf},
  language = {en}
}

@inproceedings{uriarte_single_2017,
  title = {Single Believe State Generation for Partially Observable Real-Time Strategy Games},
  booktitle = {2017 {{IEEE Conference}} on {{Computational Intelligence}} and {{Games}} ({{CIG}})},
  author = {Uriarte, Alberto and Onta{\~n}{\'o}n, Santiago},
  year = {2017},
  month = aug,
  pages = {296--303},
  publisher = {{IEEE}},
  address = {{New York, NY, USA}},
  doi = {10.1109/CIG.2017.8080449},
  abstract = {Real-Time Strategy (RTS) games pose a big challenge due their large branching factor and real-time nature. This challenge is even bigger if we consider partially observable RTS games due to the fog-of-war. This paper focuses on extending Monte Carlo Tree Search (MCTS) algorithms for RTS games to consider partially observable settings. Specifically, we investigate sampling a single believe state consistent with a perfect memory of all the past observations in the current game, and using it to perform MCTS. We evaluate the performance of this approach in the \textmu{}RTS game simulator, showing that the performance of this approach is only between 8\%-15\% lower than if we could observe the entire game state (e.g., by cheating).},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\7B6LGNED\\Uriarte and Ontañón - 2017 - Single believe state generation for partially obse.pdf},
  isbn = {978-1-5386-3233-8},
  language = {en}
}

@article{usunier_episodic_2016,
  title = {Episodic {{Exploration}} for {{Deep Deterministic Policies}}: {{An Application}} to {{StarCraft Micromanagement Tasks}}},
  shorttitle = {Episodic {{Exploration}} for {{Deep Deterministic Policies}}},
  author = {Usunier, Nicolas and Synnaeve, Gabriel and Lin, Zeming and Chintala, Soumith},
  year = {2016},
  month = sep,
  abstract = {We consider scenarios from the real-time strategy game StarCraft as new benchmarks for reinforcement learning algorithms. We propose micromanagement tasks, which present the problem of the short-term, low-level control of army members during a battle. From a reinforcement learning point of view, these scenarios are challenging because the stateaction space is very large, and because there is no obvious feature representation for the state-action evaluation function. We describe our approach to tackle the micromanagement scenarios with deep neural network controllers from raw state features given by the game engine. In addition, we present a heuristic reinforcement learning algorithm which combines direct exploration in the policy space and backpropagation. This algorithm allows for the collection of traces for learning using deterministic policies, which appears much more efficient than, for example, -greedy exploration. Experiments show that with this algorithm, we successfully learn non-trivial strategies for scenarios with armies of up to 15 agents, where both Q-learning and REINFORCE struggle.},
  archivePrefix = {arXiv},
  eprint = {1609.02993},
  eprinttype = {arxiv},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\3VPS8VU4\\Usunier et al. - 2016 - Episodic Exploration for Deep Deterministic Polici.pdf},
  journal = {arXiv:1609.02993 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,I.2.1,I.2.6},
  language = {en},
  primaryClass = {cs}
}

@inproceedings{van_den_herik_investigations_2014,
  title = {Investigations with {{Monte Carlo Tree Search}} for {{Finding Better Multivariate Horner Schemes}}},
  booktitle = {Agents and {{Artificial Intelligence}}},
  author = {{van den Herik}, H. Jaap and Kuipers, Jan and Vermaseren, Jos A. M. and Plaat, Aske},
  editor = {Filipe, Joaquim and Fred, Ana},
  year = {2014},
  pages = {3--20},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-662-44440-5_1},
  abstract = {After a computer chess program had defeated the human World Champion in 1997, many researchers turned their attention to the oriental game of Go. It turned out that the minimax approach, so successful in chess, did not work in Go. Instead, after some ten years of intensive research, a new method was developed: MCTS (Monte Carlo Tree Search), with promising results. MCTS works by averaging the results of random play-outs. At first glance it is quite surprising that MCTS works so well. However, deeper analysis revealed the reasons.The success of MCTS in Go caused researchers to apply the method to other domains. In this article we report on experiments with MCTS for finding improved orderings for multivariate Horner schemes, a basic method for evaluating polynomials. We report on initial results, and continue with an investigation into two parameters that guide the MCTS search. Horner's rule turns out to be a fruitful testbed for MCTS, allowing easy experimentation with its parameters. The results reported here provide insight into how and why MCTS works. It will be interesting to see if these insights can be transferred to other domains, for example, back to Go.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\van den Herik et al. - 2014 - Investigations with Monte Carlo Tree Search for Fi.pdf;C\:\\Users\\aesou\\Zotero\\storage\\IQJLXL22\\van den Herik et al. - 2014 - Investigations with Monte Carlo Tree Search for Fi.pdf},
  isbn = {978-3-662-44440-5},
  keywords = {Artificial intelligence,Chess,Go,High energy physics,Horners rule,Monte Carlo Tree Search},
  language = {en},
  series = {Communications in {{Computer}} and {{Information Science}}}
}

@incollection{van_lankveld_extraversion_2011,
  title = {Extraversion in {{Games}}},
  booktitle = {Computers and {{Games}}},
  author = {{van Lankveld}, Giel and Schreurs, Sonny and Spronck, Pieter and {van den Herik}, Jaap},
  year = {2011},
  volume = {6515},
  pages = {263--275},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-17928-0_24},
  abstract = {The behavior of a human player in a game expresses the personality of that player. Personality is an important characteristic for modeling the player's profile. In our research we use the five factor model of personality, in which extraversion is a notable factor. Extraversion is the human tendency of being sensitive to rewards. This often results in humans seeking socially rewarding situations. Extraversion plays a prominent part in the in-game behavior of a player. The in-game behavior can be decomposed in 20 different in-game elements.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2010\\29 Extraversion in Games.pdf},
  isbn = {978-3-642-17927-3 978-3-642-17928-0},
  language = {en}
}

@incollection{van_opheusden_people_2016,
  title = {Do {{People Think Like Computers}}?},
  booktitle = {Computers and {{Games}}},
  author = {{van Opheusden}, Bas and Bnaya, Zahy and Galbiati, Gianni and Ma, Wei Ji},
  year = {2016},
  volume = {10068},
  pages = {212--224},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-50935-8_20},
  abstract = {Human cognition inspired the earliest algorithms for gameplaying computer programs. However, the studies of human and computer game play quickly diverged: the Artificial Intelligence community focused on theory and techniques to solve games, while behavioral scientists empirically examined simple decision-making in humans. In this paper, we combine concepts and methods from the two fields to investigate whether human and AI players take similar approaches in an adversarial combinatorial game. We develop and compare five models that capture human behavior. We then demonstrate that our models can predict behavior in two related tasks. To conclude, we use our models to describe what makes a strong human player.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2016\\24 Do People Think Like Computers.pdf},
  isbn = {978-3-319-50934-1 978-3-319-50935-8},
  language = {en}
}

@inproceedings{verduga_palencia_psyrts_2019,
  title = {{{PsyRTS}}: A {{Web Platform}} for {{Experiments}} in {{Human Decision}}-{{Making}} in {{RTS Environments}}},
  shorttitle = {{{PsyRTS}}},
  booktitle = {2019 {{IEEE Conference}} on {{Games}} ({{CoG}})},
  author = {Verduga Palencia, Denis Omar and Osman, Magda},
  year = {2019},
  month = aug,
  pages = {1--4},
  issn = {2325-4270},
  doi = {10.1109/CIG.2019.8848101},
  abstract = {This paper presents PsyRTS: an open-source web-platform designed to create psychological experiments using a dynamic environment based on real-time strategy games. This platform has characteristics present in Real-Time Strategy (RTS) games and allows the researcher to manipulate variables regarding visibility, resource availability and presence of other agents while at the same time enabling human participation through existing online platforms.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Verduga Palencia and Osman - 2019 - PsyRTS a Web Platform for Experiments in Human De.pdf;C\:\\Users\\aesou\\Zotero\\storage\\SDCKMTPC\\8848101.html},
  keywords = {Artificial intelligence,computer games,Crowdsourcing,crowdsourcing.,decision making,Decision making,decision-making,dynamic environment,exploration-exploitation,Games,heuristics,human decision-making,human participation,Internet,online platforms,open-source Web-platform,player modeling,psychological experiments,psychology,Psychology,PsyRTS,public domain software,real-time strategy games,Real-time systems,Reliability,resource availability,RTS environments}
}

@book{vick_emotion_2008,
  title = {Emotion Notions: Modeling Personality in Game Character {{AI}}},
  shorttitle = {Emotion Notions},
  author = {Vick, Erik Henry},
  year = {2008},
  publisher = {{Course Technology/Cengage learning}},
  address = {{Australia}},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\UZBEEXZC\\Vick - 2008 - Emotion notions modeling personality in game char.pdf},
  isbn = {978-1-59863-527-0},
  keywords = {Artificial intelligence,Computer games,Programming},
  language = {en},
  lccn = {QA76.76.C672 V53 2008}
}

@article{vinyals_grandmaster_2019,
  title = {Grandmaster Level in {{StarCraft II}} Using Multi-Agent Reinforcement Learning},
  author = {Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M. and Mathieu, Micha{\"e}l and Dudzik, Andrew and Chung, Junyoung and Choi, David H. and Powell, Richard and Ewalds, Timo and Georgiev, Petko and Oh, Junhyuk and Horgan, Dan and Kroiss, Manuel and Danihelka, Ivo and Huang, Aja and Sifre, Laurent and Cai, Trevor and Agapiou, John P. and Jaderberg, Max and Vezhnevets, Alexander S. and Leblond, R{\'e}mi and Pohlen, Tobias and Dalibard, Valentin and Budden, David and Sulsky, Yury and Molloy, James and Paine, Tom L. and Gulcehre, Caglar and Wang, Ziyu and Pfaff, Tobias and Wu, Yuhuai and Ring, Roman and Yogatama, Dani and W{\"u}nsch, Dario and McKinney, Katrina and Smith, Oliver and Schaul, Tom and Lillicrap, Timothy and Kavukcuoglu, Koray and Hassabis, Demis and Apps, Chris and Silver, David},
  year = {2019},
  month = nov,
  volume = {575},
  pages = {350--354},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/s41586-019-1724-z},
  abstract = {AlphaStar uses a multi-agent reinforcement learning algorithm and has reached Grandmaster level, ranking among the top 0.2\% of human players for the real-time strategy game StarCraft II.},
  copyright = {2019 The Author(s), under exclusive licence to Springer Nature Limited},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Vinyals et al. - 2019 - Grandmaster level in StarCraft II using multi-agen.pdf;C\:\\Users\\aesou\\Zotero\\storage\\9GFQB4B4\\s41586-019-1724-z.html},
  journal = {Nature},
  language = {en},
  number = {7782}
}

@article{vinyals_starcraft_2017-1,
  title = {{{StarCraft II}}: {{A New Challenge}} for {{Reinforcement Learning}}},
  shorttitle = {{{StarCraft II}}},
  author = {Vinyals, Oriol and Ewalds, Timo and Bartunov, Sergey and Georgiev, Petko and Vezhnevets, Alexander Sasha and Yeo, Michelle and Makhzani, Alireza and K{\"u}ttler, Heinrich and Agapiou, John and Schrittwieser, Julian and Quan, John and Gaffney, Stephen and Petersen, Stig and Simonyan, Karen and Schaul, Tom and {van Hasselt}, Hado and Silver, David and Lillicrap, Timothy and Calderone, Kevin and Keet, Paul and Brunasso, Anthony and Lawrence, David and Ekermo, Anders and Repp, Jacob and Tsing, Rodney},
  year = {2017},
  month = aug,
  abstract = {This paper introduces SC2LE (StarCraft II Learning Environment), a reinforcement learning environment based on the game StarCraft II. This domain poses a new grand challenge for reinforcement learning, representing a more difficult class of problems than considered in most prior work. It is a multi-agent problem with multiple players interacting; there is imperfect information due to a partially observed map; it has a large action space involving the selection and control of hundreds of units; it has a large state space that must be observed solely from raw input feature planes; and it has delayed credit assignment requiring long-term strategies over thousands of steps. We describe the observation, action, and reward specification for the StarCraft II domain and provide an open source Python-based interface for communicating with the game engine. In addition to the main game maps, we provide a suite of mini-games focusing on different elements of StarCraft II gameplay. For the main game maps, we also provide an accompanying dataset of game replay data from human expert players. We give initial baseline results for neural networks trained from this data to predict game outcomes and player actions. Finally, we present initial baseline results for canonical deep reinforcement learning agents applied to the StarCraft II domain. On the mini-games, these agents learn to achieve a level of play that is comparable to a novice player. However, when trained on the main game, these agents are unable to make significant progress. Thus, SC2LE offers a new and challenging environment for exploring deep reinforcement learning algorithms and architectures.},
  archivePrefix = {arXiv},
  eprint = {1708.04782},
  eprinttype = {arxiv},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Vinyals et al. - 2017 - StarCraft II A New Challenge for Reinforcement Le 2.pdf},
  journal = {arXiv:1708.04782 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  language = {en},
  primaryClass = {cs}
}

@inproceedings{vodopivec_enhancing_2014,
  title = {Enhancing Upper Confidence Bounds for Trees with Temporal Difference Values},
  booktitle = {2014 {{IEEE Conference}} on {{Computational Intelligence}} and {{Games}}},
  author = {Vodopivec, Tom and {\v S}ter, Branko},
  year = {2014},
  month = aug,
  pages = {1--8},
  issn = {2325-4270},
  doi = {10.1109/CIG.2014.6932895},
  abstract = {Upper confidence bounds for trees (UCT) is one of the most popular and generally effective Monte Carlo tree search (MCTS) algorithms. However, in practice it is relatively weak when not aided by additional enhancements. Improving its performance without reducing generality is a current research challenge. We introduce a new domain-independent UCT enhancement based on the theory of reinforcement learning. Our approach estimates state values in the UCT tree by employing temporal difference (TD) learning, which is known to outperform plain Monte Carlo sampling in certain domains. We present three adaptations of the TD({$\lambda$}) algorithm to the UCT's tree policy and backpropagation step. Evaluations on four games (Gomoku, Hex, Connect Four, and Tic Tac Toe) reveal that our approach increases UCT's level of play comparably to the rapid action value estimation (RAVE) enhancement. Furthermore, it proves highly compatible with a modified all moves as first heuristic, where it considerably outperforms RAVE. The findings suggest that integration of TD learning into MCTS deserves further research, which may form a new class of MCTS enhancements.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Vodopivec and Šter - 2014 - Enhancing upper confidence bounds for trees with t.pdf;C\:\\Users\\aesou\\Zotero\\storage\\9TA9TSJV\\6932895.html},
  keywords = {backpropagation,Benchmark testing,Complexity theory,connect four game,domain-independent UCT enhancement,game theory,Games,gomoku game,hex game,MCTS algorithms,Monte Carlo methods,Monte Carlo sampling,Monte Carlo tree search algorithms,Optimization,Radiation detectors,rapid action value estimation enhancement,RAVE enhancement,reinforcement learning,Scalability,Sensitivity,state value estimation,temporal difference learning,temporal difference values,tic tac toe game,tree searching,trees (mathematics),UCT tree policy,upper confidence bounds for trees}
}

@incollection{walraet_googolplex_2016,
  title = {A {{Googolplex}} of {{Go Games}}},
  booktitle = {Computers and {{Games}}},
  author = {Walraet, Matthieu and Tromp, John},
  year = {2016},
  volume = {10068},
  pages = {191--201},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-50935-8_18},
  abstract = {We establish the existence of 1010100 Go games, addressing an open problem in ``Combinatorics of Go'' by Tromp and Farneb\textasciidieresis{}ack.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2016\\22 A Googolplex of Go Games.pdf},
  isbn = {978-3-319-50934-1 978-3-319-50935-8},
  language = {en}
}

@inproceedings{waltham_analysis_2016,
  title = {An {{Analysis}} of {{Artificial Intelligence Techniques}} in {{Multiplayer Online Battle Arena Game Environments}}},
  booktitle = {Proceedings of the {{Annual Conference}} of the {{South African Institute}} of {{Computer Scientists}} and {{Information Technologists}} on - {{SAICSIT}} '16},
  author = {Waltham, Michael and Moodley, Deshen},
  year = {2016},
  pages = {1--7},
  publisher = {{ACM Press}},
  address = {{Johannesburg, South Africa}},
  doi = {10.1145/2987491.2987513},
  abstract = {The 3D computer gaming industry is constantly exploring new avenues for creating immersive and engaging environments. One avenue being explored is autonomous control of the behaviour of non-player characters (NPC). This paper reviews and compares existing artificial intelligence (AI) techniques for controlling the behaviour of non-human characters in Multiplayer Online Battle Arena (MOBA) game environments. Two techniques, the fuzzy state machine (FuSM) and the emotional behaviour tree (EBT),were reviewed and compared. In addition, an alternate and simple mechanism to incorporate emotion in a behaviour tree is proposed and tested. Initial tests of the mechanism show that it is a viable and promising mechanism for effectively tracking the emotional state of an NPC and for incorporating emotion in NPC decision making.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\LPFQBTKG\\Waltham and Moodley - 2016 - An Analysis of Artificial Intelligence Techniques .pdf},
  isbn = {978-1-4503-4805-8},
  language = {en}
}

@phdthesis{walther_ai_2006,
  title = {{{AI}} for Real-Time Strategy Games},
  author = {Walther, Anders},
  year = {2006},
  abstract = {When playing a real-time strategy game (RTS game) one usually has the possibility to play the single-player option, where you play against the computer or alternatively, one can play the multiplayer option, where you plays against other players, for example over the internet.
In the multiplayer option of RTS games new strategies are continuously being developed and change the way the games are being played. Because of this, players keep finding this game option interesting and challenging. However, in the single-player part the artificial intelligence (AI) in these games doesn't learn these new strategies. It only draws from hard coded strategies provided by the programmer. Therefore, a human player quickly becomes
superior to the computer AI and looses interest in the single player part of the game. If the AI in these RTS games could adapt itself to new strategies and was able to learn new strategies on its own, the player would continue to find this part of the game interesting and would be able to try out new tactics before for example playing online against a ``human opponent''.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\SGFEPTGT\\Walther - 2006 - AI for real-time strategy games.pdf},
  language = {en},
  school = {IT-University of Copenhagen Design, Communication and Media},
  type = {Master}
}

@inproceedings{wang_alternative_2019,
  title = {Alternative {{Loss Functions}} in {{AlphaZero}}-like {{Self}}-Play},
  booktitle = {2019 {{IEEE Symposium Series}} on {{Computational Intelligence}} ({{SSCI}})},
  author = {Wang, Hui and Emmerich, Michael and Preuss, Mike and Plaat, Aske},
  year = {2019},
  month = dec,
  pages = {155--162},
  issn = {null},
  doi = {10.1109/SSCI44817.2019.9002814},
  abstract = {Recently, AlphaZero has achieved outstanding performance in playing Go, Chess, and Shogi. Players in AlphaZero consist of a combination of Monte Carlo Tree Search and a deep neural network, that is trained using self-play. The unified deep neural network has a policy-head and a value-head, and during training, the optimizer minimizes the sum of policy loss and value loss. However, it is not clear if and under which circumstances other formulations of the loss function are better. Therefore, we perform experiments with different combinations of these two minimization targets. In contrast to many recent papers who adopt single run experiments and use the whole history Elo ratings from self-play, we propose to use repeated runs. The results show that this method can describe the training performance quite well within each training run, but there is a high self-play bias, such that it is incomparable among different training runs. Therefore, inspired by the AlphaGo series papers, a self-play bias avoiding performance assessment, final best player Elo rating, is adopted to evaluate the playing strength in a direct competition between the evolved players. For relatively small games, based on this new evaluation method, surprisingly, minimizing only value loss achieves the strongest playing strength in the final best players' round-robin tournament. These results indicate that more research is needed into the relative importance of value function and policy function in small games.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Wang et al. - 2019 - Alternative Loss Functions in AlphaZero-like Self-.pdf;C\:\\Users\\aesou\\Zotero\\storage\\WRMLVTYW\\9002814.html},
  keywords = {AlphaZero-like self-play,Elo evaluation,loss combination}
}

@inproceedings{wang_portfolio_2016,
  title = {Portfolio {{Online Evolution}} in {{StarCraft}}},
  booktitle = {Proceedings, {{The Twelfth AAAI Conference}} on {{Artificial Intelligence}} and {{Interactive Digital Entertainment}} ({{AIIDE}}-16)},
  author = {Wang, Che and Chen, Pan and Li, Yuanda and Holmgard, Christoffer and Togelius, Julian},
  year = {2016},
  pages = {114--120},
  abstract = {Portfolio Online Evolution is a novel method for playing real-time strategy games through evolutionary search in the space of assignments of scripts to individual game units. This method builds on and recombines two recently devised methods for playing multi-action games: (1) Portfolio Greedy Search, which searches in the space of heuristics assigned to units rather than in the space of actions, and (2) Online Evolution, which uses evolution rather than tree search to effectively play games where multiple actions per turn lead to enormous branching factors. The combination of both ideas lead to the use of evolution to search the space of which script/heuristic is assigned to which unit. In this paper, we introduce the ideas of Portfolio Online Evolution and apply it to StarCraft micro, or individual battles. It is shown to outperform all other tested methods in battles of moderate to large size.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\KF8CAYA9\\Wang et al. - 2016 - Portfolio Online Evolution in StarCraft.pdf},
  language = {en}
}

@article{webb_not_2005,
  title = {Not {{So Naive Bayes}}: {{Aggregating One}}-{{Dependence Estimators}}},
  shorttitle = {Not {{So Naive Bayes}}},
  author = {Webb, Geoffrey I. and Boughton, Janice R. and Wang, Zhihai},
  year = {2005},
  month = jan,
  volume = {58},
  pages = {5--24},
  issn = {1573-0565},
  doi = {10.1007/s10994-005-4258-6},
  abstract = {Of numerous proposals to improve the accuracy of naive Bayes by weakening its attribute independence assumption, both LBR and Super-Parent TAN have demonstrated remarkable error performance. However, both techniques obtain this outcome at a considerable computational cost. We present a new approach to weakening the attribute independence assumption by averaging all of a constrained class of classifiers. In extensive experiments this technique delivers comparable prediction accuracy to LBR and Super-Parent TAN with substantially improved computational efficiency at test time relative to the former and at training time relative to the latter. The new algorithm is shown to have low variance and is suited to incremental learning.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\J8N693AP\\Webb et al. - 2005 - Not So Naive Bayes Aggregating One-Dependence Est.pdf},
  journal = {Machine Learning},
  language = {en},
  number = {1}
}

@inproceedings{weber_building_2011,
  title = {Building {{Human}}-{{Level AI}} for {{Real}}-{{Time Strategy Games}}},
  booktitle = {Advances in {{Cognitive Systems}}: {{Papers}} from the 2011 {{AAAI Fall Symposium}} ({{FS}}-11-01)},
  author = {Weber, Ben and Mateas, Michael and Jhala, Arnav},
  year = {2011},
  pages = {8},
  abstract = {Video games are complex simulation environments with many real-world properties that need to be addressed in order to build robust intelligence. In particular, realtime strategy games provide a multi-scale challenge which requires both deliberative and reactive reasoning processes. Experts approach this task by studying a corpus of games, building models for anticipating opponent actions, and practicing within the game environment. We motivate the need for integrating heterogeneous approaches by enumerating a range of competencies involved in gameplay and discuss how they are being implemented in EISBot, a reactive planning agent that we have applied to the task of playing real-time strategy games at the same granularity as humans.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\REEIJ9DY\\Weber et al. - 2011 - Building Human-Level AI for Real-Time Strategy Gam.pdf},
  language = {en}
}

@inproceedings{weber_data_2009,
  title = {A Data Mining Approach to Strategy Prediction},
  booktitle = {2009 {{IEEE Symposium}} on {{Computational Intelligence}} and {{Games}}},
  author = {Weber, Ben G. and Mateas, Michael},
  year = {2009},
  month = sep,
  pages = {140--147},
  publisher = {{IEEE}},
  address = {{Milano, Italy}},
  doi = {10.1109/CIG.2009.5286483},
  abstract = {We present a data mining approach to opponent modeling in strategy games. Expert gameplay is learned by applying machine learning techniques to large collections of game logs. This approach enables domain independent algorithms to acquire domain knowledge and perform opponent modeling. Machine learning algorithms are applied to the task of detecting an opponent's strategy before it is executed and predicting when an opponent will perform strategic actions. Our approach involves encoding game logs as a feature vector representation, where each feature describes when a unit or building type is first produced. We compare our representation to a state lattice representation in perfect and imperfect information environments and the results show that our representation has higher predictive capabilities and is more tolerant of noise. We also discuss how to incorporate our data mining approach into a full game playing agent.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Weber and Mateas - 2009 - A data mining approach to strategy prediction.pdf},
  isbn = {978-1-4244-4814-2},
  language = {en}
}

@inproceedings{weber_particle_2011,
  title = {A {{Particle Model}} for {{State Estimation}} in {{Real}}-{{Time Strategy Games}}},
  booktitle = {Seventh {{Artificial Intelligence}} and {{Interactive Digital Entertainment Conference}}  ({{AIIDE}}'11)},
  author = {Weber, Ben and Mateas, Michael and Jhala, Arnav},
  year = {2011},
  pages = {103--108},
  abstract = {A big challenge for creating human-level game AI is building agents capable of operating in imperfect information environments. In real-time strategy games the technological progress of an opponent and locations of enemy units are partially observable. To overcome this limitation, we explore a particle-based approach for estimating the location of enemy units that have been encountered. We represent state estimation as an optimization problem, and automatically learn parameters for the particle model by mining a corpus of expert StarCraft replays. The particle model tracks opponent units and provides conditions for activating tactical behaviors in our StarCraft bot. Our results show that incorporating a learned particle model improves the performance of EISBot by 10\% over baseline approaches.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Weber et al. - 2011 - A Particle Model for State Estimation in Real-Time.pdf},
  language = {en}
}

@article{weber_reactive_2014,
  title = {Reactive {{Planning}} for {{Micromanagement}} in {{RTS Games}}},
  author = {Weber, Ben},
  year = {2014},
  pages = {6},
  abstract = {This paper presents an agent for commanding individual units in a real-time strategy game (RTS). The agent is implemented in the reactive planning language ABL and uses micromanagement techniques to gain a tactical advantage over opponents. Two strategies are explored focusing on harassment and unit formations. The agent is compared against the built-in AI of Wargus. The results show that reactive planning is a suitable technique for specifying low-level unit commands. Improving unit behavior in an RTS provides more challenging non-playable characters are partially alleviates players from the need to control individual units.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\IFELY898\\Weber - Reactive Planning for Micromanagement in RTS Games.pdf},
  journal = {Department of Computer. Science, University of California, Santa Cruz},
  language = {en}
}

@inproceedings{weber_standard_2018,
  title = {Standard {{Economic Models}} in {{Nonstandard Settings}} \textendash{} {{StarCraft}}: {{Brood War}}},
  shorttitle = {Standard {{Economic Models}} in {{Nonstandard Settings}} \textendash{} {{StarCraft}}},
  booktitle = {2018 {{IEEE Conference}} on {{Computational Intelligence}} and {{Games}} ({{CIG}})},
  author = {Weber, B. S.},
  year = {2018},
  month = aug,
  pages = {1--8},
  doi = {10.1109/CIG.2018.8490444},
  abstract = {This paper reviews an entrant to this year's Star-Craft: Brood War AI tournament, CUNYbot. CUNYbot makes strategic decisions using a low-dimensional economic model traditionally used to describe the behavior of countries, but has applications for any real-time-strategy game (RTS) where the capital/labor ratio (k) and technology/labor ratio (t) are critical. CUNYbot first tunes the economic model parameters between games using a genetic algorithm, allowing it to learn an optimal static k for each built-in AI race. In the second step of the project, CUNYbot models the opponent during the game and is permitted to react to the opponent's inferred choices in k. The reactive model adopts a greedy "tit-for-tat" style strategy against all three races. This paper reviews the features of the capital-augmenting Cobb-Douglas model, the game theory behind the reactive strategy, and demonstrates adaptation to particular opponents in the RTS context.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\G9648QM8\\Weber - 2018 - Standard Economic Models in Nonstandard Settings –.pdf;C\:\\Users\\aesou\\Zotero\\storage\\Z3XB7S9D\\8490444.html},
  keywords = {Adaptation models,AI race,Artificial intelligence,Biological system modeling,Brood War AI tournament,Buildings,capital-augmenting Cobb-Douglas model,CUNYbot models,economic modeling,economics,Economics,game theory,Games,genetic algorithm,genetic algorithms,Genetic algorithms,low-dimensional economic model,nonstandard settings - StarCraft,reactive strategy,real-time-strategy game,RTS,standard economic models,StarCraft,strategic decisions,tit-for-tat style strategy}
}

@inproceedings{wender_combining_2014,
  title = {Combining {{Case}}-{{Based Reasoning}} and {{Reinforcement Learning}} for {{Unit Navigation}} in {{Real}}-{{Time Strategy Game AI}}},
  booktitle = {Case-{{Based Reasoning Research}} and {{Development}}},
  author = {Wender, Stefan and Watson, Ian},
  editor = {Lamontagne, Luc and Plaza, Enric},
  year = {2014},
  pages = {511--525},
  publisher = {{Springer International Publishing}},
  abstract = {This paper presents a navigation component based on a hybrid case-based reasoning (CBR) and reinforcement learning (RL) approach for an AI agent in a real-time strategy (RTS) game. Spatial environment information is abstracted into a number of influence maps. These influence maps are then combined into cases that are managed by the CBR component. RL is used to update the case solutions which are composed of unit actions with associated fitness values. We present a detailed account of the architecture and underlying model. Our model accounts for all relevant environment influences with a focus on two main subgoals: damage avoidance and target approximation. For each of these subgoals, we create scenarios in the StarCraft RTS game and look at the performance of our approach given different similarity thresholds for the CBR part. The results show, that our navigation component manages to learn how to fulfill both sub-goals given the choice of a suitable similarity threshold. Finally, we combine both subgoals for the overall navigation component and show a comparison between the integrated approach, a random action selection, and a target-selection-only agent. The results show that the CBR/RL approach manages to successfully learn how to navigate towards goal positions while at the same time avoiding enemy attacks.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\XC3CIG4Z\\Wender and Watson - 2014 - Combining Case-Based Reasoning and Reinforcement L.pdf},
  isbn = {978-3-319-11209-1},
  keywords = {CBR,Game AI,Reinforcement Learning,Unit Navigation},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{wender_combining_2016,
  title = {Combining {{Case}}-{{Based Reasoning}} and {{Reinforcement Learning}} for {{Tactical Unit Selection}} in {{Real}}-{{Time Strategy Game AI}}},
  booktitle = {Case-{{Based Reasoning Research}} and {{Development}}},
  author = {Wender, Stefan and Watson, Ian},
  editor = {Goel, Ashok and {D{\'i}az-Agudo}, M Bel{\'e}n and {Roth-Berghofer}, Thomas},
  year = {2016},
  pages = {413--429},
  publisher = {{Springer International Publishing}},
  abstract = {This paper presents a hierarchical approach to the problems inherent in parts of real-time strategy games. The overall game is decomposed into a hierarchy of sub-problems and an architecture is created that addresses a significant number of these through interconnected machine-learning (ML) techniques. Specifically, individual modules that use a combination of case-based reasoning (CBR) and reinforcement learning (RL) are organised into three distinct yet interconnected layers of reasoning. An agent is created for the RTS game StarCraft and individual modules are devised for the separate tasks that are described by the architecture. The modules are individually trained and subsequently integrated in a micromanagement agent that is evaluated in a range of test scenarios. The experimental evaluation shows that the agent is able to learn how to manage groups of units to successfully solve a number of different micromanagement scenarios.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\QBWGXMWN\\Wender and Watson - 2016 - Combining Case-Based Reasoning and Reinforcement L.pdf},
  isbn = {978-3-319-47096-2},
  keywords = {CBR,Game AI,Layered learning,Reinforcement learning},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{wender_integrating_2014-1,
  title = {Integrating {{Case}}-{{Based Reasoning}} with {{Reinforcement Learning}} for {{Real}}-{{Time Strategy Game Micromanagement}}},
  booktitle = {{{PRICAI}} 2014: {{Trends}} in {{Artificial Intelligence}}},
  author = {Wender, Stefan and Watson, Ian},
  editor = {Pham, Duc-Nghia and Park, Seong-Bae},
  year = {2014},
  pages = {64--76},
  publisher = {{Springer International Publishing}},
  abstract = {This paper describes the conception of a hybrid Reinforcement Learning (RL) and Case-Based Reasoning (CBR) approach to managing combat units in strategy games. Both methods are combined into an AI agent that is evaluated by using the real-time strategy (RTS) computer game StarCraft as a test bed. The eventual aim of this approach is an AI agent that has the same actions and information at its disposal as a human player. As part of an experimental evaluation, the agent is tested in different scenarios using optimized algorithm parameters. The integration of CBR for memory management is shown to improve the speed of convergence to an optimal policy, while also enabling the agent to address a larger variety of problems when compared to simple RL. The agent manages to beat the built-in game AI and also outperforms a simple RL-only agent. An analysis of the evolution of the case-base shows how scenarios and algorithmic parameters influence agent performance and will serve as a foundation for future improvement to the hybrid CBR/RL approach.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\GHP4RKEX\\Wender and Watson - 2014 - Integrating Case-Based Reasoning with Reinforcemen.pdf},
  isbn = {978-3-319-13560-1},
  keywords = {Enemy Unit,Game State,Optimize Algorithm Parameter,Similarity Threshold,Unit Case},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{wickramarathna_invoke_2019,
  title = {Invoke {{Artificial Intelligence}} and {{Machine Learning}} for {{Strategic}}-{{Level Games}} and {{Interactive Simulations}}},
  booktitle = {Artificial {{Intelligence}}},
  author = {Wickramarathna, Nishan Chathuranga and Ganegoda, Gamage Upeksha},
  editor = {Hemanth, Jude and Silva, Thushari and Karunananda, Asoka},
  year = {2019},
  pages = {129--143},
  publisher = {{Springer}},
  address = {{Singapore}},
  doi = {10.1007/978-981-13-9129-3_10},
  abstract = {Computer games are an important sector of the digital economy, computer and entertainment industry are very sophisticated in many ways in the current context of technology. They've gone beyond entertainment needs, and the computer game paradigm and technology together are now increasingly used in education, training, storytelling, and wherever it's necessary to create an appealing and engaging environment. More realism in virtual and artificial environments and more real interfaces to the users can be considered as two main advantages that we get using these techniques. Instead of pre-defined hard coded scripts driving these environments, we will be able to create just the environment and its relative mechanics, so the artificial intelligence could introduce tailored challenges and scenarios to the environment. This paper proposes a Behavioral Driven Procedural Content Generation methodology together with Ternary Neural Networks to be used in interactive strategy-based simulations for effective decision making. This is vital because current approaches like Experience Driven Procedural Content Generation algorithms can be very flexible and one small change could trigger complex changes in the system. Using another model created by using player behavior will be specifying the clamping conditions so the AI is capable of stabilizing itself.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Wickramarathna and Ganegoda - 2019 - Invoke Artificial Intelligence and Machine Learnin.pdf},
  isbn = {9789811391293},
  keywords = {Artificial intelligence,Computer games,Machine learning,Procedural content generation,Simulations},
  language = {en},
  series = {Communications in {{Computer}} and {{Information Science}}}
}

@article{wilson_evolving_2018,
  title = {Evolving Simple Programs for Playing {{Atari}} Games},
  author = {Wilson, Dennis G. and {Cussat-Blanc}, Sylvain and Luga, Herv{\'e} and Miller, Julian F.},
  year = {2018},
  month = jun,
  abstract = {Cartesian Genetic Programming (CGP) has previously shown capabilities in image processing tasks by evolving programs with a function set specialized for computer vision. A similar approach can be applied to Atari playing. Programs are evolved using mixed type CGP with a function set suited for matrix operations, including image processing, but allowing for controller behavior to emerge. While the programs are relatively small, many controllers are competitive with state of the art methods for the Atari benchmark set and require less training time. By evaluating the programs of the best evolved individuals, simple but e ective strategies can be found.},
  archivePrefix = {arXiv},
  eprint = {1806.05695},
  eprinttype = {arxiv},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\C9923HRQ\\Wilson et al. - 2018 - Evolving simple programs for playing Atari games.pdf},
  journal = {arXiv:1806.05695 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Neural and Evolutionary Computing},
  language = {en},
  primaryClass = {cs}
}

@article{winands_enhanced_2005,
  title = {Enhanced Forward Pruning},
  author = {Winands, Mark H. M. and {van den Herik}, H. Jaap and Uiterwijk, Jos W. H. M. and {van der Werf}, Erik C. D.},
  year = {2005},
  month = nov,
  volume = {175},
  pages = {315--329},
  issn = {0020-0255},
  doi = {10.1016/j.ins.2004.04.011},
  abstract = {In this paper forward-pruning methods, such as multi-cut and null move, are tested at so-called ALL nodes. We improved the principal variation search by four small but essential additions. The new PVS algorithm guarantees that forward pruning is safe at ALL nodes. Experiments show that multi-cut at ALL nodes (MC-A) when combined with other forward-pruning mechanisms give a significant reduction of the number of nodes searched. In comparison, a (more) aggressive version of the null move (variable null-move bound) gives less reduction at expected ALL nodes. Finally, it is demonstrated that the playing strength of the lines of action program MIA is significantly (scoring 21\% more winning points than the opponent) increased by MC-A.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Winands et al. - 2005 - Enhanced forward pruning.pdf;C\:\\Users\\aesou\\Zotero\\storage\\4UPRKRNS\\S0020025504002750.html},
  journal = {Information Sciences},
  keywords = {Lines of action,Multi-cut pruning,search},
  language = {en},
  number = {4},
  series = {Heuristic {{Search}} and {{Computer Game Playing IV}}}
}

@incollection{winands_evaluation-function_2011,
  title = {Evaluation-{{Function Based Proof}}-{{Number Search}}},
  booktitle = {Computers and {{Games}}},
  author = {Winands, Mark H. M. and Schadd, Maarten P. D.},
  year = {2011},
  volume = {6515},
  pages = {23--35},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-17928-0_3},
  abstract = {This article introduces Evaluation-Function based Proof\textendash{}Number Search (EF-PN) and its second-level variant EF-PN2. It is a framework for initializing the proof and disproof number of a leaf node with the help of a heuristic evaluation function. Experiments in LOA and Surakarta show that compared to PN and PN2, which use mobility to initialize the proof and disproof numbers, EF-PN and EF-PN2 take between 45\% to 85\% less time for solving positions. Based on these results, we may conclude that EF-PN and EF-PN2 reduce the search space considerably.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2010\\8 Evaluation-Function Based Proof-Number Search.pdf},
  isbn = {978-3-642-17927-3 978-3-642-17928-0},
  language = {en}
}

@incollection{winands_monte-carlo_2008,
  title = {Monte-{{Carlo Tree Search Solver}}},
  booktitle = {Computers and {{Games}}},
  author = {Winands, Mark H. M. and Bj{\"o}rnsson, Yngvi and Saito, Jahn-Takeshi},
  year = {2008},
  volume = {5131},
  pages = {25--36},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-87608-3_3},
  abstract = {Recently, Monte-Carlo Tree Search (MCTS) has advanced the field of computer Go substantially. In this article we investigate the application of MCTS for the game Lines of Action (LOA). A new MCTS variant, called MCTS-Solver, has been designed to play narrow tactical lines better in sudden-death games such as LOA. The variant differs from the traditional MCTS in respect to backpropagation and selection strategy. It is able to prove the game-theoretical value of a position given sufficient time. Experiments show that a Monte-Carlo LOA program using MCTS-Solver defeats a program using MCTS by a winning score of 65\%. Moreover, MCTS-Solver performs much better than a program using MCTS against several different versions of the world-class {$\alpha\beta$} program MIA. Thus, MCTS-Solver constitutes genuine progress in using simulation-based search approaches in sudden-death games, significantly improving upon MCTS-based programs.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2008\\8 Monte-Carlo Tree Search Solver.pdf},
  isbn = {978-3-540-87607-6 978-3-540-87608-3},
  language = {en}
}

@article{wolf_introduction_2005,
  title = {{{AN INTRODUCTION TO THE VIDEO GAME THEORY}}},
  author = {Wolf, Mark J. P.},
  year = {2005},
  issn = {2385-3697},
  abstract = {This essay gives a brief history of Video Games Studies and then looks at basic elements of video game
theory and possible directions for the development of the field.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\55X2Q7MI\\Wolf - 2005 - AN INTRODUCTION TO THE VIDEO GAME THEORY.pdf;C\:\\Users\\aesou\\Zotero\\storage\\TUCSI7YA\\257485.html},
  journal = {Formats: revista de comunicaci\'o audiovisual},
  language = {eng},
  number = {4}
}

@book{wolf_video_2009,
  title = {The Video Game Theory Reader.},
  author = {Wolf, Mark J. P},
  year = {2009},
  publisher = {{Routledge}},
  address = {{New York, NY}},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\E5AIUN6J\\Wolf and Perron - The Video Game Theory Reader.pdf},
  isbn = {978-0-415-96578-1 978-0-415-96579-8},
  language = {English}
}

@incollection{wu_dependency-based_2014,
  title = {Dependency-{{Based Search}} for {{Connect6}}},
  booktitle = {Computers and {{Games}}},
  author = {Wu, I-Chen and Kang, Hao-Hua and Lin, Hung-Hsuan and Lin, Ping-Hung and Wei, Ting-Han and Chang, Chieh-Min},
  year = {2014},
  volume = {8427},
  pages = {1--13},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-09165-5_1},
  abstract = {Allis proposed dependency-based search (DBS) to solve Go-Moku, a kind of five-in-a-row game. DBS is critical for threat space search (TSS) when there are many independent or nearly independent TSS areas. Similarly, DBS is also important for the game Connect6, a kind of six-in-a-row game with two pieces per move. Unfortunately, the rule that two pieces are played per move in Connect6 makes DBS extremely difficult to apply to Connect6 programs. This paper is the first attempt to apply DBS to Connect6 programs. The targeted program is NCTU6, which won Connect6 tournaments in the Computer Olympiad twice and defeated many professional players in Man-Machine Connect6 championships. The experimental results show that DBS yields a speedup factor of 4.12 on average, and up to 50 for some hard positions.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2013\\5 Dependency-Based Search for Connect6.pdf},
  isbn = {978-3-319-09164-8 978-3-319-09165-5},
  language = {en}
}

@inproceedings{wu_job-level_2011,
  title = {Job-{{Level Proof}}-{{Number Search}} for {{Connect6}}},
  booktitle = {Computers and {{Games}}},
  author = {Wu, I-Chen and Lin, Hung-Hsuan and Lin, Ping-Hung and Sun, Der-Johng and Chan, Yi-Chih and Chen, Bo-Ting},
  editor = {{van den Herik}, H. Jaap and Iida, Hiroyuki and Plaat, Aske},
  year = {2011},
  pages = {11--22},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-17928-0_2},
  abstract = {This paper proposes a new approach for proof number (PN) search, named job-level PN (JL-PN) search, where each search tree node is evaluated or expanded by a heavy-weight job, which takes normally over tens of seconds. Such JL-PN search is well suited for parallel processing, since these jobs are allowed to be performed by remote processors independently. This paper applies JL-PN search to solving automatically several Connect6 positions including openings on desktop grids. For some of these openings, so far no human expert had been able to find a winning strategy. Our experiments also show that the speedups for solving the test positions are roughly linear, fluctuating from sublinear to superlinear. Hence, JL-PN search appears to be a quite promising approach to solving games.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2010\\7 Job-Level Proof-Number Search for Connect6.pdf},
  isbn = {978-3-642-17928-0},
  keywords = {Desktop Grid,Free Worker,Greedy Policy,Logarithmic Time Scale,Winning Strategy},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{wu_strength_2019,
  title = {On {{Strength Adjustment}} for {{MCTS}}-{{Based Programs}}},
  booktitle = {Proceedings of the {{AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Wu, I.-Chen and Wu, Ti-Rong and Liu, An-Jen and Guei, Hung and Wei, Tinghan},
  year = {2019},
  month = jul,
  volume = {33},
  pages = {1222--1229},
  doi = {10.1609/aaai.v33i01.33011222},
  abstract = {This paper proposes an approach to strength adjustment for MCTS-based game-playing programs. In this approach, we use a softmax policy with a strength index z to choose moves. Most importantly, we filter low quality moves by excluding those that have a lower simulation count than a pre-defined threshold ratio of the maximum simulation count. We perform a theoretical analysis, reaching the result that the adjusted policy is guaranteed to choose moves exceeding a lower bound in strength by using a threshold ratio. The approach is applied to the Go program ELF OpenGo. The experiment results show that z is highly correlated to the empirical strength; namely, given a threshold ratio 0.1, z is linearly related to the Elo rating with regression error 47.95 Elo where -2{$\leq$}z
{$\leq$}2. Meanwhile, the covered strength range is about 800 Elo ratings in the interval of z in [-2,2]. With the ease of strength adjustment using z, we present two methods to adjust strength and predict opponents' strengths dynamically. To our knowledge, this result is state-of-the-art in terms of the range of strengths in Elo rating while maintaining a controllable relationship between the strength and a strength index.},
  copyright = {Copyright (c) 2019 Association for the Advancement of Artificial Intelligence},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Wu et al. - 2019 - On Strength Adjustment for MCTS-Based Programs.pdf;C\:\\Users\\aesou\\Zotero\\storage\\CUZZ4C4W\\Wu et al. - 2019 - On Strength Adjustment for MCTS-Based Programs.pdf;C\:\\Users\\aesou\\Zotero\\storage\\FJAI9V4L\\3917.html},
  language = {en}
}

@inproceedings{xie_deep_2019,
  title = {Deep {{Deterministic Policy Gradients}} with {{Transfer Learning Framework}} in {{StarCraft Micromanagement}}},
  booktitle = {2019 {{IEEE International Conference}} on {{Electro Information Technology}} ({{EIT}})},
  author = {Xie, Dong and Zhong, Xiangnan},
  year = {2019},
  month = may,
  pages = {410--415},
  issn = {2154-0357},
  doi = {10.1109/EIT.2019.8833742},
  abstract = {This paper proposes an intelligent multi-agent approach in a real-time strategy game, StarCraft, based on the deep deterministic policy gradients (DDPG) techniques. An actor and a critic network are established to estimate the optimal control actions and corresponding value functions, respectively. A special reward function is designed based on the agents' own condition and enemies' information to help agents make intelligent control in the game. Furthermore, in order to accelerate the learning process, the transfer learning techniques are integrated into the training process. Specifically, the agents are trained initially in a simple task to learn the basic concept for the combat, such as detouring moving, avoiding and joining attacking. Then, we transfer this experience to the target task with a complex and difficult scenario. From the experiment, it is shown that our proposed algorithm with transfer learning can achieve better performance.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Xie and Zhong - 2019 - Deep Deterministic Policy Gradients with Transfer .pdf;C\:\\Users\\aesou\\Zotero\\storage\\EIFRS3EF\\8833742.html},
  keywords = {computer games,corresponding value functions,deep deterministic policy gradients,deep deterministic policy gradients techniques,Games,intelligent control,Intelligent control,learning (artificial intelligence),learning process,multi-agent,multi-agent systems,multiagent approach,Neural networks,optimal control actions,real-time strategy game,Real-time systems,Reinforcement learning,special reward function,StarCraft micromanagement,strategy game,Task analysis,Training,training process,transfer learning,transfer learning framework,transfer learning techniques}
}

@article{xu_adaptive_2018,
  title = {Adaptive {{CGF Commander Behavior Modeling Through HTN Guided Monte Carlo Tree Search}}},
  author = {Xu, Xiao and Yang, Mei and Li, Ge},
  year = {2018},
  month = apr,
  volume = {27},
  pages = {231--249},
  issn = {1004-3756, 1861-9576},
  doi = {10.1007/s11518-018-5366-8},
  abstract = {Improving the intelligence of virtual entities is an important issue in Computer Generated Forces (CGFs) construction. Some traditional approaches try to achieve this by specifying how entities should react to predefined conditions, which is not suitable for complex and dynamic environments. This paper aims to apply Monte Carlo Tree Search (MCTS) for the behavior modeling of CGF commander. By look-ahead reasoning, the model generates adaptive decisions to direct the whole troops to fight. Our main work is to formulate the tree model through the state and action abstraction, and extend its expansion process to handle simultaneous and durative moves. We also employ Hierarchical Task Network (HTN) planning to guide the search, thus enhancing the search efficiency. The final implementation is tested in an infantry combat simulation where a company commander needs to control three platoons to assault and clear enemies within defined areas. Comparative results from a series of experiments demonstrate that the HTN guided MCTS commander can outperform other commanders following fixed strategies.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\EK7G7E7P\\Xu et al. - 2018 - Adaptive CGF Commander Behavior Modeling Through H.pdf},
  journal = {Journal of Systems Science and Systems Engineering},
  language = {en},
  number = {2}
}

@inproceedings{xu_macro_2019,
  title = {Macro {{Action Selection}} with {{Deep Reinforcement Learning}} in {{StarCraft}}},
  booktitle = {Proceedings of the {{AAAI Conference}} on {{Artificial Intelligence}} and {{Interactive Digital Entertainment}}},
  author = {Xu, Sijia and Kuang, Hongyu and Zhi, Zhuang and Hu, Renjie and Liu, Yang and Sun, Huyang},
  year = {2019},
  month = oct,
  volume = {15},
  pages = {94--99},
  abstract = {StarCraft (SC) is one of the most popular and successful Real Time Strategy (RTS) games. In recent years, SC is also widely accepted as a challenging testbed for AI research because of its enormous state space, partially observed information, multi-agent collaboration, and so on. With the help of annual AIIDE and CIG competitions, a growing number of SC bots are proposed and continuously improved. However, a large gap remains between the top-level bot and the professional human player. One vital reason is that current SC bots mainly rely on predefined rules to select macro actions during their games. These rules are not scalable and efficient enough to cope with the enormous yet partially observed state space in the game. In this paper, we propose a deep reinforcement learning (DRL) framework to improve the selection of macro actions. Our framework is based on the combination of the Ape-X DQN and the Long-Short-Term-Memory (LSTM). We use this framework to build our bot, named as LastOrder. Our evaluation, based on training against all bots from the AIIDE 2017 StarCraft AI competition set, shows that LastOrder achieves an 83\% winning rate, outperforming 26 bots in total 28 entrants.},
  copyright = {Copyright (c) 2019 Association for the Advancement of Artificial Intelligence},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\63TY6974\\Xu et al. - 2019 - Macro Action Selection with Deep Reinforcement Lea.pdf},
  language = {en}
}

@incollection{yajima_node-expansion_2011,
  title = {Node-{{Expansion Operators}} for the {{UCT Algorithm}}},
  booktitle = {Computers and {{Games}}},
  author = {Yajima, Takayuki and Hashimoto, Tsuyoshi and Matsui, Toshiki and Hashimoto, Junichi and Spoerer, Kristian},
  year = {2011},
  volume = {6515},
  pages = {116--123},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-17928-0_11},
  abstract = {Recent works on the MCTS and UCT framework in the domain of Go focused on introducing knowledge to the playout and on pruning variations from the tree, but so far node expansion has not been investigated. In this paper we show that delaying expansion according to the number of the siblings delivers a gain of more than 92\% when compared to normal expansion. We propose three improvements; one that uses domain knowledge and two that are domain-independent methods. Experimental results show that all advanced operators significantly improve the UCT performance when compared to the basic delaying expansion. From the results we may conclude that the new expansion operators are an appropriate means to improve the UCT algorithm.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2010\\16 Node-Expansion Operators for the UCT Algorithm.pdf},
  isbn = {978-3-642-17927-3 978-3-642-17928-0},
  language = {en}
}

@incollection{yamaguchi_cylinder-infinite-connect-four_2014,
  title = {Cylinder-{{Infinite}}-{{Connect}}-{{Four Except}} for {{Widths}} 2, 6, and 11 {{Is Solved}}: {{Draw}}},
  shorttitle = {Cylinder-{{Infinite}}-{{Connect}}-{{Four Except}} for {{Widths}} 2, 6, and 11 {{Is Solved}}},
  booktitle = {Computers and {{Games}}},
  author = {Yamaguchi, Yoshiaki and Tanaka, Tetsuro and Yamaguchi, Kazunori},
  year = {2014},
  volume = {8427},
  pages = {163--174},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-09165-5_14},
  abstract = {Cylinder-Infinite-Connect-Four is a variant of the ConnectFour game played on cylindrical boards of differing cyclic widths and infinite height. In this paper, we show strategies to avoid losing at CylinderInfinite-Connect-Four except for Widths 2, 6, and 11. If both players use the strategies, the game will be drawn. This result can also be used to show that Width-Limited-Infinite-Connect-Four is drawn for any width. We also show that Connect-Four of any size with passes allowed is drawn.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2013\\18 Cylinder-Infinite-Connect-Four Except for Widths 2, 6, and 11 Is Solved Draw.pdf},
  isbn = {978-3-319-09164-8 978-3-319-09165-5},
  language = {en}
}

@inproceedings{yang_dynamic_2018,
  title = {A {{Dynamic Hierarchical Evaluating Network}} for {{Real}}-{{Time Strategy Games}}},
  booktitle = {{{MATEC Web}} of {{Conferences}}},
  author = {Yang, Weilong and Zhang, Qi and Peng, Yong},
  editor = {Bevrani, H. and Shuhui, W.},
  year = {2018},
  month = sep,
  volume = {208},
  pages = {05003},
  doi = {10.1051/matecconf/201820805003},
  abstract = {Researches of AI planning in Real-Time Strategy (RTS) games have been widely applied to human behavior modeling and combat simulation. State evaluation is an important research area for AI planning, which ensures the decision accuracy. Since complex interactions exist among different game aspects, the weighted average model usually cannot be well used to compute the evaluation of game state, which results in misleading player's generation strategy. In this paper, we take dynamic changes and player's preference into consideration, analyze player's preference and units' relationships base on game theory and propose a dynamic hierarchical evaluating network, denoted as DHEN. Experiments show that the modified evaluating algorithm can effectively improve the accuracy of task planning algorithm for RTS games.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\4PXCDCWD\\Yang et al. - 2018 - A Dynamic Hierarchical Evaluating Network for Real.pdf},
  language = {en}
}

@inproceedings{yang_extracting_2019,
  title = {Extracting {{Policies}} from {{Replays}} to {{Improve MCTS}} in {{Real Time Strategy Games}}},
  booktitle = {The 2nd {{Workshop}} on {{Knowledge Extraction}} from {{Games}} Co-Located with 33rd {{AAAI Conference}} on {{Artificial Intelligence}} ({{AAAI}} 2019)},
  author = {Yang, Zuozhi and Onta{\~n}{\'o}n, Santiago},
  year = {2019},
  address = {{Honolulu, Hawaii}},
  abstract = {In this paper we study the topic of integrating supervised learning models into Monte Carlo Tree Search (MCTS) in the context of RTS games. Specifically, we focus on learning a tree policy for MCTS using existing supervised learning algorithms. We evaluate and compare two families of models: Bayesian classifiers and decision trees classifiers. Our results show that in experiments under same iteration budget for MCTS, the models with higher classification performance also have better gameplay strength when used within MCTS. However, when we constrain computation budget by time, faster models tend to outperform slower, more accurate, models. Surprisingly, the classic C4.5 algorithm stands out in our experiments as the best model since it has good classification performance and fast classification speed.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Yang and Ontañón - 2019 - Extracting Policies from Replays to Improve MCTS i.pdf},
  language = {en}
}

@article{yang_fuzzy_2019,
  title = {Fuzzy {{Theory Based Single Belief State Generation}} for {{Partially Observable Real}}-{{Time Strategy Games}}},
  author = {Yang, Weilong and Xie, Xu and Peng, Yong},
  year = {2019},
  month = jun,
  volume = {7},
  pages = {79320--79330},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2019.2923419},
  abstract = {As the basic problem of the real-time strategy (RTS) games, AI planning has attracted wide attention of researchers, but it still remains as a huge challenge due to its large searching space and realtime nature. The situation may get worse when the planning in RTS games is implemented under a partially observable environment considering the existence of the fog-of-war. Given the recorded past positions of an agent, it would be helpful if the targets' next position can be predicted based on the recorded data since this will increase the certainty of the target. Therefore, this paper proposes a fuzzy theory-based single belief state generation method named FTH to do what based on multi-layer information sets extracted from the history position information. Besides, we incorporate the FTH generation method into adversarial hierarchical task network repairing (AHTNR) planning algorithm, which can be used for the prediction of the unit's position and task planning. Finally, we carry out an empirical study based on the \textmu{}RTS game and validate its effectiveness by comparing its performance with that of other state-of-the-art algorithms.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Yang et al. - 2019 - Fuzzy Theory Based Single Belief State Generation .pdf},
  journal = {IEEE Access},
  language = {en}
}

@inproceedings{yang_guiding_2019,
  title = {Guiding {{Monte Carlo Tree Search}} by {{Scripts}} in {{Real}}-{{Time Strategy Games}}},
  booktitle = {Proceedings of the {{AAAI Conference}} on {{Artificial Intelligence}} and {{Interactive Digital Entertainment}}},
  author = {Yang, Zuozhi and Onta{\~n}{\'o}n, Santiago},
  year = {2019},
  month = oct,
  volume = {15},
  pages = {100--106},
  abstract = {In Real-Time Strategy (RTS) games, the action space grows combinatorially with respect to the number of units. With limited computing budget between actions, methods like Monte Carlo Tree Search (MCTS) tend to get lost in the massive search space. An interesting line of existing work is to incorporate human knowledge in the form of scripts. In this paper, we investigate different possibilities for incorporating scripts into the tree policy while still maintaining the convergence guarantees of MCTS. We also report experiments on incorporating the scripts into the playout policy, which showed that unbiased bots perform better than biased bots.},
  copyright = {Copyright (c) 2019 Association for the Advancement of Artificial Intelligence},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\B6ETS6MR\\Yang and Ontañón - 2019 - Guiding Monte Carlo Tree Search by Scripts in Real.pdf},
  language = {en}
}

@inproceedings{yang_identifying_2014,
  title = {Identifying {{Patterns}} in {{Combat}} That Are {{Predictive}} of {{Success}} in {{MOBA Games}}},
  booktitle = {Proceedings of the {{Foundations}} of {{Digital Games}} 2014 {{Conference}} ({{FDG}} 14)},
  author = {Yang, Pu and Harrison, Brent and Roberts, David L},
  year = {2014},
  pages = {8},
  abstract = {Multiplayer Online Battle Arena (MOBA) games rely primarily on combat to determine the ultimate outcome of the game. Combat in these types of games is highly-dynamic and can be difficult for novice players to learn. Typically, mastery of combat requires that players obtain expert knowledge through practice, which can be difficult to concisely describe. In this paper, we present a data-driven approach for discovering patterns in combat tactics that are common among winning teams in MOBA games. We model combat as a sequence of graphs and extract patterns that predict successful outcomes not just of combat, but of the entire game. To identify those patterns, we attribute features to these graphs using well known graph metrics. These features allow us to describe, in meaningful terms, how different combat tactics contribute to team success. We also present an evaluation of our methodology on the popular MOBA game, DotA 2 (Defense of the Ancients 2). Experiments show that extracted patterns achieve an 80\% prediction accuracy when testing on new game logs.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\R9XKYW3D\\Yang et al. - 2014 - Identifying Patterns in Combat that are Predictive.pdf},
  language = {en}
}

@inproceedings{yang_integrating_2020,
  title = {Integrating {{Search}} and {{Scripts}} for {{Real}}-{{Time Strategy Games}}: {{An Empirical Survey}}},
  booktitle = {{{AAAI}}-20 {{Workshop}} on {{Reinforcement Learning}} in {{Games}}},
  author = {Yang, Zuozhi and Onta{\~n}{\'o}n, Santiago},
  year = {2020},
  address = {{New York}},
  abstract = {Real-time strategy games are a challenging problem from an AI point of view. Specially, they are particularly hard for tree search algorithms due to their combinatorial branching factors the limited amount of time available to choose actions. As the community grows and many RTS game competitions are held, much work has been done in the direction of integrating hand-authored scripted bots into tree search algorithms, with the goal of making search tractable. In this paper, we survey a collection of representative algorithms that integrate scripts into search or planning algorithms. Then we compare them empirically in the \textmu{}RTS environment and examine the trade-offs for designing such algorithms. We also discuss the potential future work in this direction of research and connections to other types of algorithms.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Yang and Ontañón - 2020 - Integrating Search and Scripts for Real-Time Strat.pdf},
  language = {en}
}

@inproceedings{yang_learning_2018,
  title = {Learning {{Map}}-{{Independent Evaluation Functions}} for {{Real}}-{{Time Strategy Games}}},
  booktitle = {2018 {{IEEE Conference}} on {{Computational Intelligence}} and {{Games}} ({{CIG}})},
  author = {Yang, Z. and Onta{\~n}{\'o}n, S.},
  year = {2018},
  month = aug,
  doi = {10.1109/CIG.2018.8490369},
  abstract = {Real-time strategy (RTS) games have drawn great attention in the AI research community, for they offer a challenging and rich testbed for both machine learning and AI techniques. Due to their enormous state spaces and possible map configurations, learning good and generalizable representations for machine learning is crucial to build agents that can perform well in complex RTS games. In this paper we present a convolutional neural network approach to learn an evaluation function that focuses on learning general features that are independent of the map configuration or size. We first train and evaluate the network on a winner prediction task on a dataset collected with a small set of maps with a fixed size. Then we evaluate the network's generalizability to three set of larger maps. by using it as an evaluation function in the context of Monte Carlo Tree Search. Our results show that the presented architecture can successfully capture general and map-independent features applicable to more complex RTS situations.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\KC9FPDJM\\Yang and Ontañón - 2018 - Learning Map-Independent Evaluation Functions for .pdf;C\:\\Users\\aesou\\Zotero\\storage\\UVG66HC3\\8490369.html},
  keywords = {AI techniques,Artificial intelligence,computer games,convolution,convolutional neural network approach,feedforward neural nets,Games,learning (artificial intelligence),machine learning,map-independent evaluation functions,MCTS,Monte Carlo methods,Monte Carlo tree search,neural networks,Neural networks,real-time strategy,real-time strategy games,Real-time systems,RTS games,Task analysis,Training,tree searching}
}

@inproceedings{yang_mixtape_2019,
  title = {Mixtape: {{Breaking}} the {{Softmax Bottleneck Efficiently}}},
  shorttitle = {Mixtape},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32},
  author = {Yang, Zhilin and Luong, Thang and Salakhutdinov, Russ R and Le, Quoc V},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and d\textbackslash{}textquotesingle {Alch{\'e}-Buc}, F. and Fox, E. and Garnett, R.},
  year = {2019},
  pages = {5775--5783},
  publisher = {{Curran Associates, Inc.}},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Yang et al. - 2019 - Mixtape Breaking the Softmax Bottleneck Efficient.pdf;C\:\\Users\\aesou\\Zotero\\storage\\5EI2ZTLM\\Yang et al. - 2019 - Mixtape Breaking the Softmax Bottleneck Efficient.pdf;C\:\\Users\\aesou\\Zotero\\storage\\673YACFQ\\9723-mixtape-breaking-the-softmax-bottleneck-efficiently.html}
}

@inproceedings{yang_modified_2019,
  title = {A {{Modified Multi}}-Size {{Convolution Neural Network}} for {{Winner Prediction Based}} on {{Time Serial Datasets}}},
  booktitle = {Proceedings of the 2019 4th {{International Conference}} on {{Mathematics}} and {{Artificial Intelligence}}  - {{ICMAI}} 2019},
  author = {Yang, Weilong and Huang, Jie and Hu, Yue},
  year = {2019},
  pages = {110--114},
  publisher = {{ACM Press}},
  address = {{Chegndu, China}},
  doi = {10.1145/3325730.3325744},
  abstract = {Researches of AI planning in Real-Time Strategy (RTS) games have been widely applied to human behavior modeling and combat simulation. Winner prediction is an important research area for AI planning, which ensures the decision accuracy. Convolution neural network has proved effective in predicting winner for RTS games. This paper focuses on modify the neural network to handle the time serial datasets. Experiments show that the modified evaluating algorithm can effectively improve the accuracy of winner prediction for time serial data in RTS games.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Yang et al. - 2019 - A Modified Multi-size Convolution Neural Network f.pdf},
  isbn = {978-1-4503-6258-0},
  language = {en}
}

@book{yannakakis_artificial_2018,
  title = {Artificial {{Intelligence}} and {{Games}}},
  author = {Yannakakis, Georgios N. and Togelius, Julian},
  year = {2018},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-63519-4},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\KREC6LJA\\Yannakakis and Togelius - 2018 - Artificial Intelligence and Games.pdf},
  isbn = {978-3-319-63518-7 978-3-319-63519-4},
  language = {en}
}

@inproceedings{yannakakis_game_2012,
  title = {Game {{AI}} Revisited},
  booktitle = {Proceedings of the 9th Conference on {{Computing Frontiers}} - {{CF}} '12},
  author = {Yannakakis, Geogios N.},
  year = {2012},
  pages = {285},
  publisher = {{ACM Press}},
  address = {{Cagliari, Italy}},
  doi = {10.1145/2212908.2212954},
  abstract = {More than a decade after the early research efforts on the use of artificial intelligence (AI) in computer games and the establishment of a new AI domain the term ``game AI'' needs to be redefined. Traditionally, the tasks associated with game AI revolved around non player character (NPC) behavior at different levels of control, varying from navigation and pathfinding to decision making. Commercial-standard games developed over the last 15 years and current game productions, however, suggest that the traditional challenges of game AI have been well addressed via the use of sophisticated AI approaches, not necessarily following or inspired by advances in academic practices. The marginal penetration of traditional academic game AI methods in industrial productions has been mainly due to the lack of constructive communication between academia and industry in the early days of academic game AI, and the inability of academic game AI to propose methods that would significantly advance existing development processes or provide scalable solutions to real world problems. Recently, however, there has been a shift of research focus as the current plethora of AI uses in games is breaking the non-player character AI tradition. A number of those alternative AI uses have already shown a significant potential for the design of better games.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\X3XDQHQM\\Yannakakis - 2012 - Game AI revisited.pdf},
  isbn = {978-1-4503-1215-8},
  language = {en}
}

@article{yannakakis_panorama_2015,
  title = {A {{Panorama}} of {{Artificial}} and {{Computational Intelligence}} in {{Games}}},
  author = {Yannakakis, Georgios N. and Togelius, Julian},
  year = {2015},
  month = dec,
  volume = {7},
  pages = {317--335},
  issn = {1943-068X, 1943-0698},
  doi = {10.1109/TCIAIG.2014.2339221},
  abstract = {This paper attempts to give a high-level overview of the field of artificial and computational intelligence (AI/CI) in games, with particular reference to how the different core research areas within this field inform and interact with each other, both actually and potentially. We identify ten main research areas within this field: NPC behavior learning, search and planning, player modeling, games as AI benchmarks, procedural content generation, computational narrative, believable agents, AI-assisted game design, general game artificial intelligence and AI in commercial games. We view and analyze the areas from three key perspectives: (1) the dominant AI method(s) used under each area; (2) the relation of each area with respect to the end (human) user; and (3) the placement of each area within a human-computer (player-game) interaction perspective. In addition, for each of these areas we consider how it could inform or interact with each of the other areas; in those cases where we find that meaningful interaction either exists or is possible, we describe the character of that interaction and provide references to published studies, if any. We believe that this paper improves understanding of the current nature of the game AI/CI research field and the interdependences between its core areas by providing a unifying overview. We also believe that the discussion of potential interactions between research areas provides a pointer to many interesting future research projects and unexplored subfields.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\NF28VY8Y\\Yannakakis and Togelius - 2015 - A Panorama of Artificial and Computational Intelli 2.pdf;C\:\\Users\\aesou\\Zotero\\storage\\W5DTQLVI\\Yannakakis and Togelius - 2015 - A Panorama of Artificial and Computational Intelli.pdf},
  journal = {IEEE Transactions on Computational Intelligence and AI in Games},
  language = {en},
  number = {4}
}

@article{yannakakis_player_2013,
  title = {Player {{Modeling}}},
  author = {Yannakakis, Georgios N. and Spronck, Pieter and Loiacono, Daniele and Andr{\'e}, Elisabeth},
  year = {2013},
  doi = {10.4230/dfu.vol6.12191.45},
  abstract = {Player modeling is the study of computational models of players in games. This includes the detection, modeling, prediction and expression of human player characteristics which are manifested through cognitive, affective and behavioral patterns. This chapter introduces a holistic view of player modeling and provides a high level taxonomy and discussion of the key components of a player's model. The discussion focuses on a taxonomy of approaches for constructing a player model, the available types of data for the model's input and a proposed classification for the model's output. The chapter provides also a brief overview of some promising applications and a discussion of the key challenges player modeling is currently facing which are linked to the input, the output and the computational model.},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\IADK6PUZ\\Yannakakis et al. - 2013 - Player Modeling.pdf},
  journal = {Schloss Dagstuhl - Leibniz-Zentrum fuer Informatik GmbH, Wadern/Saarbruecken, Germany},
  language = {en}
}

@article{yin_semi-markov_2016,
  title = {A {{Semi}}-{{Markov Decision Model}} for {{Recognizing}} the {{Destination}} of a {{Maneuvering Agent}} in {{Real Time Strategy Games}}},
  author = {Yin, Quanjun and Yue, Shiguang and Zha, Yabing and Jiao, Peng},
  year = {2016},
  volume = {2016},
  pages = {1--12},
  issn = {1024-123X, 1563-5147},
  doi = {10.1155/2016/1907971},
  file = {C\:\\Users\\aesou\\Zotero\\storage\\SUL4QZH7\\Yin et al. - 2016 - A Semi-Markov Decision Model for Recognizing the D.pdf},
  journal = {Mathematical Problems in Engineering},
  language = {en}
}

@incollection{yoshizoe_new_2008,
  title = {A {{New Proof}}-{{Number Calculation Technique}} for {{Proof}}-{{Number Search}}},
  booktitle = {Computers and {{Games}}},
  author = {Yoshizoe, Kazuki},
  year = {2008},
  volume = {5131},
  pages = {135--145},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-87608-3_13},
  abstract = {We propose a new simple calculation technique for proof numbers in Proof-Number Search. Search algorithms based on (dis)proof numbers are known to be effective for solving problems such as tsumego, tsume-shogi, and checkers. Usually, the Proof-Number Search expands child nodes with the smallest (dis)proof number because such nodes are expected to be the easiest to (dis)prove the node. However, when many unpromising child nodes exist, (dis)proof numbers are not always a suitable measure for move ordering because unpromising nodes temporarily increase the (dis)proof numbers. For such cases, we propose the use of only some child nodes (instead of all child nodes) for calculating (dis)proof numbers. We call this technique Dynamic Widening.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2008\\18 A New Proof-Number Calculation Technique for Proof-Number Search.pdf},
  isbn = {978-3-540-87607-6 978-3-540-87608-3},
  language = {en}
}

@inproceedings{yoshizoe_scalable_2011,
  title = {Scalable {{Distributed Monte}}-{{Carlo Tree Search}}},
  booktitle = {Fourth {{Annual Symposium}} on {{Combinatorial Search}}},
  author = {Yoshizoe, Kazuki and Kishimoto, Akihiro and Kaneko, Tomoyuki and Yoshimoto, Haruhiro and Ishikawa, Yutaka},
  year = {2011},
  month = jul,
  abstract = {Monte-Carlo Tree Search (MCTS) is remarkably successful in two-player games, but parallelizing MCTS has been notoriously difficult to scale well, especially in distributed environments. For a distributed parallel search, transposition-table driven scheduling (TDS) is known to be efficient in several domains. We present a massively parallel MCTS algorithm, that applies the TDS parallelism to the Upper Confidence bound Applied to Trees (UCT) algorithm, which is the most representative MCTS algorithm. To drastically decrease communication overhead, we introduce a reformulation of UCT called Depth-First UCT. The parallel performance of the algorithm is evaluated on clusters using up to 1,200 cores in artificial game-trees. We show that this approach scales well, achieving 740-fold speedups in the best case.},
  copyright = {Authors who publish a paper in this conference agree to the following terms:     Author(s) agree to transfer their copyrights in their article/paper to the Association for the Advancement of Artificial Intelligence (AAAI), in order to deal with future requests for reprints, translations, anthologies, reproductions, excerpts, and other publications. This grant will include, without limitation, the entire copyright in the article/paper in all countries of the world, including all renewals, extensions, and reversions thereof, whether such rights current exist or hereafter come into effect, and also the exclusive right to create electronic versions of the article/paper, to the extent that such right is not subsumed under copyright.   The author(s) warrants that they are the sole author and owner of the copyright in the above article/paper, except for those portions shown to be in quotations; that the article/paper is original throughout; and that the undersigned right to make the grants set forth above is complete and unencumbered   The author(s) agree that if anyone brings any claim or action alleging facts that, if true, constitute a breach of any of the foregoing warranties, the author(s) will hold harmless and indemnify AAAI, their grantees, their licensees, and their distributors against any liability, whether under judgment, decree, or compromise, and any legal fees and expenses arising out of that claim or actions, and the undersigned will cooperate fully in any defense AAAI may make to such claim or action. Moreover, the undersigned agrees to cooperate in any claim or other action seeking to protect or enforce any right the undersigned has granted to AAAI in the article/paper. If any such claim or action fails because of facts that constitute a breach of any of the foregoing warranties, the undersigned agrees to reimburse whomever brings such claim or action for expenses and attorneys\&rsquo; fees incurred therein.   Author(s) retain all proprietary rights other than copyright (such as patent rights).   Author(s) may make personal reuse of all or portions of the above article/paper in other works of their own authorship.   Author(s) may reproduce, or have reproduced, their article/paper for the author\&rsquo;s personal use, or for company use provided that AAAI copyright and the source are indicated, and that the copies are not used in a way that implies AAAI endorsement of a product or service of an employer, and that the copies per se are not offered for sale. The foregoing right shall not permit the posting of the article/paper in electronic or digital form on any computer network, except by the author or the author\&rsquo;s employer, and then only on the author\&rsquo;s or the employer\&rsquo;s own web page or ftp site. Such web page or ftp site, in addition to the aforementioned requirements of this Paragraph, must provide an electronic reference or link back to the AAAI electronic server, and shall not post other AAAI copyrighted materials not of the author\&rsquo;s or the employer\&rsquo;s creation (including tables of contents with links to other papers) without AAAI\&rsquo;s written permission.   Author(s) may make limited distribution of all or portions of their article/paper prior to publication.   In the case of work performed under U.S. Government contract, AAAI grants the U.S. Government royalty-free permission to reproduce all or portions of the above article/paper, and to authorize others to do so, for U.S. Government purposes.   In the event the above article/paper is not accepted and published by AAAI, or is withdrawn by the author(s) before acceptance by AAAI, this agreement becomes null and void.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Yoshizoe et al. - 2011 - Scalable Distributed Monte-Carlo Tree Search.pdf;C\:\\Users\\aesou\\Zotero\\storage\\E6DURHC2\\Yoshizoe et al. - 2011 - Scalable Distributed Monte-Carlo Tree Search.pdf;C\:\\Users\\aesou\\Zotero\\storage\\AQ3XRQDH\\4023.html},
  language = {en}
}

@article{you_gate_2019,
  title = {Gate {{Decorator}}: {{Global Filter Pruning Method}} for {{Accelerating Deep Convolutional Neural Networks}}},
  shorttitle = {Gate {{Decorator}}},
  author = {You, Zhonghui and Yan, Kun and Ye, Jinmian and Ma, Meng and Wang, Ping},
  year = {2019},
  month = sep,
  abstract = {Filter pruning is one of the most effective ways to accelerate and compress convolutional neural networks (CNNs). In this work, we propose a global filter pruning algorithm called Gate Decorator, which transforms a vanilla CNN module by multiplying its output by the channel-wise scaling factors (i.e. gate). When the scaling factor is set to zero, it is equivalent to removing the corresponding filter. We use Taylor expansion to estimate the change in the loss function caused by setting the scaling factor to zero and use the estimation for the global filter importance ranking. Then we prune the network by removing those unimportant filters. After pruning, we merge all the scaling factors into its original module, so no special operations or structures are introduced. Moreover, we propose an iterative pruning framework called Tick-Tock to improve pruning accuracy. The extensive experiments demonstrate the effectiveness of our approaches. For example, we achieve the state-of-the-art pruning ratio on ResNet-56 by reducing 70\% FLOPs without noticeable loss in accuracy. For ResNet-50 on ImageNet, our pruned model with 40\% FLOPs reduction outperforms the baseline model by 0.31\% in top-1 accuracy. Various datasets are used, including CIFAR-10, CIFAR-100, CUB-200, ImageNet ILSVRC-12 and PASCAL VOC 2011.},
  archivePrefix = {arXiv},
  eprint = {1909.08174},
  eprinttype = {arxiv},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\You et al. - 2019 - Gate Decorator Global Filter Pruning Method for A.pdf},
  journal = {arXiv:1909.08174 [cs, eess]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing},
  language = {en},
  primaryClass = {cs, eess}
}

@incollection{young_reverse_2016,
  title = {A {{Reverse Hex Solver}}},
  booktitle = {Computers and {{Games}}},
  author = {Young, Kenny and Hayward, Ryan B.},
  year = {2016},
  volume = {10068},
  pages = {137--148},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-50935-8_13},
  abstract = {We present Solrex, an automated solver for the game of Reverse Hex. Reverse Hex, also known as Rex, or Mis`ere Hex, is the variant of the game of Hex in which the player who joins her two sides loses the game. Solrex performs a mini-max search of the state space using Scalable Parallel Depth First Proof Number Search, enhanced by the pruning of inferior moves and the early detection of certain winning strategies.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2016\\17 A Reverse Hex Solver.pdf},
  isbn = {978-3-319-50934-1 978-3-319-50935-8},
  language = {en}
}

@incollection{zhao_using_2008,
  title = {Using {{Artificial Boundaries}} in the {{Game}} of {{Go}}},
  booktitle = {Computers and {{Games}}},
  author = {Zhao, Ling and M{\"u}ller, Martin},
  year = {2008},
  volume = {5131},
  pages = {81--91},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-87608-3_8},
  abstract = {Local search in the game of Go is easier if local areas have well-defined boundaries. An artificial boundary consists of temporarily added stones that close off an area. This paper describes a new general framework for finding boundaries in a way such that existing local search methods can be used. Furthermore, by using a revised local UCT search method, it is shown experimentally that this framework increases performance on local Go problems with open boundaries.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\ICCG 2008\\13 Using Artificial Boundaries in the Game of Go.pdf},
  isbn = {978-3-540-87607-6 978-3-540-87608-3},
  language = {en}
}

@inproceedings{zhu_programming_2019,
  title = {Programming in Game Space: How to Represent Parallel Programming Concepts in an Educational Game},
  shorttitle = {Programming in Game Space},
  booktitle = {Proceedings of the 14th {{International Conference}} on the {{Foundations}} of {{Digital Games}}  - {{FDG}} '19},
  author = {Zhu, Jichen and Alderfer, Katelyn and Furqan, Anushay and Nebolsky, Jessica and Char, Bruce and Smith, Brian and Villareale, Jennifer and Onta{\~n}{\'o}n, Santiago},
  year = {2019},
  pages = {1--10},
  publisher = {{ACM Press}},
  address = {{San Luis Obispo, California}},
  doi = {10.1145/3337722.3337749},
  abstract = {Concurrent and parallel programming (CPP) skills are increasingly important in today's world of parallel hardware. However, the conceptual leap from deterministic sequential programming to CPP is notoriously challenging to make. Our educational game Parallel is designed to support the learning of CPP core concepts through a game-based learning approach, focusing on the connection between gameplay and CPP. Through a 10-week user study (n 25) in an undergraduate concurrent programming course, the first empirical study for a CPP educational game, our results show that Parallel offers both CPP knowledge and student engagement. Furthermore, we provide a new framework to describe the design space for programming games in general.},
  file = {C\:\\Users\\aesou\\Google Drive\\Doctorate\\Articles\\Zhu et al. - 2019 - Programming in game space how to represent parall.pdf},
  isbn = {978-1-4503-7217-6},
  language = {en}
}


